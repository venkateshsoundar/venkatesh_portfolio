[
    {
        "page": 1,
        "text": "VENKATESH SOUNDARARAJAN \n +1-(825) 962-9211 \u25cf Calgary, AB \u25cf venkatesh.balusoundar@gmail.com \u25cf ca.linkedin.com/in/venkateshbalus  \nPortfolio: https://venkateshbalu.streamlit.app/ \n \nPROFILE \nBusiness Analyst and Data Quality Engineer with over eight years of experience in the insurance industry, specializing in \nGuidewire Insurance Suite implementations and functional analysis across PolicyCenter, BillingCenter, and ClaimCenter \nmodules. Proven ability to gather and translate complex business requirements into effective technical solutions, supporting \nprocess improvements and system enhancements. Currently advancing expertise in data science by completing a Master\u2019s \ndegree in Data Science and Analytics at the University of Calgary, focusing on data quality, statistical modeling, and machine \nlearning. Committed to leveraging a strong foundation in insurance domain knowledge and emerging data analytics skills to drive \ninnovation and deliver value-added Guidewire business solutions. \n \nCORE SKILLS \nFunctional Skills: \n\u2022 \nQuality Assurance: Test Automation, Functional & Integration Testing, Defect Management \n\u2022 \nBusiness Analysis & Requirements Management \n\u2022 \nGuidewire Insurance Suite \nProgramming & Data Analysis: \n\u2022 \nJava (Selenium WebDriver), Python, R, SQL \n\u2022 \nData Analysis Libraries: Pandas, NumPy, Matplotlib \n\u2022 \nMachine Learning: Scikit-Learn \nData Visualization & Reporting: \n\u2022 \nTableau, Power BI, Excel, Streamlit Dashboard Development \nCloud & DevOps Tools: \n\u2022 \nAWS (EC2, S3, DynamoDB) \n\u2022 \nVersion Control: Git, GitHub \n\u2022 \nAgile Project Management: JIRA, Rally \nEDUCATION  \nMasters in Data Science and Analytics                                                                                                                          September 2024 \u2013 Present \nUniversity of Calgary, Alberta, Canada \nBachelor of Engineering                                                                             \n \n \n \n              August 2009 \u2013 May-2013 \nAnna University, Chennai, India \n \nEMPLOYMENT EXPERIENCE \nSenior Consultant \n \n \n \n \n \n \n                     \n       October 2021 - August 2024 \nDeloitte Consulting India Private Limited, India  \n\u2022 \nWorked as a Functional Analyst for Personal Lines insurance projects, gathering and documenting business requirements \nwith SMEs and product owners, translating them into detailed functional specs and JIRA user stories. \n\u2022 \nParticipated in 3 Amigos sessions (QA, BA, Dev) for new enhancements, helping define clear acceptance criteria and \nreduce requirement ambiguities. \n\u2022 \nSupported client demos by walkthrough of implemented functionalities, enhancing client understanding and engagement. \n\u2022 \nExecuted integration and configuration functionalities aligned with evolving requirements, contributing to a 95% defect \nremoval rate before release.  \n\u2022 \nDeveloped an automated status dashboard framework, reducing manual effort by 50% and improving project delivery \nefficiency. \n\u2022 \nPrepared and shared Daily Status Reports with stakeholders to ensure clear communication of project progress and issue \ntracking \n\u2022 \nGained hands-on experience with AWS cloud services including EC2, DynamoDB, and S3, supporting cloud-based project \ncomponents and enhancing technical proficiency."
    },
    {
        "page": 2,
        "text": "Consultant \n \n \n \n \n \n \n \n                          \n             May 2018 - October 2021  \nCapgemini Technology Services India Private Limited, India \n\u2022 \nConducted end-to-end testing of Guidewire-based Worker Compensation policies, validating Operational DataStore (ODS) \ndata accuracy through ETL mapping verification from Customer Portal to Data Warehouse. \n\u2022 \nTransitioned from QA to a hybrid BA/QA role, balancing responsibilities in requirements gathering, analysis, and test \nautomation for insurance software projects. \n\u2022 \nCreated an automation tool that sent real-time failure alerts, improving response times to critical issues. \n\u2022 \nActively participated in agile forums within a team of eight members, reducing the sprint carryover rate by 15% and helping \nthe team consistently meet sprint goals. \n\u2022 Provided mentorship and leadership to the data QA team, fostering skill development and ensuring high standards of \nproductivity and quality. \nAssociate     \n \n \n \n \n \n \n \n                                       September 2013 - May 2018 \nCognizant Technology Solutions India Private Limited, India \n\u2022 \nJoined as a fresher in the Mainframe Testing batch, specializing in DB2 database and batch processing testing within \nhealthcare IT projects. \n\u2022 \nManaged and tracked key project metrics for healthcare initiatives valued at over $2M across all phases of the Software \nTesting Life Cycle (STLC), contributing to a 10% increase in process efficiency. \n\u2022 \nLed test environment management for continuous testing across three healthcare projects, driving automation and \nreducing test environment setup time by 40%. \n\u2022 \nDelivered 10+ knowledge transfer sessions to ensure smooth handover and retention of critical healthcare domain \nknowledge among team members and stakeholders. \n\u2022 \nSupported healthcare production remediation processes including ad hoc data extraction, reporting, data patching, and \ndocument re-triggering to resolve critical live system issues. \nPROFESSIONAL DEVELOPMENT \n\u2022 \nMember of Data Science Club - University of Calgary -2024. \n\u2022 \nMember of Test Tribe - Calgary Chapter- 2024. \n \nCERTIFICATIONS \n\u2022 \nInsurance and Guidewire Suite Analyst 10.0 \u2013 Jasper \u2013Guidewire Education -2024. \n\u2022 \nKarate DSL \u2013 Udemy -2023. \n\u2022 \nRest API Automation - Test Leaf Software Solutions Private Limited \u2013 2023. \n\u2022 \nSelenium WebDriver - Test Leaf Software Solutions Private Limited \u2013 2022. \n\u2022 \nSQL for Data Science - Coursera -2020. \n\u2022 \nSDET- Capgemini \u2013 2020. \nACCOMPLISHMENTS \n\u2022 \nAwarded as \u201cSpot Award\u201d 2022 & 2023 for Insurcloud \u2013 Deloitte, Canada. \n\u2022 \nAwarded as \u201cBest Contributor\u201d 2018 for COMPASS Program \u2013 Hartford Insurance, USA. \n\u2022 \nAwarded as \u201cQE & A Maestro\u201d 2017 of Centene by Cognizant QE&A, USA. \n\u2022 \nAwarded as \u201cPride of the Quarter Q1\u201d-April 2017 of Health Net by Cognizant QE&A, USA. \n\u2022 \nAwarded as the \u201cPillar of the Month \u201cMay2014\u201d & \u201cAugust-2015\u201d of Health Net by Cognizant QE&A, USA. \nINTERESTS \n\u2022 \nEvent Coordinator and active in Cognizant Outreach, India. \n\u2022 \nVolunteered in Cognizant Go Green Activities- Beach and Lake Cleaning, India."
    },
    {
        "page": 3,
        "text": "CanadianQualityofLifeAnalysis\nJuly 13, 2025\n0.1\nIntroduction\nQuality of life (QoL) refers to an individual\u2019s perception of their position in life, covering physical\nhealth, mental health, and social relationships, relative to their expectations and cultural context\n(World Health Organization, 1995). In health research, QoL is affected by many factors, including\nhealth drivers and health barriers.\nHealth drivers such as physical activity and mental health\nconsultations typically have a positive influence on QoL, while health barriers like smoking, alcohol\nconsumption, and cannabis use are often linked to negative health outcomes (Haskell et al. 2007;\nRehm et al., 2009).\nThis project aims to explore the relationship between these health drivers and barriers, focusing\non their impact on physical and mental health outcomes. Using Visualizations and correlation\nstatistics, we will analyze variables such as body mass index (BMI), self-reported health status,\nand stress to evaluate the influence of health barriers. Following this, we will perform subgroup\nanalyses to determine how age, sex, region, and education are impacted by these health barriers.\nDepending on our findings we will either help identify the most vulnerable populations and suggest\ntargeted interventions to improve their QoL, or try to suggest more appropriate ways to analyze\nthis dataset.\n0.2\nDataset and Guiding Questions\nWe chose the Canadian Community Health Survey as our dataset. Specifically, we will be using\nthe 2019-2020 microdata file available at Statistics Canada (Statistics Canada, 2023).\nThe Canadian Community Health Survey is an annual survey that collects data from respondents\nacross Canada and differentiates them by their province and respective health regions. The survey\nis voluntary response but is designed to have a large representative sample, all identifying data\nis removed prior to the release of the microdata file. The survey collects data on a wide variety\nof health indicators, potential health determinants, and plenty of demographic data to assist in\nanalysis. The datafile comes in the form of a CSV file and comes with a data dictionary and a very\naccessible format given the sheer size of the CSV file.\nThe dataset has 691 individual columns including the respondent\u2019s record number with the columns\nrepresenting the variables. For each of these 691 variables there are 108,252 individual responses\nas the rows for the data file. We plan to choose a specific number of variables and analyze them\naccording to the questions listed below.\nOur guiding questions can be listed into two categories:\na. Health Drivers Vs. Health Barriers\n1. How does alcohol consumption affect mental health among different age groups?\n1"
    },
    {
        "page": 4,
        "text": "2. How does cannabis use impact stress levels?\n3. What is the relationship between smoking and physical health outcomes?\n4. Which demographic groups (age, sex, region) are most affected by health barriers?\nb. Health Drivers Vs. Health Improvements\n1. What is the relationship between regular exercise and self-reported health status?\n2. How does stress influence the maintenance or improvement of mental health?\n[4]: #Make sure all appropriate libraries loaded\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport plotly.express as px\nfrom pydataset import data\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pydataset import data\n[7]: #Load dataset attached to submission\ndf=pd.read_csv('pumf_cchs.csv')\n[15]: #This should filter all of the variables we want on the general level. You will\u2423\n\u21aaneed to determine which variables you will use\n#IMPORTANT I did not include all the CCC values for chronic disease here. I can\u2423\n\u21aalook and see what we need, and try to adjust shortly\n#If you think anything is missing here, feel free to change\ndf_general=df.filter(items=[\n'ALC_015',\n'ALC_020',\n'GEN_010',\n'GEN_015',\n'GEN_020',\n'GEN_025',\n'CCC_195',\n'GEN_005',\n'HWTDGISW',\n'CAN_015',\n'SMK_005',\n'SMK_060',\n'PAADVACV',\n'PAA_030',\n'PAA_060',\n'PAA_095',\n'SBE_005',\n'SBE_010',\n2"
    },
    {
        "page": 5,
        "text": "'PAA_005',\n'DHHGAGE',\n'DHH_SEX',\n'GEOGPRV',\n'EHG2DVH3',\n'HWT_050',\n'PEX_005',\n'ADM_RNO1',\n'GENDVHDI'\n])\nprint(df_general.head())\nALC_015\nALC_020\nGEN_010\nGEN_015\nGEN_020\nGEN_025\nCCC_195\nGEN_005\n\\\n0\n5.0\n3.0\n9.0\n3.0\n2.0\n2.0\n2.0\n3.0\n1\n1.0\n1.0\n4.0\n3.0\n3.0\n6.0\n1.0\n3.0\n2\n96.0\n96.0\n7.0\n3.0\n3.0\n6.0\n2.0\n2.0\n3\n96.0\n96.0\n8.0\n3.0\n3.0\n6.0\n2.0\n3.0\n4\n96.0\n96.0\n0.0\n5.0\n4.0\n6.0\n2.0\n5.0\nHWTDGISW\nCAN_015\n\u2026\nSBE_010\nPAA_005\nDHHGAGE\nDHH_SEX\nGEOGPRV\n\\\n0\n1.0\n2.0\n\u2026\n6.0\n2.0\n3.0\n2.0\n47.0\n1\n2.0\n2.0\n\u2026\n6.0\n2.0\n5.0\n1.0\n47.0\n2\n2.0\n2.0\n\u2026\n1.0\n6.0\n5.0\n2.0\n59.0\n3\n2.0\n2.0\n\u2026\n6.0\n6.0\n5.0\n1.0\n13.0\n4\n2.0\n2.0\n\u2026\n6.0\n6.0\n4.0\n1.0\n46.0\nEHG2DVH3\nHWT_050\nPEX_005\nADM_RNO1\nGENDVHDI\n0\n3.0\n3.0\n96.0\n1000\n2.0\n1\n2.0\n1.0\n96.0\n100005\n2.0\n2\n1.0\n1.0\n96.0\n100012\n3.0\n3\n1.0\n1.0\n96.0\n100015\n2.0\n4\n3.0\n3.0\n96.0\n100018\n0.0\n[5 rows x 27 columns]\n0.3\nAnalysis\n0.3.1\nHealth Drivers Vs. Health Barriers\nQuestion 1 - How does alcohol consumption affect mental health among different age\ngroups?\nTo determine how Alcohol Consumption Affected Mental Health Among Different Age\nGroups, the data was first cleaned.\nDue to the size of the initial dataset, any data columns\ncontaining unnecessary variables were removed. Following this, it was made sure there were no\nmissing values, and removed any values that were of no interest to my analysis, such as \u2018valid skip\u2019,\ndo not know, and refuse to say. As they were not pertinent to the analysis, we felt it was safe to\nremove them.\n3"
    },
    {
        "page": 6,
        "text": "[ ]: #Make a copy of our initial filtered data, and then filter for our question\ndf3=df_general.copy()\ndf_q3=df3.\n\u21aafilter(items=['ALC_015','ALC_020','GEN_010','GEN_015','GEN_020','GEN_025','CCC_195','DHHGAG\n[ ]: #Some basic info on the dataset\nprint(df_q3.shape)\nprint(df_q3.info())\n[ ]: #Ensure no null values\nprint(df_q3.isnull().sum())\n[ ]: #Removed invalid values that relected responses like unknown, did not answer,\u2423\n\u21aaor valid skip\ndf_q3=df_q3[~df_q3['GEN_010'].isin(range(97,100))]\ndf_q3=df_q3[~df_q3['GEN_015'].isin(range(7,10))]\ndf_q3=df_q3[~df_q3['GEN_020'].isin(range(7,9))]\ndf_q3=df_q3[~df_q3['GEN_025'].isin(range(6,10))]\ndf_q3=df_q3[~df_q3['ALC_015'].isin(range(96,100))]\ndf_q3=df_q3[~df_q3['ALC_020'].isin(range(96,100))]\ndf_q3=df_q3[~df_q3['CCC_195'].isin(range(7,9))]\nUpon inspection of the data dictionary, it was discovered that some likret scales went in the direction\nof negative to positive, while other went from positive to negative. To avoid confusion, the variable\norder was changed so that all progressed in a single logical direction.\n[ ]: #Convert GEN_015,_020,_025 so they progress in a logical orderm with 1=more\u2423\n\u21aanegative and 5=more positive\ngen_015_new_order={1:5,2:4,3:3,4:2,5:1}\ngen_020_new_order={1:5,2:4,3:3,4:2,5:1}\ngen_025_new_order={1:5,2:4,3:3,4:2,5:1}\ndf_q3['GEN_015'] = df_q3['GEN_015'].map(gen_015_new_order)\ndf_q3['GEN_020'] = df_q3['GEN_020'].map(gen_015_new_order)\ndf_q3['GEN_025'] = df_q3['GEN_025'].map(gen_015_new_order)\n[ ]: #Create labels that represent the values listed in the data dictionary\nperceived_mental_health={1: 'Poor', 2: 'Fair', 3: 'Good', 4: 'Very good', 5:\u2423\n\u21aa'Excellent'}\nperceived_life_stress= {1: 'Extremely stressful',2: 'Quite a bit stressful',3:\u2423\n\u21aa'A bit stressful',4: 'Not very stressful',5: 'Not at all stressful'}\nperceived_work_stress={1: 'Extremely stressful',2: 'Quite a bit stressful',3:\u2423\n\u21aa'A bit stressful',4: 'Not very stressful',5: 'Not at all stressful'}\nalc_freq={\n1: 'Less than once/month',2: 'Once/month',3: '2-3 times/month',\n4: 'Once/week',5: '2-3 times/week',6: '4-6 times/week',7: 'Every day'}\n4"
    },
    {
        "page": 7,
        "text": "alc_binge_freq= {1: 'Never',2: 'Less than once/month',3: 'Once/month',4: '2-3\u2423\n\u21aatimes/month',\n5: 'Once/week',6: 'More than once/week',}\nage_group={1:\"12 to 17 years\",2:\"18 to 34 years\",3:\"35 to 49 years\",4:\"50 to 64\u2423\n\u21aayears\",5:\"65 and older\"}\n[ ]: #Create new columns that list the previously created labels\ndf_q3[\"perceived_mental_health\"] = df_q3[[\"GEN_015\"]].copy().replace({\"GEN_015\":\n\u21aa perceived_mental_health})\ndf_q3[\"perceived_life_stress\"] = df_q3[[\"GEN_020\"]].copy().replace({\"GEN_020\":\u2423\n\u21aaperceived_life_stress})\ndf_q3[\"perceived_work_stress\"] = df_q3[[\"GEN_025\"]].copy().replace({\"GEN_025\":\u2423\n\u21aaperceived_work_stress})\ndf_q3[\"alc_freq\"] = df_q3[[\"ALC_015\"]].copy().replace({\"ALC_015\": alc_freq})\ndf_q3[\"alc_binge_freq\"] = df_q3[[\"ALC_020\"]].copy().replace({\"ALC_020\":\u2423\n\u21aaalc_binge_freq})\ndf_q3[\"age_group\"] = df_q3[[\"DHHGAGE\"]].copy().replace({\"DHHGAGE\": age_group})\n[ ]: #Make a copy of the dataset, so if we make a mistake we don't need to start\u2423\n\u21aafrom the beginning\ndf_q3a=df_q3.copy()\n[ ]: #Create a correlation amongst our variables of interest, and then plot them in\u2423\n\u21aaa heatmap\nlabels = {\n'ALC_015': 'Alcohol Use Frequency',\n'ALC_020': 'Binge Drinking Frequency',\n'GEN_010': 'Life Satisfaction',\n'GEN_015': 'Self-Perceived Mental Health',\n'GEN_020': 'Perceived Life Stress',\n'CCC_195': 'Mood Disorders',\n'DHHGAGE': 'Age Group'\n}\ncorrelations = df_q3a[['ALC_015', 'ALC_020', 'GEN_010', 'GEN_015', 'GEN_020',\u2423\n\u21aa'CCC_195','DHHGAGE']].corr()\ncorrelations.rename(columns=labels,index=labels, inplace=True)\nprint(\"Correlation Matrix:\")\nprint(correlations)\n#correlation heatmap\nsns.heatmap(correlations, annot=True, cmap='coolwarm', center=0)\nplt.title(\"Correlation Between Alcohol Use and Mental Health Variables\")\nplt.show()\nBased on our correlation of all our variables of interest, we most see weak relationships, with a few\nmoderate relationships, none of which were very surprising. The strongest relationships were seen\n5"
    },
    {
        "page": 8,
        "text": "between alcohol consumpiton frequency and binge drinking frequency, and between life satisfaction\nand self-perceived mental health. We also see a weak correlation between any alcohol consumption\nvariables and mental health variables.\n[ ]: #Create multiple correlations based on age group\ndef correlation_by_age_group(df_q3a, age_groups):\nsubset = df[df['DHHGAGE'] == age_groups]\ncorrelation_matrix = subset[['ALC_015', 'ALC_020', 'GEN_010', 'GEN_015',\u2423\n\u21aa'GEN_020', 'CCC_195']].corr()\nreturn correlation_matrix\ncorrelation_age_2 = correlation_by_age_group(df_q3a, 2)\ncorrelation_age_3 = correlation_by_age_group(df_q3a, 3)\ncorrelation_age_4 = correlation_by_age_group(df_q3a, 4)\nprint(correlation_age_2,'\\n')\nprint(correlation_age_3,'\\n')\nprint(correlation_age_4,'\\n')\n[ ]: #Plot the different heatmap for each correlation based on age group\nlabels = {\n'ALC_015': 'Alcohol Use Frequency',\n'ALC_020': 'Binge Drinking Frequency',\n'GEN_010': 'Life Satisfaction',\n'GEN_015': 'Self-Perceived Mental Health',\n'GEN_020': 'Perceived Life Stress',\n'CCC_195': 'Mood Disorders',\n'DHHGAGE': 'Age Group'\n}\ncorrelation_age_2 = correlation_by_age_group(df_q3a, 2)\ncorrelation_age_3 = correlation_by_age_group(df_q3a, 3)\ncorrelation_age_4 = correlation_by_age_group(df_q3a, 4)\ncorrelation_age_2.rename(columns=labels,index=labels, inplace=True)\ncorrelation_age_3.rename(columns=labels,index=labels, inplace=True)\ncorrelation_age_4.rename(columns=labels,index=labels, inplace=True)\n#heatmaps for the 3 age groups\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nsns.heatmap(correlation_age_2, annot=True, cmap='coolwarm', center=0,\u2423\n\u21aaax=axes[0, 0])\naxes[0, 0].set_title('Correlation Heatmap for Age Group 18-34')\nsns.heatmap(correlation_age_3, annot=True, cmap='coolwarm', center=0,\u2423\n\u21aaax=axes[0, 1])\n6"
    },
    {
        "page": 9,
        "text": "axes[0, 1].set_title('Correlation Heatmap for Age Group 35-49')\nsns.heatmap(correlation_age_4, annot=True, cmap='coolwarm', center=0,\u2423\n\u21aaax=axes[1, 0])\naxes[1, 0].set_title('Correlation Heatmap for Age Group 50-64')\nfig.delaxes(axes[1, 1])\n#rmv empty plot\nplt.tight_layout()\nplt.show()\nFrom the above output, we can see the correlations of our variables of interest, segmented by age\ncategories. By segmenting by age categories we are able to highlight how differening age groups\nexperience the relationships between alcohol use and mental health differently. A few things to\nnote are:\n\u2022 The relationship between alcohol consumption frequency and binge drinking frequency is\nstronger for all age groups.\n\u2022 The relationship between self-perceived mental health and life satisfaction is highest in the\n18-34 age group.\n\u2022 The relationship between self-perceived mental health and mood disorders has a negative\ncorrelation for all age groups, suggesting that individuals with mood disorders are will more\nfrequently have lower self-perceived mental health\n\u2022 The relationship between perceived life stress and mood disorders has a negative correlation\nfor all age groups, suggesting that those with mood disorders are more life to have higher\nperceived life stress\nDespite this, similar to our previous correlation with all variables, we continue to see a weak\ncorrelation between alcohol consumption variables and mental health variables, indicating that\nthere are additional variables that play a large role in affecting mental health.\n[ ]: #Look at the mean value for the various age groups for each variable of interest\nage_group_means = df_q3a.groupby('age_group')[['ALC_015', 'ALC_020', 'GEN_010',\u2423\n\u21aa'GEN_015', 'GEN_020', 'GEN_025', 'CCC_195']].mean()\nprint(age_group_means)\n[ ]: #Creates a dataframe to give the proportion of GEN_015, grouped by ALC_015\navg_counts_alc_freq=df_q3a.groupby('ALC_015', as_index=False)['GEN_015'].\n\u21aavalue_counts(normalize=True, sort=False)\navg_counts_alc_freq\n[ ]: #Plotting alcohol consumption freq and perceived mental health proportions\n#change the color palette\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(np.linspace(0, 1, 5))\nplt.figure().set_figwidth(16)\n7"
    },
    {
        "page": 10,
        "text": "plt.bar(avg_counts_alc_freq['ALC_015'].unique()-0.2,\u2423\n\u21aaavg_counts_alc_freq['proportion'][::5], width=0.1, label='Poor',\u2423\n\u21aacolor=colors[0])\nplt.bar(avg_counts_alc_freq['ALC_015'].unique()-0.1,\u2423\n\u21aaavg_counts_alc_freq['proportion'][1::5], width=0.1, label='Fair',\u2423\n\u21aacolor=colors[1])\nplt.bar(avg_counts_alc_freq['ALC_015'].unique(),\u2423\n\u21aaavg_counts_alc_freq['proportion'][2::5], width=0.1, label='Good',\u2423\n\u21aacolor=colors[2])\nplt.bar(avg_counts_alc_freq['ALC_015'].unique()+0.1,\u2423\n\u21aaavg_counts_alc_freq['proportion'][3::5], width=0.1, label='Very good',\u2423\n\u21aacolor=colors[3])\nplt.bar(avg_counts_alc_freq['ALC_015'].unique()+0.2,\u2423\n\u21aaavg_counts_alc_freq['proportion'][4::5], width=0.1, label='Excellent',\u2423\n\u21aacolor=colors[4])\nplt.xticks(avg_counts_alc_freq['ALC_015'].unique()+0.1/2,('Less than once/\n\u21aamonth','Once/month','2-3 times/month','Once/week','2-3 times/week',\n'4-6 times/week','Every\u2423\n\u21aaday'))\nplt.xlabel('Perceived Mental Health Level by Alcohol Consumption Frequency')\nplt.ylabel('Proportion')\nplt.legend()\nplt.show()\nBased on output above, we can see that as alcohol consumption increase from less than once per\nmonth to 2-3 times per week, the proportion of individuals self perceived mental health improves\nslightly, with a larger proportion reporting excellent perceived mental health when consuming\nalcohol 2-3 times per week. However for individuals that drink 4-6 times per week, or every day,\nthe proportion that report excellent mental health drops. This may indicate that those drinking\nmore frequently may temporarily increase their perceived mental health, but increasing the alcohol\nconsumption frequency by too much will lead to worse self perceived mental health.\n[ ]: #Creates a dataframe to give the proportion of GEN_015, grouped by ALC_020\navg_counts_binge_freq=df_q3a.groupby('ALC_020', as_index=False)['GEN_015'].\n\u21aavalue_counts(normalize=True, sort=False)\navg_counts_binge_freq\n[ ]: #Plotting Binge freq and perceived mental health proportions\n#change the color palette\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(np.linspace(0, 1, 5))\nplt.figure().set_figwidth(16)\nplt.bar(avg_counts_binge_freq['ALC_020'].unique()-0.\n\u21aa2,avg_counts_binge_freq['proportion'][::5],width=0.\n\u21aa1,label='Poor',color=colors[0])\n8"
    },
    {
        "page": 11,
        "text": "plt.bar(avg_counts_binge_freq['ALC_020'].unique()-0.\n\u21aa1,avg_counts_binge_freq['proportion'][1::5],width=0.1,\u2423\n\u21aalabel='Fair',color=colors[1])\nplt.bar(avg_counts_binge_freq['ALC_020'].\n\u21aaunique(),avg_counts_binge_freq['proportion'][2::5],width=0.1,\u2423\n\u21aalabel='Good',color=colors[2])\nplt.bar(avg_counts_binge_freq['ALC_020'].unique()+0.\n\u21aa1,avg_counts_binge_freq['proportion'][3::5],width=0.1, label='Very\u2423\n\u21aagood',color=colors[3])\nplt.bar(avg_counts_binge_freq['ALC_020'].unique()+0.\n\u21aa2,avg_counts_binge_freq['proportion'][4::5],width=0.1,\u2423\n\u21aalabel='Excellent',color=colors[4])\nplt.xticks(avg_counts_binge_freq['ALC_020'].unique()+0.1/2,('Never','Less than\u2423\n\u21aaonce/month','Once/month','2-3 times/month','Once/week',\n'More than once/week'))\nplt.xlabel('Perceived Mental Health Level by Binge Drinking Frequency')\nplt.ylabel('Proportion')\nplt.legend()\nplt.show()\nBased on output above, we can see that as binge drinking frequency increase, the proportion of\nindividuals self perceived mental health steadily decreases, with individuals who binge drink more\nthan once a week reporting the lowest proportions of very good and excellent mental health, while\nreporting the highest proportions of poor and fair mental health. This suggests a possible link\nbetween binge drinking and worse perceived mental health.\n[ ]: #Plots average self-perceived mental health by alcohol consumption\navg_scores_num=df_q3a.groupby(\n['ALC_015','ALC_020'])['GEN_015'].mean().reset_index()\nalc_015_labels = {\n1: 'Less than once/month',\n2: 'Once/month',\n3: '2-3 times/month',\n4: 'Once/week',\n5: '2-3 times/week',\n6: '4-6 times/week',\n7: 'Every day'\n}\n# Mapping for ALC_020\nalc_020_labels = {\n1: 'Never',\n2: 'Less than once/month',\n3: 'Once/month',\n4: '2-3 times/month',\n5: 'Once/week',\n6: 'More than once/week',\n9"
    },
    {
        "page": 12,
        "text": "}\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(np.linspace(0, 1, len(alc_020_labels)))\n# Replace the numeric values with the corresponding labels\navg_scores_num['ALC_015'] = avg_scores_num['ALC_015'].map(alc_015_labels)\navg_scores_num['ALC_020'] = avg_scores_num['ALC_020'].map(alc_020_labels)\nalc_020_order = list(alc_020_labels.values())\n# Ensure correct order for\u2423\n\u21aacategories\ncolor_map = dict(zip(alc_020_order, colors))\n# Plotting\nplt.figure(figsize=(12, 6))\nsns.barplot(data=avg_scores_num, x='ALC_015', y='GEN_015', hue='ALC_020',\u2423\n\u21aaerrorbar='sd',palette=color_map)\nplt.title('Average Self-Perceived Mental Health by Alcohol Consumption')\nplt.xlabel('Frequency of Alcohol Use')\nplt.ylabel('Average Self-Perceived Mental Health')\nplt.legend(title='Binge Drinking Frequency', bbox_to_anchor=(1.05, 1),\u2423\n\u21aaloc='upper left')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\nTo visualize the relationship between alcohol consumption and mental health among different age\ngroup, a barchart was created to compare the average self-perceived mental health by both frequency\nof alcohol use and frequency of binge drinking. One of the main things noticed was the effect binge\ndrinking had on mental health, with the avg self-perceived mental health of the most frequent binge\ndrinkers dropping by almost 0.5 compared to those that did not\n[ ]: #Plots average self-perceived mental health by age group and alcohol consumption\navg_scores2 = df_q3a.groupby(['DHHGAGE', 'ALC_015'])['GEN_015'].mean().\n\u21aareset_index()\nalc_015_labels = {\n1: 'Less than once/month',\n2: 'Once/month',\n3: '2-3 times/month',\n4: 'Once/week',\n5: '2-3 times/week',\n6: '4-6 times/week',\n7: 'Every day'\n}\nage_group={1:\"12 to 17 years\",2:\"18 to 34 years\",3:\"35 to 49 years\",4:\"50 to 64\u2423\n\u21aayears\",5:\"65 and older\"}\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(np.linspace(0, 1, len(alc_015_labels)))\n10"
    },
    {
        "page": 13,
        "text": "avg_scores2['ALC_015'] = avg_scores2['ALC_015'].map(alc_015_labels)\navg_scores2['DHHGAGE'] = avg_scores2['DHHGAGE'].map(age_group)\nalc_015_order = list(alc_015_labels.values())\n# Ensure correct order for\u2423\n\u21aacategories\ncolor_map = dict(zip(alc_015_order, colors))\n# Create a bar plot\nplt.figure(figsize=(12, 6))\nsns.barplot(x='DHHGAGE', y='GEN_015', hue='ALC_015',\u2423\n\u21aadata=avg_scores2,palette=color_map)\nplt.title('Average Self-Perceived Mental Health by Age Group and Alcohol\u2423\n\u21aaConsumption')\nplt.xlabel('Age Group')\nplt.ylabel('Average Self-Perceived Mental Health')\nplt.legend(title='Frequency of Alcohol Use',bbox_to_anchor=(1.05, 1),\u2423\n\u21aaloc='upper left')\nplt.show()\nFrom the above figure, we can see that as frequency of alcohol consumption increase, there is a slight\nincrease in the average perceived mental health, most notably in those 18-34 and 50-64. However,\nas individuals drink 4-6 times per week, or more, there is a decrease in average self perceived mental\nhealth.\n[ ]: #Plots average self-perceived mental health by age group and binge frequency\navg_scores3 = df_q3a.groupby(['DHHGAGE', 'ALC_020'])['GEN_015'].mean().\n\u21aareset_index()\nalc_020_labels = {\n1: 'Never',\n2: 'Less than once/month',\n3: 'Once/month',\n4: '2-3 times/month',\n5: 'Once/week',\n6: 'More than once/week',\n}\nage_group={1:\"12 to 17 years\",2:\"18 to 34 years\",3:\"35 to 49 years\",4:\"50 to 64\u2423\n\u21aayears\",5:\"65 and older\"}\ncmap = plt.get_cmap('coolwarm')\ncolors = cmap(np.linspace(0, 1, len(alc_020_labels)))\navg_scores3['ALC_020'] = avg_scores3['ALC_020'].map(alc_020_labels)\navg_scores3['DHHGAGE'] = avg_scores3['DHHGAGE'].map(age_group)\nalc_020_order = list(alc_020_labels.values())\ncolor_map = dict(zip(alc_020_order, colors))\n# Create a bar plot\n11"
    },
    {
        "page": 14,
        "text": "plt.figure(figsize=(12, 6))\nsns.barplot(x='DHHGAGE', y='GEN_015', hue='ALC_020',\u2423\n\u21aadata=avg_scores3,palette=color_map)\nplt.title('Average Self-Perceived Mental Health by Age Group and Frequency of\u2423\n\u21aaBinge Drinking')\nplt.xlabel('Age Group')\nplt.ylabel('Average Self-Perceived Mental Health')\nplt.legend(title='Frequency of Binge Drinking',bbox_to_anchor=(1.05, 1),\u2423\n\u21aaloc='upper left')\nplt.show()\nHere we can see that as the frequency of binge drinking occurs, the average self perceived mental\nhealth of individuals decrease amongst all age groups.\nQuestion 2 - How does cannabis use impact stress levels?\nTo explore the impact of\ncannabis use on one\u2019s stress level, we want to extract the interested varibles to this question that\nare cannabis use, perceived life stress and perceived work stress.\nThen these vriables will be\ncompared and analyzd to determine the imacts of cannabis use.\nFrom the initial survey data,\nvalues that are no interest to this question and may create problems to the analysis such as \u201cvalide\nskip\u201d, \u201cnot state\u201d, \u201cdon\u2019t know\u201d and \u201crefusal\u201d were removed from the columns. All values from\nvaribles are converted into integer type as on the inital dataset, each survey code is represented by\nan integer.\n[ ]: df_q4=df.copy() # make a copy of initial filtered data and filter for the\u2423\n\u21aavaribles relavant to this question.\ndf_q4=df_q4.filter(items=['GEN_020','GEN_025','CAN_015'])\nprint(df_q4.head())\n[ ]: df_q4.to_csv('Question 4.csv',index=False) #Create a new csv for analysis for\u2423\n\u21aathis guiding question.\n[ ]: df_q4.info() #A peek to the dataset info for its data types and columns names.\ndf_q4.describe()\ndf_q4.dtypes\n0.3.2\nData Wrangling\n[ ]: df_q4.isnull().sum() #check for missing values\nAs we can see, after running the below code, all the values from interested varibles have been\nconverted to Int32 type.\n[ ]: df_q4[['CAN_015','GEN_020','GEN_025']]=df_q4[['CAN_015','GEN_020','GEN_025']].\n\u21aaastype(int) #Converts Cannabis Use CAN_015, GEN_020, GEN_025 into integer\u2423\n\u21aatype.\ndf_q4.info()\n12"
    },
    {
        "page": 15,
        "text": "[ ]: #Removing irrelavent values from perceived life stress veriable\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_020']==7].index)\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_020']==8].index)\n[ ]: #Removing irrelavent values from perceived work stress varible\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_025']==6].index)\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_025']==7].index)\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_025']==8].index)\ndf_q4=df_q4.drop(df_q4[df_q4['GEN_025']==9].index)\n[ ]: #Removing irrelavent values from cannabis use variable\ndf_q4=df_q4.drop(df_q4[df_q4['CAN_015']==7].index)\ndf_q4=df_q4.drop(df_q4[df_q4['CAN_015']==8].index)\ndf_q4=df_q4.drop(df_q4[df_q4['CAN_015']==9].index)\nAfter running the above code, we can see from below that the minimum value is 1 and maximum\nis 5. values of 6 ,7 ,8, 9 have been removed from dataset.\n[ ]: df_q4.describe()\n0.3.3\nData Analysis and Visualization\n[ ]: # Group preceived life stress by cannabis use to prepare for the data analysis.\ndf4_stress=df_q4.groupby('GEN_020', as_index=False)['CAN_015'].\n\u21aavalue_counts(normalize=True,sort=False)\ndisplay(df4_stress)\n[ ]: # Create a bar chart to visualize the impact of cannabis use to perceived life\u2423\n\u21aastress.\nplt.figure(figsize=(12, 6))\nplt.bar(df4_stress['GEN_020'].unique(),df4_stress['proportion'][::2],width=0.\n\u21aa3,label='Poeple Used Cannabis in Past 12 Months')\nplt.bar(df4_stress['GEN_020'].unique()+0.3,df4_stress['proportion'][1::\n\u21aa2],width=0.3, label=\"People Didn't Use Cannabis in Past 12 Months\")\nplt.xticks(df4_stress['GEN_020'].unique()+0.3/2,('Not at all stressful','Not\u2423\n\u21aavery stressful','A bit stressful','Quite bit stressful','Extremely\u2423\n\u21aastressful'))\nplt.xlabel('Preceived Life Stress by Canabis Use')\nplt.ylabel('Percent')\nplt.legend()\nplt.show()\nBased on the visualization of the graph, we can see that people who used cannais and feeling\nstressful is slightly more than people who used cannabis feeling not stressful. This might show us\nthat using cannabis can still increase one\u2019s life stress level. From the group of people who didn\u2019t\nuse cannabis, the amount of people feeling not stressful is also more than the amount of people\nfeeling stressful. This tells us that not using cannabis will result in a less stressful life. However,\n13"
    },
    {
        "page": 16,
        "text": "the results can also be influnced by the factor that the number of people who didn\u2019t use cannabis\nis significantly more than the number of people who used cannabis in this dataset.\n[ ]: # Group perceived work stress results by cannabis use.\ndf4_stress=df_q4.groupby('GEN_025', as_index=False)['CAN_015'].\n\u21aavalue_counts(normalize=True,sort=False)\ndisplay(df4_stress)\n[ ]: # Plot a bar chart to visulize the relationship of cannabis use and perceived\u2423\n\u21aawork stress.\nplt.figure(figsize=(12, 6))\nplt.bar(df4_stress['GEN_025'].unique(),df4_stress['proportion'][::2],width=0.\n\u21aa3,label='Poeple Used Cannabis in Past 12 Months')\nplt.bar(df4_stress['GEN_025'].unique()+0.3,df4_stress['proportion'][1::\n\u21aa2],width=0.3, label=\"People Didn't Use Cannabis in Past 12 Months\")\nplt.xticks(df4_stress['GEN_025'].unique()+0.3/2,('Not at all stressful','Not\u2423\n\u21aavery stressful','A bit stressful','Quite bit stressful','Extremely\u2423\n\u21aastressful'))\nplt.xlabel('Preceived Work Stress by Canabis Use')\nplt.ylabel('Percent')\nplt.legend()\nplt.show()\nFrom the visualization of this graph, we can see that the amount of people who used canabis and\nfeeling stressful is almost the same as the amount of people who used cannabis and feeling not\nstressful. This may reveal that the cannabis use does not make significant impact to one\u2019s work\nstress. As we can see people who didn\u2019t use canabis can still feel stressful. We didn\u2019t see a tendency\nof increassing stress level in both groups. This may reveal that using canabis does not make work\nmuch more stressful.\nQuestion 3 - What is the relationship between smoking and physical health outcomes\nObjective:\nWhen we start our analysis for this dataset, the most important barriers which we\nchose to focus is smoking and how it impacted the health levels.Also Smoking has complex as-\nsociations with BMI and various physical health outcomes.Health effects of smoking can lead to\nconditions that may influence weight, such as chronic obstructive pulmonary disease (COPD),\nwhich can decrease physical activity levels.\nSmoking can affect metabolism and body fat distribution, often leading to more visceral fat (fat\naround internal organs) compared to non-smokers, which is linked to higher health risks.While\nsmokers may have a lower BMI, smoking itself is associated with numerous health risks, including\ncardiovascular disease, respiratory issues, and cancer. These risks can outweigh any benefits of\nhaving a lower BMI.\nMany individuals experience weight gain after quitting smoking, as appetite increases and\nmetabolism may normalize.\nThis can lead to concerns about obesity, especially if individuals\ndo not adopt healthier eating habits or increase physical activity.\nMain Objective for this question is to analyze the relation between Smoking/BMI against Health\nlevel Indicators derived from the survery results.Analyze the health improvments whether Stopped\nSmoking and Physically Active Indicators impact and improve the health level of the people\n14"
    },
    {
        "page": 17,
        "text": "[ ]: #dataset.head(5)\nData Cleaning and Transformation\n1) Identifying the potential variables from the dataset and filtering those columns from\nthe dataset and store it in seperate data frame Below are the list of variables which\nwe focused to continue with our analysis Focus Variable(Data Set Column) Survey\nNumber(ADM_RNO1) Age(DHHGAGE) Sex(DHH_SEX) Perceived Health(GENDVHDI)\nProvince Name(GEOGPRV) Smoking Level(SMK_005) BMI Level(HWTDGISW) Physical\nActivity Indicator(PAADVACV) Stopped Smoking IndicatorIndicato(SMK_060)(SMK_060)\n[ ]: print(\"Filtered out the required variables from the data set\")\nconvdata=df[[\"ADM_RNO1\",\"DHH_SEX\",\"DHHGAGE\",\"GENDVHDI\",\"GEOGPRV\",\"HWTDGISW\",\n\"PAADVACV\",\"SMK_005\",\"SMK_060\"]]\nprint(convdata.dtypes)\n2) Apply Transformation of the perceived health indicator variable to quantify it for further\nanalysis. Orginal Data Set has the indicators starting from 0 which will not support our\nanalysis during the average health calculation.Hence applying transformation to capture the\nindicators from (1-5)\n[ ]: convdata[\"GENDVHDI\"]=convdata[\"GENDVHDI\"]+1\n3) Create python dictionary for the categorical variable with the values from the data dictionary\n[ ]: agegroup={1:\"12-17 years\",2:\"18-34 years\",3:\"35-49 years\",4:\"50-64 years\",5:\n\u21aa\"65<\"}\nsextype={1:\"Male\",2:\"Female\"}\nperceivedhealthd={1:\"Poor\",2:\"Fair\",3:\"Good\",4:\"Very good\",5:\"Excellent\",10:\n\u21aa\"Don\u2019t know\"}\ngeo={10:\"NEWFOUNDLAND AND LABRADOR\",11:\"PRINCE EDWARD ISLAND\",12:\"NOVA SCOTIA\",\n13:\"NEW BRUNSWICK\",24:\"QUEBEC\",35:\"ONTARIO\",46:\"MANITOBA\",47:\n\u21aa\"SASKATCHEWAN\",\n48:\"ALBERTA\",59:\"BRITISH COLUMBIA\",60:\"YUKON/NORTHWEST/NUNAVUT\"}\nbmi={1:\"Normal weight\",2:\"Overweight/Obese\",6:\"Valid skip\",9:\"Not stated\"}\nSmokingFrequency={1:\"Daily\",2:\"Occasionally\",3:\"Not at all\",7:\"Don\u2019t know\",8:\n\u21aa\"Refusal\",9:\"Not stated\"}\nphysicallyactive={1:\"Physically active above recommended level\",2:\"Physically\u2423\n\u21aaactive below recommended level\",3:\"No physical activity\",6:\"Valid skip\",9:\n\u21aa\"Not stated\"}\nstoppedsmk={1:\"<1 yr ago\",2:\">1 year to <2 yrs ago\",3:\">2 year to <3 yrs ago\",4:\n\u21aa\">3 yrs ago\",6:\"Valid skip\",7:\"Don\u2019t know\",8:\"Refusal\",9:\"Not stated\"}\n4) Create new columns with the copy of the categorical variable and convert them using the\ndictionaries created below.Hence for the categorical variable we shall have two columns once\nwith the numeric and other with the types which help in driving the analysis better in\nvisualization.\n15"
    },
    {
        "page": 18,
        "text": "[ ]: convdata[\"sextype\"] = convdata[[\"DHH_SEX\"]].copy().replace({\"DHH_SEX\": sextype})\nconvdata[\"agegroup\"] = convdata[[\"DHHGAGE\"]].copy().replace({\"DHHGAGE\":\u2423\n\u21aaagegroup})\nconvdata[\"perceivedhealthd\"] = convdata[[\"GENDVHDI\"]].copy().\n\u21aareplace({\"GENDVHDI\": perceivedhealthd})\nconvdata[\"geo\"] = convdata[[\"GEOGPRV\"]].copy().replace({\"GEOGPRV\": geo})\nconvdata[\"bmi\"] = convdata[[\"HWTDGISW\"]].copy().replace({\"HWTDGISW\": bmi})\nconvdata[\"SmokingFrequency\"] = convdata[[\"SMK_005\"]].copy().replace({\"SMK_005\":\u2423\n\u21aaSmokingFrequency})\nconvdata[\"physicallyactive\"] = convdata[[\"PAADVACV\"]].copy().\n\u21aareplace({\"PAADVACV\": physicallyactive})\nconvdata[\"stoppedsmk\"] = convdata[[\"SMK_060\"]].copy().replace({\"SMK_060\":\u2423\n\u21aastoppedsmk})\n[ ]: print(convdata.dtypes)\n[ ]: heatmap=convdata.copy()\nconvdata=convdata[['ADM_RNO1','agegroup','sextype','geo','perceivedhealthd','bmi','SmokingFre\n[ ]: values1=['Daily','Occasionally','Not at all']\nconvdata= convdata[convdata['SmokingFrequency'].isin(values1)]\ndisplay(convdata['SmokingFrequency'].unique())\nvalues2=['Normal weight','Overweight/Obese']\nconvdata= convdata[convdata['bmi'].isin(values2)]\nvalues3=['Don\u2019t know']\nconvdata= convdata[~convdata['perceivedhealthd'].isin(values3)]\nconvdata\nconvdata.to_csv('C:/Users/Venkateshwaran/OneDrive/Desktop/Data601_Assignment/\n\u21aamine.csv', index=False)\nData Summary:\nInitiated the Analysis by the Pie Plotting the Individual Charts to Understand\nthe summaty of the different Health levels,Smoking and BMI\n[ ]: analysis1a = convdata.groupby('SmokingFrequency').size().reset_index().\n\u21aarename(columns={\"index\": \"value\", 0: \"count\"})\nanalysis1a= analysis1a[analysis1a['SmokingFrequency'].isin(values1)]\nfig1 = px.pie(analysis1a, values='count', names='SmokingFrequency',hole=.\n\u21aa5,width=500,height=500,color_discrete_sequence=[\"green\", \"red\",\"yellow\"])\nfig1.update_layout(title=\"Survey Population Density against Smoking\u2423\n\u21aaFrequency\",title_x=0.5)\nfig1.show()\n[ ]: analysis1c = convdata.groupby('perceivedhealthd').size().reset_index().\n\u21aarename(columns={\"index\": \"value\", 0: \"count\"})\n#analysis1c= analysis1b[analysis1b['bmi'].isin(values2)]\nfig3 = px.pie(analysis1c, values='count', names='perceivedhealthd',hole=.\n\u21aa5,width=500,height=500,color_discrete_sequence=px.colors.qualitative.G10)\n16"
    },
    {
        "page": 19,
        "text": "fig3.update_layout(title=\"Overall Health Status\",title_x=0.5)\nfig3.show()\n[ ]: analysis1b = convdata.groupby('bmi').size().reset_index().\n\u21aarename(columns={\"index\": \"value\", 0: \"count\"})\nanalysis1b= analysis1b[analysis1b['bmi'].isin(values2)]\nfig2 = px.pie(analysis1b, values='count', names='bmi',hole=.\n\u21aa5,width=500,height=500,color_discrete_sequence=[\"red\",\"green\"])\nfig2.update_layout(title=\"Survey Population Density against BMI\",title_x=0.5)\nfig2.show()\n[ ]: #analysis1.to_csv('C:/Users/Venkateshwaran/OneDrive/Desktop/Data601_Assignment/\n\u21aaquestion1_data.csv', index=False)\nInference:\n1) About 19.3% of people have reported their Health status as Excellent and 3.6% of Population\nas Poor. Our aim is to analyze and understand the health status of the people with the poor\ndata reported\n2) About 12.67% of people having smoking habits with 10.5% with Daily smokers\n3) About 60% of people reported them as Obese which is a noticeable Data for suggestion.\nUnderstanding the Correlation between the BMI and Smoking\n[ ]: analysis1=convdata.\n\u21aagroupby(['bmi','SmokingFrequency'],as_index=False)['GENDVHDI'].mean().\n\u21aareset_index()\nanalysis1['percentage'] = analysis1['GENDVHDI'] / analysis1.\n\u21aagroupby('bmi')['GENDVHDI'].transform('sum') * 100\nanalysis1['percentage'] = round(analysis1['percentage'],2)\n[ ]: fig = px.bar(analysis1, x=\"bmi\",\u2423\n\u21aay=\"percentage\",color='SmokingFrequency',barmode='group',height=500,width=1000,text_auto=Tru\n#fig.update_yaxes(range=[0, 100])\nfig.update_layout(\ntitle=\"Body Mass Index Against Average Perceived Health Indicator wrt to\u2423\n\u21aaSmoking Frequency\",title_x=0.5,\nxaxis_title=\"Body Mass Index (BMI)\",\nyaxis_title=\"Perceived Health Indicator\",\n)\nfig.show()\nInference:\n1) People with smoking habits have less health satisfaction level than people with not smoke.\n2) However, Occasional Smokers have improved health than Non- Smokers under Overweight\nCategory and which signifies that having obese will ideally affect human life span\n3) Overall Daily Smokers has Bad Average Health Indicator ,With Obese it even worse.\n17"
    },
    {
        "page": 20,
        "text": "4) Overall Average Falls in the Good- Very Good Zone\nUnderstanding the Correlation of the BMI & Smoking with Different Age groups\n[ ]: analysis2=convdata.groupby(['SmokingFrequency','agegroup'])['GENDVHDI'].mean().\n\u21aareset_index()\nfig = px.line(analysis2, x=\"agegroup\", y=\"GENDVHDI\",\u2423\n\u21aacolor=\"SmokingFrequency\",hover_data=['agegroup'],color_discrete_sequence=['red','green','or\nheight=500,width=1000)\nfig.update_yaxes(range=[0, 5])\nfig.update_layout(\ntitle=\"Impact of Perceived Health VS smoking( Agewise)\",title_x=0.5,\nxaxis_title=\"Age Category\",\nyaxis_title=\"Perceived Health Indicator\",\n)\nfig.show()\n[ ]: analysis3=convdata.groupby(['bmi','agegroup'])['GENDVHDI'].mean().reset_index()\nanalysis3\nfig = px.line(analysis3, x=\"agegroup\", y=\"GENDVHDI\",\u2423\n\u21aacolor=\"bmi\",hover_data=['agegroup'],height=500,width=1000)\nfig.update_yaxes(range=[0, 5])\nfig.update_layout(\ntitle=\"Impact of Perceived Health Vs BMI (AgeWise) \",title_x=0.5,\nxaxis_title=\"Age Category\",\nyaxis_title=\"Perceived Health Indicator\",\n)\nfig.show()\nInference:\nBoth Smoking habit and Increased BMI is gradually affecting the Indicator of the\npeoples when they grow older and older\nUnderstanding the people Density of Smoking and BMI\n[ ]: analysis4=convdata.groupby(['SmokingFrequency','bmi'])['GENDVHDI'].count().\n\u21aareset_index()\nanalysis4['percentage'] = analysis4['GENDVHDI'] / analysis4.\n\u21aagroupby('SmokingFrequency')['GENDVHDI'].transform('sum') * 100\nanalysis4['percentage'] = round(analysis4['percentage'],2)\n[ ]: fig = px.bar(analysis4, x=\"SmokingFrequency\", y=\"percentage\",\u2423\n\u21aacolor=\"bmi\",hover_data=['bmi'],barmode='group',height=500,width=800,text_auto=True)\nfig.update_yaxes(range=[0, 70])\nfig.show()\nInference\n1) Bar Plot clearly tells us the health levels are being impacted when the frequency of smoking\nis increased and people with the higher frequency of smoking has more prone to obese with\n18"
    },
    {
        "page": 21,
        "text": "higher BMI.\n2) Interestingly people with no smoke has more obese which clearlt sticked to our inital under-\nstanding of BMI and Smoking relation\nUnderstanding the Health Level of People with Canada Geographical Province\n[ ]: analysis5=convdata.groupby(['geo'])['GENDVHDI'].mean().reset_index()\nanalysis5[\"geo\"] = analysis5[\"geo\"].replace([\"NEWFOUNDLAND AND LABRADOR\"],\u2423\n\u21aa\"Newfoundland\n& Labrador\")\nanalysis5[\"geo\"] = analysis5[\"geo\"].replace([\"YUKON/NORTHWEST/NUNAVUT\"], \"YUKON\u2423\n\u21aaTERRITORY\")\nanalysis5['geo']=\nanalysis5['geo'].str.title()\nimport json\ncanada_states= json.load(open(\"canada_provinces.geojson\",'r'))\nstate_id_map={}\nfor feature in canada_states['features']:\nfeature['id']= feature['properties']['CODE']\nstate_id_map[feature['properties']['NAME']] = feature['id']\nstate_id_map\n[ ]: analysis5['id']=analysis5['geo'].apply(lambda x: state_id_map[x])\nanalysis5.rename(columns={'GENDVHDI': 'HealthStatus(1-5)'}, inplace=True)\nanalysis5\n[ ]: fig= px.\n\u21aachoropleth_mapbox(analysis5,locations='id',geojson=canada_states,color='HealthStatus(1-5)',\nmapbox_style=\"carto-positron\",center={'lat':62.\n\u21aa24,'lon':-96.78})\nfig.update_layout(mapbox_zoom=2)\nfig.update_layout(title='Geographical wise Health Status',title_x=0.5)\nfig.show()\nInference:\nAverage Health levels of people in Quebec is decent and the health level of people in\nNew Brunswick is compartitively poor\n[ ]: analysissmokingmap=convdata.groupby('geo')['SmokingFrequency'].count().\n\u21aareset_index()\nanalysissmokingmap[\"geo\"] = analysissmokingmap[\"geo\"].replace([\"NEWFOUNDLAND\u2423\n\u21aaAND LABRADOR\"], \"Newfoundland\n& Labrador\")\nanalysissmokingmap[\"geo\"] = analysissmokingmap[\"geo\"].replace([\"YUKON/NORTHWEST/\n\u21aaNUNAVUT\"], \"YUKON TERRITORY\")\nanalysissmokingmap['geo']=analysissmokingmap['geo'].str.title()\nanalysissmokingmap['geo']\nanalysissmokingmap['id']=analysissmokingmap['geo'].apply(lambda x:\u2423\n\u21aastate_id_map[x])\nanalysissmokingmap['percentage'] = analysissmokingmap['SmokingFrequency'] /\u2423\n\u21aaanalysissmokingmap['SmokingFrequency'].sum() * 100\n19"
    },
    {
        "page": 22,
        "text": "fig= px.\n\u21aachoropleth_mapbox(analysissmokingmap,locations='id',geojson=canada_states,color='percentage\nmapbox_style=\"carto-positron\",center={'lat':62.\n\u21aa24,'lon':-96.78})\nfig.update_layout(mapbox_zoom=2)\nfig.update_layout(title='Geographical wise Smoking Status',title_x=0.5)\nfig.show()\nInference:\n1) Smoking Frequency of Yukon reported higher all across the canada, however their health\nstatus is average\n2) We infer that because of the climatic conditions people tends to smoke comparitively more\nthan other province\n3) Peoples in Ontario has lesser smoking frequency and their health status is reported average\n[ ]: analysisbmimap=convdata.copy()\nanalysisbmimap = analysisbmimap[analysisbmimap['bmi'] == 'Overweight/Obese']\nanalysisbmimap=analysisbmimap.groupby('geo')['bmi'].count().reset_index()\nanalysisbmimap[\"geo\"] = analysisbmimap[\"geo\"].replace([\"NEWFOUNDLAND AND\u2423\n\u21aaLABRADOR\"], \"Newfoundland\n& Labrador\")\nanalysisbmimap[\"geo\"] = analysisbmimap[\"geo\"].replace([\"YUKON/NORTHWEST/\n\u21aaNUNAVUT\"], \"YUKON TERRITORY\")\nanalysisbmimap['geo']= analysisbmimap['geo'].str.title()\nanalysisbmimap['id']=analysisbmimap['geo'].apply(lambda x: state_id_map[x])\nanalysisbmimap['percentage'] = analysisbmimap['bmi'] / analysisbmimap['bmi'].\n\u21aasum() * 100\nfig2= px.\n\u21aachoropleth_mapbox(analysisbmimap,locations='id',geojson=canada_states,color='percentage',ho\nmapbox_style=\"carto-positron\",center={'lat':62.\n\u21aa24,'lon':-96.78})\nfig2.update_layout(mapbox_zoom=2)\nfig2.update_layout(title='Geographical wise Obese level',title_x=0.5)\nfig2.show()\nInference:\n1) Ontario Reported people with overweight its because of the population density with the\naverage mean health reported\n2) People in Yukon reported Lower BMi, with the smoking habit which clearly tell us the smoking\nis not directtly related to the BMI component on peoples\u2019health.However smoking ideally\naffects the health levels in other plots.\nUnderstanding the Health Level of People with Stopped Smoking\nThis Analysis is to\nunderstand the average health status of the people who have stopped smoking and people with still\nhaving smoking habits\n20"
    },
    {
        "page": 23,
        "text": "[ ]: analysis5a = convdata[convdata['stoppedsmk'].isin(['<1 yr ago', '>1 year to <2\u2423\n\u21aayrs ago', '>2 year to <3 yrs ago', '>3 yrs ago'])]\nanalysis5a= analysis5a.groupby(['stoppedsmk'])['GENDVHDI'].mean().reset_index()\nanalysis5a['Ind']='Yes'\nanalysis5b = convdata[~convdata['stoppedsmk'].isin(['<1 yr ago', '>1 year to <2\u2423\n\u21aayrs ago', '>2 year to <3 yrs ago', '>3 yrs ago'])]\nanalysis5b= analysis5b.groupby(['stoppedsmk'])['GENDVHDI'].mean().reset_index()\nanalysis5b['Ind']='No'\nanalysis5c=pd.concat([analysis5a, analysis5b], axis=0)\nanalysis5c.groupby(['Ind'])['GENDVHDI'].mean().reset_index()\n[ ]: fig = px.bar(analysis5c.groupby(['Ind'])['GENDVHDI'].mean().reset_index(),\u2423\n\u21aax=\"Ind\", y=\"GENDVHDI\",color='Ind',text_auto=True,height=500,width=500)\nfig.update_yaxes(range=[0, 5])\nfig.update_layout(\ntitle=\"Average Health Improvement upon stopping the smoking\",\nxaxis_title=\"Stopped Smoking Indicator\",\nyaxis_title=\"Perceived Health Indicator\",title_x=0.5\n)\nfig.show()\nInference:\nIt is clear that the people who have stopped smoking have better health indicator\nthan the people with smoking habits.Hence its clear that Smoking has the direct relationship with\nthe people health\nUnderstanding the Health Level of People with Physical Activity\nThis Analysis is to\nunderstand the health levels of the people with their levels of phyical activities\n[ ]: analysis6=convdata[['physicallyactive','bmi','GENDVHDI']]\nanalysis6['physicallyactive']= analysis6['physicallyactive'].apply(lambda x:\u2423\n\u21aa'No physical activity' if x in ('Not stated') else x)\nanalysis6= analysis6[analysis6['physicallyactive']!= 'Valid skip']\nanalysis6=analysis6.groupby(['physicallyactive','bmi'])['GENDVHDI'].mean().\n\u21aareset_index()\nanalysis6['percentage'] = analysis6['GENDVHDI'] / analysis6.\n\u21aagroupby('bmi')['GENDVHDI'].transform('sum') * 100\nanalysis6.sort_values(by = 'percentage', ascending=True)\n[ ]: fig = px.bar(analysis6, x=\"bmi\",\u2423\n\u21aay=\"percentage\",color='physicallyactive',barmode='group',height=500,width=1000,text_auto=Tru\nfig.update_yaxes(range=[0, 50])\nfig.update_layout(\ntitle=\"Physically Active Against Average Perceived Health Indicator wrt to\u2423\n\u21aaBMI\",title_x=0.5,\nxaxis_title=\"Body Mass Index (BMI)\",\nyaxis_title=\"Perceived Health Indicator\",\n21"
    },
    {
        "page": 24,
        "text": ")\nfig.show()\nInference:\nWe infer that the people with the physical activity shows greater health level than\npeople with no activity Its Seems like linear based on the activity progression health levels may\ntend to improve a lot\nUnderstanding the Health Level of People with BMI (Gender Type- Comparison)\nThis Analysis is to understand the health levels of the people against BMI with Gender Level\nComparisons\n[ ]: import plotly.express as px\nvalues1=['Daily','Occasionally','Not at all']\nheatmap= heatmap[heatmap['SmokingFrequency'].isin(values1)]\ndisplay(heatmap['SmokingFrequency'].unique())\nvalues2=['Normal weight','Overweight/Obese']\nheatmap= heatmap[heatmap['bmi'].isin(values2)]\nvalues3=['Don\u2019t know']\nheatmap= heatmap[~heatmap['perceivedhealthd'].isin(values3)]\nheatmap\nfig = px.density_heatmap(heatmap, x=\"SmokingFrequency\", y=\"bmi\",\u2423\n\u21aaz=\"GENDVHDI\",facet_col=\"sextype\", histfunc=\"avg\",text_auto=True,\nlabels=dict(x=\"Smoking Frequency\", y=\"bmi\", z= \"avge\u2423\n\u21aahealth Indicator\"))\nfig.update_layout(title='Gender Wise- BMI level for different smoking\u2423\n\u21aacategories',title_x=0.5)\nfig.show()\nInference\nAs observed in the other charts, Its clear that the health levels are relating to BMI\nand it explains that People with the Daily smoking has very less health level , with the higher obese\nstatus\nUnderstanding the Health Level of People with their Gender Wise- Physical Active\nlevel\nThis Analysis is to understand the health levels of the people who are physically active\n[ ]: heatmap2= heatmap[heatmap['PAADVACV'].isin([1,2,3])]\nfig = px.density_heatmap(heatmap2, x=\"physicallyactive\", y=\"bmi\", z=\"GENDVHDI\",\u2423\n\u21aafacet_col=\"sextype\", histfunc=\"avg\",text_auto=True,\nlabels=dict(x=\"physicallyactive\", y=\"bmi\", z= \"avge\u2423\n\u21aahealth Indicator\"))\nfig.update_layout(title='Gender Wise- Physical Active level',title_x=0.5)\nfig.show()\nInference\nWe Infer that the people with physically active shows improved health than people\nwith no activity\n22"
    },
    {
        "page": 25,
        "text": "Understanding the Health Level of People with their Stopped Smoking Status\nThis\nAnalysis is to understand the health levels of the people against their Stopped Smoking Status in\nyears.\n[ ]: fig = px.density_heatmap(heatmap[~heatmap['stoppedsmk'].isin([\"Not\u2423\n\u21aastated\",\"Don\u2019t know\",\"Valid skip\",\"Refusal\"])], x=\"stoppedsmk\", y=\"sextype\",\u2423\n\u21aaz=\"GENDVHDI\", histfunc=\"avg\",text_auto=True,\nlabels=dict(x=\"stoppedsmk\", y=\"SmokingFrequency\", z=\u2423\n\u21aa\"avge health Indicator\"))\nfig.update_layout(title='Genderwise Status of People Stopped Smoking',title_x=0.\n\u21aa5)\nfig.show()\nfig = px.density_heatmap(heatmap[heatmap['stoppedsmk'].isin([\"Not\u2423\n\u21aastated\",\"Don\u2019t know\",\"Valid skip\",\"Refusal\"])], x=\"stoppedsmk\", y=\"sextype\",\u2423\n\u21aaz=\"GENDVHDI\", histfunc=\"avg\",text_auto=True,\nlabels=dict(x=\"stoppedsmk\", y=\"SmokingFrequency\", z=\u2423\n\u21aa\"avge health Indicator\"))\nfig.update_layout(title='Genderwise Status of People dont smoke',title_x=0.5)\nfig.show()\nInference\n1) We Infer that the people with stopped smoking has shown greater improvement in health and\npeople without smoking habit has close to excellent health status\nConclusion\nOverall we infer that the smoking has major impact on physical health upon our\nanalyis against geography & gender,However overweight/obese doesn\u2019t have much directly related\nto smoking. People who stopped smoking shows greater improvments in health levels.Also we infer\nthat the BMI is greatly improved for the people with physicall active and its declining for people\nwith no physically active\nHence we conclude that Smoking is directly related to Physical health in converse to BMI and\nsuggest that to improve physical health its strongly advised to stop smoking and increase fitness\nlevel.\nQuestion 4 - Which demographic groups (age, sex, region) are most affected by health\nbarriers?\nThis is a more comprehensive question that invovled different demogaphic of people.\nThe health barries refer to cannabis use, smoking frequency and alcohol consumption. In the initial\ndataset, the demographic groups includes people of different gender, age, region and education level.\nThis analysis will explore the impact of each health barries to each demographic group one by one.\nIt\u2019s crucial for our analysis to further consider people from differnt demographic groups as the\nhealth barries ussually affects peole differntly to their background. To begin data analysis, there\nare several steps to conduct the data cleaning. The data cleaning process removes the values that\nare undesired to answering this quiding questions such as \u201cdon\u2019t know\u201d, \u201crefusal\u201d, \u201cnot stated\u201d\nand \u201cvalid skip\u201d. Then it will conver the values of all the desired varibels into integer type as the\nsurvey response received are all in format of integer.\n23"
    },
    {
        "page": 26,
        "text": "[ ]: df_q5=df.copy() # make a copy of initial filtered data and filter for the\u2423\n\u21aavaribles relavant to this question.\ndf_q5=df_q5.\n\u21aafilter(items=['CAN_015','SMK_005','SMMK_060','ALC_015','ALC_020','DHHGAGE','DHH_SEX','GEOGP\nprint(df_q5.head())\n[ ]: df_q5.to_csv('Question 5.csv',index=False) #Create a new csv for analysis for\u2423\n\u21aathis guiding question.\n[ ]: #A peek to the dataset info for its data types and columns names.\ndf_q5.info()\ndf_q5.describe()\ndf_q5.dtypes\n[ ]: df_q5.isnull().sum() #check for missing values\n0.3.4\nData Cleaning:\n[ ]: #Removing undesired values from cannabis use variable.\ndf_q5=df_q5.drop(df_q5[df_q5['CAN_015']==7].index)\ndf_q5=df_q5.drop(df_q5[df_q5['CAN_015']==8].index)\ndf_q5=df_q5.drop(df_q5[df_q5['CAN_015']==9].index)\n[ ]: #Removing undesired values from smoking frequency variable.\ndf_q5=df_q5.drop(df_q5[df_q5['SMK_005']==7].index)\ndf_q5=df_q5.drop(df_q5[df_q5['SMK_005']==8].index)\ndf_q5=df_q5.drop(df_q5[df_q5['SMK_005']==9].index)\n[ ]: #Removing undesired values from alcohol consumption variable.\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_015']==96].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_015']==97].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_015']==98].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_015']==99].index)\n[ ]: #Removing undesired values from alcohol consumption (drinking frequency)\u2423\n\u21aavariable.\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_020']==96].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_020']==97].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_020']==98].index)\ndf_q5=df_q5.drop(df_q5[df_q5['ALC_020']==99].index)\n[ ]: #Removing undesired values from household education level variable.\ndf_q5=df_q5.drop(df_q5[df_q5['EHG2DVH3']==9].index)\n[ ]: # Converted interested varibles into integer type\ndf_q5['CAN_015'] = df_q5['CAN_015'].astype(int)\ndf_q5['GEOGPRV'].dtypes\n24"
    },
    {
        "page": 27,
        "text": "df_q5['SMK_005'] = df_q5['SMK_005'].astype(int)\ndf_q5['SMK_005'].dtypes\ndf_q5['ALC_015'] = df_q5['ALC_015'].astype(int)\ndf_q5['ALC_015'].dtypes\ndf_q5['ALC_020'] = df_q5['ALC_020'].astype(int)\ndf_q5['ALC_020'].dtypes\ndf_q5['DHH_SEX'] = df_q5['DHH_SEX'].astype(int)\ndf_q5['DHH_SEX'].dtypes\ndf_q5['DHHGAGE'] = df_q5['DHHGAGE'].astype(int)\ndf_q5['DHHGAGE'].dtypes\ndf_q5['GEOGPRV'] = df_q5['GEOGPRV'].astype(int)\ndf_q5['GEOGPRV'].dtypes\ndf_q5['EHG2DVH3'] = df_q5['EHG2DVH3'].astype(int)\ndf_q5['EHG2DVH3'].dtypes\nAfter running the above code, we can see from below that the maximum values shows the undesired\nvalues have been removed from all the interested varibels, and all the values have been converted\nto integer type.\n[ ]: df_q5.describe()\n0.3.5\nData Analysis and Visualization\n[ ]: #Group cannabis use results based on sex type.\ndf5_cansex=df_q5.groupby('DHH_SEX', as_index=False)['CAN_015'].\n\u21aavalue_counts(normalize=True,sort=False)\ndisplay(df5_cansex)\n[ ]: # Create a bar plot to show the cannabis use infromation based on each gender\u2423\n\u21aatype.\nplt.figure(figsize=(10,7))\nplt.bar(df5_cansex['DHH_SEX'].unique(),df5_cansex['proportion'][::2],width=0.\n\u21aa3,label='Poeple Used Cannabis in Past 12 Months')\nplt.bar(df5_cansex['DHH_SEX'].unique()+0.3,df5_cansex['proportion'][1::\n\u21aa2],width=0.3, label=\"People Didn't Use Cannabis in Past 12 Months\")\nplt.xticks(df5_cansex['DHH_SEX'].unique()+0.3/2,('Male','Female'))\nplt.xlabel('Gender Impacted by Canabis')\nplt.ylabel('Percent')\nplt.legend()\nplt.show()\nFrom the above bar plot, we can find that females uses canabis slightly more than males.\n[ ]: #Create a pie chart to visualize the proportaion of male and female using\u2423\n\u21aacannabis.\ncannabis_sex_data=df_q5[['CAN_015', 'DHH_SEX']].dropna()\ncannabis_sex=cannabis_sex_data.groupby('DHH_SEX')['CAN_015'].sum()\nDHH_SEX_labels={\n25"
    },
    {
        "page": 28,
        "text": "1: \"Male\",\n2: \"Female\"\n}\ncannabis_sex.index=cannabis_sex.index.map(DHH_SEX_labels)\nplt.figure(figsize=(5,5))\ncannabis_sex.plot(kind='pie',autopct='%1.\n\u21aa1f%%',colors=['blue','orange'],startangle=90,textprops={'fontsize':12})\nplt.title('Cannabis Use by Sex')\nplt.ylabel('')\nplt.show()\nA supplementry pie chart is created to better visulize the proportion of male and female using\ncannabis. We can see that 8% more of females use cannabis compared to males.\n[ ]: #Create a bar plot to explore the smoking frequency by each gender.\nsmoke_sex=df_q5[['SMK_005', 'DHH_SEX']].dropna()\nsmoke_sex=smoke_sex.groupby('DHH_SEX')['SMK_005'].mean()\nDHH_SEX_labels={\n1: \"Male\",\n2: \"Female\"\n}\nsmoke_sex.index=smoke_sex.index.map(DHH_SEX_labels)\nplt.figure(figsize=(8,6))\nplt.ylim(0,3)\nsmoke_sex.plot(kind='bar', color=['skyblue', 'pink'])\nplt.title('Average Smoke Frequency by Gender')\nplt.xlabel('Sex')\nplt.ylabel('Average Smoke Frequency')\nplt.xticks(rotation=0)\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.show()\nBased on the bar chart, we can interpret that females smoke slighly more often than males.\n[ ]: #Create a pie chart to visualize the proportaion of male and female for their\u2423\n\u21aasmoking frequency.\nsmoking_sex=df_q5[['SMK_005', 'DHH_SEX']].dropna()\nsmoking_bysex=smoking_sex.groupby('DHH_SEX')['SMK_005'].sum()\nDHH_SEX_labels={\n1: \"Male\",\n2: \"Female\"\n}\nsmoking_bysex.index=smoking_bysex.index.map(DHH_SEX_labels)\nplt.figure(figsize=(5, 5))\n26"
    },
    {
        "page": 29,
        "text": "smoking_bysex.plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'pink'],\u2423\n\u21aastartangle=90, textprops={'fontsize': 12})\nplt.title('Smoke Frequency by Sex')\nplt.ylabel('')\nplt.show()\nA supplementry pie chart is created to better visulize the proportion of male and female for their\nsmoking frequency. We can see that females smoke 8% more often than males.\n[ ]: # Create bar chart for alcohol comsumptions based on two gender types.\nalcohol_sex=df_q5[['ALC_015', 'ALC_020', 'DHH_SEX']].dropna()\nalc_015_bysex=alcohol_sex.groupby('DHH_SEX')['ALC_015'].mean()\nalc_020_bysex=alcohol_sex.groupby('DHH_SEX')['ALC_020'].mean()\nDHH_SEX_labels={\n1: \"Male\",\n2: \"Female\"\n}\nalc_015_bysex.index=alc_015_bysex.index.map(DHH_SEX_labels)\nalc_020_bysex.index=alc_020_bysex.index.map(DHH_SEX_labels)\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nalc_015_bysex.plot(kind='bar',color=['blue','orange'])\nplt.title('Average Alcohol Consumption Frequency by Sex')\nplt.xlabel('Sex')\nplt.ylabel('Average Alcohol Consumption Frequency')\nplt.xticks(rotation=0)\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.subplot(1,2,2)\nalc_020_bysex.plot(kind='bar',color=['blue','orange'])\nplt.title('Average Alcohol Consumption (Drink 4+/5+ One Occasion) by Sex')\nplt.xlabel('Sex')\nplt.ylabel('Average Alcohol Consumption (Drink 4+/5+ One Occasion')\nplt.xticks(rotation=0)\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nFrom the above bar plots, we found that males generally consumes more alcohols than femles in\nterms of both alcohol comspmtion frequency and drinking 4+/5+ cans on one occasion.\n[ ]: # Plot a bar chart to investigate the cannabis use by differnt age group.\ndata_filtered=df_q5[['CAN_015', 'DHHGAGE']].dropna()\ncannabis_use_different_age=data_filtered.groupby('DHHGAGE')['CAN_015'].mean()\nDHHGAGE_labels={\n1: '12 to 17 years',\n2: '18 to 34 years',\n3: '35 to 49 years',\n27"
    },
    {
        "page": 30,
        "text": "4: '50 to 64 years',\n5: '65 years and older'\n}\ncannabis_use_different_age.index=cannabis_use_different_age.index.\n\u21aamap(DHHGAGE_labels)\nplt.figure(figsize=(10,6))\ncannabis_use_different_age.plot(kind='bar',color='olive')\nplt.title('Average Cannabis Use by Different Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Average Cannabis Use')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.show()\nBased on the above graph, we can tell that people who are at 12 to 17 years and 65 years and older\nuse cannabis most often, with age group 50 years to 64 years and 30 years to 49 years following\nclosely behind. Poeple of 18 years to 34 years use cannabis least often.\n[ ]: # Plot a bar chart to investigate the smoking freqnecy by differnt age group.\nsmoke_data_filtered=df_q5[['SMK_005', 'DHHGAGE']].dropna()\nsmoke_byage=smoke_data_filtered.groupby('DHHGAGE')['SMK_005'].mean()\nDHHGAGE_labels={\n1: \"12 to 17 years\",\n2: \"18 to 34 years\",\n3: \"35 to 49 years\",\n4: \"50 to 64 years\",\n5: \"65 years and older\"\n}\nsmoke_byage.index=smoke_byage.index.map(DHHGAGE_labels)\nplt.figure(figsize=(10,6))\nsmoke_byage.plot(kind='bar',color='purple')\nplt.title('Average Smoke Frequency by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Average Smoke Frequency')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.show()\nFrom the bar plot, we found that people people who are at 12 to 17 years and 65 years and older\nsmoke most often. However, the smoking frequency for the rest of age groups are approximately\nthe same.\n[ ]: # Create two bar charts to analyze the affect of alcohol consumption to each\u2423\n\u21aaage group.\nalcohol_data_filtered=df_q5[['ALC_015', 'ALC_020', 'DHHGAGE']].dropna()\nalc_015_by_age=alcohol_data_filtered.groupby('DHHGAGE')['ALC_015'].mean()\n28"
    },
    {
        "page": 31,
        "text": "alc_020_by_age=alcohol_data_filtered.groupby('DHHGAGE')['ALC_020'].mean()\nDHHGAGE_labels={\n1: \"12 to 17 years\",\n2: \"18 to 34 years\",\n3: \"35 to 49 years\",\n4: \"50 to 64 years\",\n5: \"65 years and older\"\n}\nalc_015_by_age.index=alc_015_by_age.index.map(DHHGAGE_labels)\nalc_020_by_age.index=alc_020_by_age.index.map(DHHGAGE_labels)\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nalc_015_by_age.plot(kind='bar',color='coral')\nplt.title('Average Alcohol Consumption Frequency in Past 12 Months')\nplt.xlabel('Age Group')\nplt.ylabel('Average Alcohol Consumption Frequency')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.subplot(1,2,2)\nplt.ylim(0,2.5)\nalc_020_by_age.plot(kind='bar',color='mediumblue')\nplt.title('Average Alcohol Consumption (Drink 4+/5+ One Occasion)')\nplt.xlabel('Age Group')\nplt.ylabel('Average Alcohol Consumption (Drink 4+/5+ One Occasion)')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nBased on the above graphs, people at the age of 12 to 17 years drink least often. This is becuase\nthe law forbids minors to drink alcohol. From the age of 18 years to 34 years, people start to drink\nmore often, and then reach the maximum alcohol comsumption frequcny at the age of 50 years to\n64 years. However, people at younger age (18 years to 34 years) tend to drink most of alcohol at\nonce. After the age of 35 years, people start to drink less and less alcohol at one occasion. This\nmight be the result of physically aging that the body is less able to handle alcohols over time.\n[ ]: # Create a bar chart to analyze cannabis use by region.\ncannabis_region=df_q5[['CAN_015', 'GEOGPRV']].dropna()\nGEOGPRV_labels={\n10: \"NEWFOUNDLAND AND LABRADOR\",\n11: \"PRINCE EDWARD ISLAND\",\n12: \"NOVA SCOTIA\",\n13: \"NEW BRUNSWICK\",\n24: \"QUEBEC\",\n35: \"ONTARIO\",\n46: \"MANITOBA\",\n47: \"SASKATCHEWAN\",\n29"
    },
    {
        "page": 32,
        "text": "48: \"ALBERTA\",\n59: \"BRITISH COLUMBIA\",\n60: \"YUKON/NORTHWEST/NUNAVUT TERRITORIES\"\n}\ncannabis_region['GEOGPRV']=cannabis_region['GEOGPRV'].map(GEOGPRV_labels)\ncannabis_byregion=cannabis_region.groupby('GEOGPRV')['CAN_015'].mean()\nplt.figure(figsize=(10,7))\nplt.ylim(0,2)\ncannabis_byregion.plot(kind='bar', color='olive')\nplt.title('Average Cannabis Use by Region')\nplt.xlabel('Region')\nplt.ylabel('Average Cannabis Use')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.tight_layout()\nplt.show()\nThe above plot reveals that people among different region use cannabis approximately same fe-\nquently except for the YUKON/NORTHWEST/NUNAVUT territories. This may be due to the\nless population living in these territories.\n[ ]: # Create bar plot for smoking frequency for people at each region.\nsmoke_region=df_q5[['SMK_005', 'GEOGPRV']].dropna()\nGEOGPRV_labels={\n10: \"NEWFOUNDLAND AND LABRADOR\",\n11: \"PRINCE EDWARD ISLAND\",\n12: \"NOVA SCOTIA\",\n13: \"NEW BRUNSWICK\",\n24: \"QUEBEC\",\n35: \"ONTARIO\",\n46: \"MANITOBA\",\n47: \"SASKATCHEWAN\",\n48: \"ALBERTA\",\n59: \"BRITISH COLUMBIA\",\n60: \"YUKON/NORTHWEST/NUNAVUT TERRITORIES\"\n}\nsmoke_region['GEOGPRV']=smoke_region['GEOGPRV'].map(GEOGPRV_labels)\nsmoke_byregion=smoke_region.groupby('GEOGPRV')['SMK_005'].mean()\nplt.figure(figsize=(10,7))\nsmoke_byregion.plot(kind='bar',color='purple')\nplt.title('Average Smoke Frequency by Region')\nplt.xlabel('Region')\nplt.ylabel('Average Smoke Frequency')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\n30"
    },
    {
        "page": 33,
        "text": "plt.tight_layout()\nplt.show()\nBased on the plot, we found that the smoking frequency for people at each provinces are approx-\nimately the same excep for the three territories and Alberta. The reason behind why Albertans\nsmoke less frequently comapred to people at other provinces needs to be further investigated.\n[ ]: # Create bar plot for alcohol consumption freuquency acorss each province/\n\u21aaregion.\nalcohol_data_filtered=df_q5[['ALC_015', 'ALC_020', 'GEOGPRV']].dropna()\nalc_015_region=alcohol_data_filtered.groupby('GEOGPRV')['ALC_015'].mean()\nalc_020_region=alcohol_data_filtered.groupby('GEOGPRV')['ALC_020'].mean()\nGEOGPRV_labels={\n10: \"NEWFOUNDLAND AND LABRADOR\",\n11: \"PRINCE EDWARD ISLAND\",\n12: \"NOVA SCOTIA\",\n13: \"NEW BRUNSWICK\",\n24: \"QUEBEC\",\n35: \"ONTARIO\",\n46: \"MANITOBA\",\n47: \"SASKATCHEWAN\",\n48: \"ALBERTA\",\n59: \"BRITISH COLUMBIA\",\n60: \"YUKON/NORTHWEST/NUNAVUT TERRITORIES\"\n}\nalc_015_region.index=alc_015_region.index.map(GEOGPRV_labels)\nalc_020_region.index=alc_020_region.index.map(GEOGPRV_labels)\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nalc_015_region.plot(kind='bar',color='coral')\nplt.title('Average Alcohol Consumption Frequency (Past 12 Months) By Region')\nplt.xlabel('Region Group')\nplt.ylabel('Average Alcohol Consumption Frequency')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.subplot(1,2,2)\nalc_020_region.plot(kind='bar',color='mediumblue')\nplt.title('Average Alcohol Consumption (Drink 4+/5+ One Occasion) By Region')\nplt.xlabel('Region Group')\nplt.ylabel('Average Alcohol Consumption (Drink 4+/5+ One Occasion)')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nFrom the bar charts, we found that people in Quebec and British Columbia drink acohol most fre-\nquently, and people in New Brunswick have least alcohol consumption frequency. However, people\n31"
    },
    {
        "page": 34,
        "text": "in the YUKON/NORTHWEST/NUNAVUT territories consumes more alcohol at one occasion at\na significantly high frequency compared to other provinces.\n[ ]: # Plot bar chart to visualize the cannabis use circumstance for hosueholds at\u2423\n\u21aaeach educational level.\ncannabis_education=df_q5[['CAN_015', 'EHG2DVH3']].dropna()\nEHG2DVH3_labels={\n1: \"Less than secondary school graduation\",\n2: \"Secondary school graduation, no post-secondary education\",\n3: \"Post-secondary certificate/diploma/university degree\"\n}\ncannabis_education['EHG2DVH3']=cannabis_education['EHG2DVH3'].\n\u21aamap(EHG2DVH3_labels)\ncannabis_education=cannabis_education.groupby('EHG2DVH3')['CAN_015'].mean()\nplt.figure(figsize=(8.5,7))\ncannabis_education.plot(kind='bar',color='olive',width=0.25)\nplt.title('Average Cannabis Use by Education Level')\nplt.xlabel('Education Level')\nplt.ylabel('Average Cannabis Use')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.tight_layout()\nplt.show()\nBased on the bar chart, we see that households with less than secondary graduation use cannabis\nmost often, however, the difference is not significant among other two groups.\n[ ]: # Creat bar plot to visualize smoking frequency among each education level\u2423\n\u21aagroup.\nsmoke_education=df_q5[['SMK_005', 'EHG2DVH3']].dropna()\nEHG2DVH3_labels={\n1: \"Less than secondary school graduation\",\n2: \"Secondary school graduation, no post-secondary education\",\n3: \"Post-secondary certificate/diploma/university degree\"\n}\nsmoke_education['EHG2DVH3']=smoke_education['EHG2DVH3'].map(EHG2DVH3_labels)\nsmoke_education=smoke_education.groupby('EHG2DVH3')['SMK_005'].mean()\nplt.figure(figsize=(8, 7))\nplt.ylim(0,3)\nsmoke_education.plot(kind='bar', color='purple', width=0.3)\nplt.title('Average Smoke Frequency by Education Level')\nplt.xlabel('Education Level')\nplt.ylabel('Average Smoke Frequency')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.tight_layout()\n32"
    },
    {
        "page": 35,
        "text": "plt.show()\nFor smoking frequency, we see that the post secondary group has slightly higher smoking frequency\nbut not signicantly high among other groups. The smoking freuqency for less than secondary group\nand secondary graduation group is about the same.\n[ ]: #Plot bar chart for alcohol consumption for each edcuation level group.\nalcohol_education=df_q5[['ALC_015', 'ALC_020', 'EHG2DVH3']].dropna()\nalc_015_education=alcohol_education.groupby('EHG2DVH3')['ALC_015'].mean()\nalc_020_education=alcohol_education.groupby('EHG2DVH3')['ALC_020'].mean()\nEHG2DVH3_labels={\n1: \"Less than secondary school graduation\",\n2: \"Secondary school graduation, no post-secondary education\",\n3: \"Post-secondary certificate/diploma/university degree\"\n}\nalc_015_education.index=alc_015_education.index.map(EHG2DVH3_labels)\nalc_020_education.index=alc_020_education.index.map(EHG2DVH3_labels)\nplt.figure(figsize=(14,8))\nplt.subplot(1,2,1)\nalc_015_education.plot(kind='bar',color='coral')\nplt.title('Average Alcohol Consumption by Education Level')\nplt.xlabel('Education Level')\nplt.ylabel('Average Alcohol Consumption')\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.subplot(1,2,2)\nalc_020_education.plot(kind='bar',color='mediumblue')\nplt.title('Average Alcohol Consumption by Education Level')\nplt.xlabel('Education Level')\nplt.ylabel('Average Alcohol Consumption')\nplt.xticks(rotation=45,ha='right')\nplt.grid(axis='y',linestyle='--',alpha=0.7)\nplt.tight_layout()\nplt.show()\nBased on the graph, we found that people with post-secondary education drink most frequently,\nwith people with secondary education following closely behind. We also found that people wth\npost-secondary or secondary education consume more alcohol at once compared to people with no\nsecondary education.\n[ ]:\n33"
    },
    {
        "page": 36,
        "text": "0.3.6\nHealth Drivers Vs. Health Improvements\nQuestion 5 - What is the relationship between regular exercise and self-reported health\nstatus?\nIn order to answer our 5th question, I\u2019ll start by charting reported health by reported\nphysical activity against each other. Afterwards I\u2019ll break down how different demographics and\nother relevant variables provided in the dataset affect how the two variables of interest affect their\nresults. To begin I\u2019ll give some details on reported health and reported physical activity. Reported\nhealth is broken down into 5 responses ranging from \u201cPoor\u201d to \u201cExcellent\u201d and reported physical\nhealth is broken down into if the respondent\u2019s activity is \u201cAbove\u201d or \u201cBelow\u201d recommended activity\nguidelines.\n[ ]: df_question6=df_general.copy()\ndf_question6=df_question6.filter(items=[\n'GEN_005',\n'HWTDGISW',\n'PAADVACV',\n'DHHGAGE',\n'DHH_SEX',\n'GEOGPRV',\n'EHG2DVH3'\n])\n#print(df_question6.head())\n#setting some print and summary functions to comments to remove clutter\nConcerning data cleaning I have cleaned the data to select the variables I will be working with\nand then removed rows with response that are not useful to my analysis. Responses such as \u201cvalid\nskip\u201d, \u201crefused\u201d, and \u201cdon\u2019t know\u201d are not of interest to my analysis and have been removed from\nthe dataset I will be working with. Finally, I run code to check for missing values in my dataset.\n[ ]: df_question6=df_question6.drop(df_question6[df_question6['GEN_005']==8].index)\ndf_question6=df_question6.drop(df_question6[df_question6['GEN_005']==7].index)\ndf_question6=df_question6.drop(df_question6[df_question6['HWTDGISW']==6].index)\ndf_question6=df_question6.drop(df_question6[df_question6['HWTDGISW']==9].index)\ndf_question6=df_question6.drop(df_question6[df_question6['PAADVACV']==9].index)\ndf_question6=df_question6.drop(df_question6[df_question6['PAADVACV']==6].index)\ndf_question6=df_question6.drop(df_question6[df_question6['PAADVACV']==3].index)\ndf_question6=df_question6.drop(df_question6[df_question6['EHG2DVH3']==9].index)\ndf_question6.isnull().sum()\nI added a correlation heat map but there are no strong correlations between any of the variables\nincluded in my analysis.\n[ ]: cor6 = df_question6.corr()\nlabels6=['General Health', 'BMI', 'Physical Activity', 'Age', 'Sex','Location',\u2423\n\u21aa'Education']\nsns.heatmap(cor6, annot=True, cmap='coolwarm', center=0, xticklabels=labels6,\u2423\n\u21aayticklabels=labels6)\nplt.title(\"Correlation Between Physical Activity and Health Variables\")\n34"
    },
    {
        "page": 37,
        "text": "plt.show()\nTo start I\u2019ve visualized the initial two variables physical activity and reported health. To do this I\ngrouped physical activity by the reported health categories and visualised them in a bar chart. The\ninitial plot shows a clear trend between the reported health and how much of that population is\nabove the recommended activity guideline. As the reported health decreases the difference between\nthe proportion of people above and below the activity guideline shows downwards and upward\ntrends respectively.\n[ ]: df6_health=df_question6.groupby('GEN_005', as_index=False)['PAADVACV'].\n\u21aavalue_counts(normalize=True,sort=False)\nplt.figure().set_figwidth(10)\nplt.bar(df6_health['GEN_005'].unique(),df6_health['proportion'][::2],width=0.\n\u21aa3,label='Percent Above Activity Guideline')\nplt.bar(df6_health['GEN_005'].unique()+0.3,df6_health['proportion'][1::\n\u21aa2],width=0.3, label='Percent Below Activity Guideline')\nplt.xticks(df6_health['GEN_005'].unique()+0.3/2,('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\nplt.xlabel('Physical Activity By Health Status')\nplt.ylabel('Percent')\nplt.legend()\nplt.show()\nI created subplots showing the first relationship but divided up by age groups. As can be seen in\nthe graphs for the younger age groups there is no discernible trend between physical activity and\nreported health. This plot indicates that the younger age group are overall more active than older\nage groups but there are other factors that are more important to the younger age groups reported\nhealth. The age groups over 50 years old do show the trend in the initial plot. From this it could\nbe reasonably concluded that as age increases physical activity becomes more important to ones\nreported health.\n[ ]: df_question6_age1=df_question6.drop(df_question6[df_question6['DHHGAGE']!=2].\n\u21aaindex)\ndf6_health_age1=df_question6_age1.groupby('GEN_005',\u2423\n\u21aaas_index=False)['PAADVACV'].value_counts(normalize=True,sort=False)\ndf_question6_age2=df_question6.drop(df_question6[df_question6['DHHGAGE']!=3].\n\u21aaindex)\ndf6_health_age2=df_question6_age2.groupby('GEN_005',\u2423\n\u21aaas_index=False)['PAADVACV'].value_counts(normalize=True,sort=False)\ndf_question6_age3=df_question6.drop(df_question6[df_question6['DHHGAGE']!=4].\n\u21aaindex)\ndf6_health_age3=df_question6_age3.groupby('GEN_005',\u2423\n\u21aaas_index=False)['PAADVACV'].value_counts(normalize=True,sort=False)\ndf_question6_age4=df_question6.drop(df_question6[df_question6['DHHGAGE']!=5].\n\u21aaindex)\ndf6_health_age4=df_question6_age4.groupby('GEN_005',\u2423\n\u21aaas_index=False)['PAADVACV'].value_counts(normalize=True,sort=False)\n35"
    },
    {
        "page": 38,
        "text": "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0,0].bar(df6_health_age1['GEN_005'].\n\u21aaunique(),df6_health_age1['proportion'][::2],width=0.3,label='Percent Above\u2423\n\u21aaActivity Guideline')\naxes[0,0].bar(df6_health_age1['GEN_005'].unique()+0.\n\u21aa3,df6_health_age1['proportion'][1::2],width=0.3, label='Percent Below\u2423\n\u21aaActivity Guideline')\naxes[0,0].set_xticks(df6_health_age1['GEN_005'].unique()+0.3/\n\u21aa2,('Excellent','Very Good','Good','Fair','Poor'))\naxes[0,0].set_xlabel('Physical Activity By Health Status 18-34')\naxes[0,0].set_ylabel('Percent')\naxes[0,0].legend()\naxes[0,1].bar(df6_health_age2['GEN_005'].\n\u21aaunique(),df6_health_age2['proportion'][::2],width=0.3,label='Percent Above\u2423\n\u21aaActivity Guideline')\naxes[0,1].bar(df6_health_age2['GEN_005'].unique()+0.\n\u21aa3,df6_health_age2['proportion'][1::2],width=0.3, label='Percent Below\u2423\n\u21aaActivity Guideline')\naxes[0,1].set_xticks(df6_health_age2['GEN_005'].unique()+0.3/\n\u21aa2,('Excellent','Very Good','Good','Fair','Poor'))\naxes[0,1].set_xlabel('Physical Activity By Health Status 35-49')\naxes[0,1].set_ylabel('Percent')\naxes[0,1].legend()\naxes[1,0].bar(df6_health_age3['GEN_005'].\n\u21aaunique(),df6_health_age3['proportion'][::2],width=0.3,label='Percent Above\u2423\n\u21aaActivity Guideline')\naxes[1,0].bar(df6_health_age3['GEN_005'].unique()+0.\n\u21aa3,df6_health_age3['proportion'][1::2],width=0.3, label='Percent Below\u2423\n\u21aaActivity Guideline')\naxes[1,0].set_xticks(df6_health_age3['GEN_005'].unique()+0.3/\n\u21aa2,('Excellent','Very Good','Good','Fair','Poor'))\naxes[1,0].set_xlabel('Physical Activity By Health Status 50-64')\naxes[1,0].set_ylabel('Percent')\naxes[1,0].legend()\naxes[1,1].bar(df6_health_age4['GEN_005'].\n\u21aaunique(),df6_health_age4['proportion'][::2],width=0.3,label='Percent Above\u2423\n\u21aaActivity Guideline')\naxes[1,1].bar(df6_health_age4['GEN_005'].unique()+0.\n\u21aa3,df6_health_age4['proportion'][1::2],width=0.3, label='Percent Below\u2423\n\u21aaActivity Guideline')\n36"
    },
    {
        "page": 39,
        "text": "axes[1,1].set_xticks(df6_health_age4['GEN_005'].unique()+0.3/\n\u21aa2,('Excellent','Very Good','Good','Fair','Poor'))\naxes[1,1].set_xlabel('Physical Activity By Health Status 65 plus')\naxes[1,1].set_ylabel('Percent')\naxes[1,1].legend()\nplt.show()\nAnother breakdown was the two main variables of interest by the respondent\u2019s level of education.\nThe plots show that if a respondent did not complete high school, they are less likely to be active\nthan respondents who completed high school or have a post secondary degree. There were minimal\ndifferences between those that completed high school or hold a post secondary degree. All plots\nfollow the trend shown in the first graph.\n[ ]: df_question6edu1=df_question6.drop(df_question6[df_question6['EHG2DVH3']!=1].\n\u21aaindex)\ndf6_edu1=df_question6edu1.groupby('GEN_005', as_index=False)['PAADVACV'].\n\u21aavalue_counts(normalize=True,sort=False)\ndf_question6edu2=df_question6.drop(df_question6[df_question6['EHG2DVH3']!=2].\n\u21aaindex)\ndf6_edu2=df_question6edu2.groupby('GEN_005', as_index=False)['PAADVACV'].\n\u21aavalue_counts(normalize=True,sort=False)\ndf_question6edu2=df_question6.drop(df_question6[df_question6['EHG2DVH3']!=3].\n\u21aaindex)\ndf6_edu3=df_question6edu2.groupby('GEN_005', as_index=False)['PAADVACV'].\n\u21aavalue_counts(normalize=True,sort=False)\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0,0].bar(df6_edu1['GEN_005'].unique(),df6_edu1['proportion'][::2],width=0.\n\u21aa3,label='Percent Above Activity Guideline')\naxes[0,0].bar(df6_edu1['GEN_005'].unique()+0.3,df6_edu1['proportion'][1::\n\u21aa2],width=0.3, label='Percent Below Activity Guideline')\naxes[0,0].set_xticks(df6_edu1['GEN_005'].unique()+0.3/2,('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[0,0].set_xlabel('Physical Activity By Health Status No HS Diploma')\naxes[0,0].set_ylabel('Percent')\naxes[0,0].legend()\naxes[0,1].bar(df6_edu2['GEN_005'].unique(),df6_edu2['proportion'][::2],width=0.\n\u21aa3,label='Percent Above Activity Guideline')\naxes[0,1].bar(df6_edu2['GEN_005'].unique()+0.3,df6_edu2['proportion'][1::\n\u21aa2],width=0.3, label='Percent Below Activity Guideline')\naxes[0,1].set_xticks(df6_edu2['GEN_005'].unique()+0.3/2,('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[0,1].set_xlabel('Physical Activity By Health Status HS Diploma')\naxes[0,1].set_ylabel('Percent')\naxes[0,1].legend()\n37"
    },
    {
        "page": 40,
        "text": "axes[1,0].bar(df6_edu3['GEN_005'].unique(),df6_edu3['proportion'][::2],width=0.\n\u21aa3,label='Percent Above Activity Guideline')\naxes[1,0].bar(df6_edu3['GEN_005'].unique()+0.3,df6_edu3['proportion'][1::\n\u21aa2],width=0.3, label='Percent Below Activity Guideline')\naxes[1,0].set_xticks(df6_edu3['GEN_005'].unique()+0.3/2,('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[1,0].set_xlabel('Physical Activity By Health Status Uni Degree')\naxes[1,0].set_ylabel('Percent')\naxes[1,0].legend()\nfig.delaxes(axes[1, 1])\nplt.show()\nQuestion 6 - How does stress influence the maintenance or improvement of mental\nhealth?\nFor our 6th question I started by charting reported mental health by the respondent\u2019s\nstress level. After that ill break down the relationship by relevant demographic variables in the\ndataset along with some other visualizations of the data. Much like question 6 reported mental\nhealth is broken down into categories ranging from \u201cpoor\u201d to \u201cexcellent\u201d. Reported stress level is\nbroken down into 5 categories ranging from \u201cnot at all stressful\u201d to \u201cextremely stressful\u201d.\n[ ]: df_question7=df_general.copy()\ndf_question7=df_question7.filter(items=[\n'GEN_015',\n'GEN_020',\n'HWTDGISW',\n'PAADVACV',\n'DHHGAGE',\n'DHH_SEX',\n'GEOGPRV',\n'EHG2DVH3',\n'HWT_050'\n])\n#print(df_question7.head())\nLike question 5 for data cleaning, I selected variables of interest and put them into a new data set\nfor me to perform my analysis. I cleaned unwanted variables like \u201cdon\u2019t know\u201d and \u201crefused\u201d as\nthey are not of interest to my analysis. Lastly, I checked the dataset for missing values. For the\ninitial graph plotting stress level by reported mental health status. As can be seen in the graph\nthere is a distinct trend shown as respondents mental health decreases the percentage of the mental\nhealth level reporting higher stress levels grows. This trend shows the relationship that one could\nexpect that stress does seem have be a determining factor or mental health.\n[ ]: df_question7=df_question7.drop(df_question7[df_question7['GEN_015']==9].index)\ndf_question7=df_question7.drop(df_question7[df_question7['GEN_015']==8].index)\ndf_question7=df_question7.drop(df_question7[df_question7['GEN_015']==7].index)\ndf_question7=df_question7.drop(df_question7[df_question7['GEN_020']==8].index)\ndf_question7=df_question7.drop(df_question7[df_question7['GEN_020']==7].index)\n38"
    },
    {
        "page": 41,
        "text": "df_question7=df_question7.drop(df_question7[df_question7['HWTDGISW']==6].index)\ndf_question7=df_question7.drop(df_question7[df_question7['HWTDGISW']==9].index)\ndf_question7=df_question7.drop(df_question7[df_question7['PAADVACV']==9].index)\ndf_question7=df_question7.drop(df_question7[df_question7['PAADVACV']==6].index)\ndf_question7=df_question7.drop(df_question7[df_question7['PAADVACV']==3].index)\ndf_question7=df_question7.drop(df_question7[df_question7['HWT_050']==6].index)\ndf_question7=df_question7.drop(df_question7[df_question7['HWT_050']==7].index)\ndf_question7=df_question7.drop(df_question7[df_question7['HWT_050']==8].index)\ndf_question7=df_question7.drop(df_question7[df_question7['HWT_050']==9].index)\ndf_question7=df_question7.drop(df_question7[df_question7['EHG2DVH3']==9].index)\ndf_question7.isnull().sum()\nThe correlation matrix with a heat map is included but it does not show any particular results of\ninterest.\n[ ]: cor7 = df_question7.corr()\nlabels7=['Mental Health', 'Stress Level', 'BMI', 'Physical Activity', 'Age',\u2423\n\u21aa'Sex','Location', 'Education','Self Percieved Weight']\nsns.heatmap(cor7, annot=True, cmap='coolwarm', center=0, xticklabels=labels7,\u2423\n\u21aayticklabels=labels7)\nplt.title(\"Correlation Between Stress and Mental Health Variables\")\nplt.show()\nFor the first breakdown by demographics, I have broken down the first graph by the age groups\nreported in the dataset. In each graph the general trend of higher stress levels being seen at the\nlower reported mental health states. However, as age increases the number of respondents in the\nextremes of the reported stress levels differs significantly with older age groups responding more in\nthe extremes.\n[ ]: df_healthstress=df_question7.groupby('GEN_015', as_index=False)['GEN_020'].\n\u21aavalue_counts(normalize=True, sort=False)\nplt.figure().set_figwidth(10)\nplt.bar(df_healthstress['GEN_015'].unique()-0.2,df_healthstress['proportion'][::\n\u21aa5],width=0.1,label='Not at all stressful')\nplt.bar(df_healthstress['GEN_015'].unique()-0.1,df_healthstress['proportion'][1:\n\u21aa:5],width=0.1, label='Not very stressful')\nplt.bar(df_healthstress['GEN_015'].unique(),df_healthstress['proportion'][2::\n\u21aa5],width=0.1, label='A bit stressful')\nplt.bar(df_healthstress['GEN_015'].unique()+0.1,df_healthstress['proportion'][3:\n\u21aa:5],width=0.1, label='Quite a bit stressful')\nplt.bar(df_healthstress['GEN_015'].unique()+0.2,df_healthstress['proportion'][4:\n\u21aa:5],width=0.1, label='Extremely stressful')\nplt.xticks(df_healthstress['GEN_015'].unique(),('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\nplt.xlabel('Stress Level by Mental Health Status')\nplt.ylabel('Percent')\n39"
    },
    {
        "page": 42,
        "text": "plt.legend()\nplt.show()\nFor my last visualisation I plotted reported mental health by self perceived weight to see how\none\u2019s self-perceived weight influences their mental health at all.\nIn the graph there is a trend\nshown, as the respondents mental health decreases the percentage of the population in each mental\nhealth category reporting they view themselves as \u201cjust right\u201d decreases. Likewise, the amount of\neach category\u2019s population reporting that their view themselves as \u201cunderweight\u201d or \u201coverweight\u201d\nincreases as reported mental health decreases.\n[ ]: df_question7_age1=df_question7.drop(df_question7[df_question7['DHHGAGE']!=2].\n\u21aaindex)\ndf7_health_age1=df_question7_age1.groupby('GEN_015', as_index=False)['GEN_020'].\n\u21aavalue_counts(normalize=True,sort=False)\ndf_question7_age2=df_question7.drop(df_question7[df_question7['DHHGAGE']!=3].\n\u21aaindex)\ndf7_health_age2=df_question7_age2.groupby('GEN_015', as_index=False)['GEN_020'].\n\u21aavalue_counts(normalize=True,sort=False)\ndf_question7_age3=df_question7.drop(df_question7[df_question7['DHHGAGE']!=4].\n\u21aaindex)\ndf7_health_age3=df_question7_age3.groupby('GEN_015', as_index=False)['GEN_020'].\n\u21aavalue_counts(normalize=True,sort=False)\ndf_question7_age4=df_question7.drop(df_question7[df_question7['DHHGAGE']!=5].\n\u21aaindex)\ndf7_health_age4=df_question7_age4.groupby('GEN_015', as_index=False)['GEN_020'].\n\u21aavalue_counts(normalize=True,sort=False)\ntemp=pd.DataFrame({'GEN_015':5,'GEN_015':1,'proportion':0},index=[20])\ndf7_health_age1=pd.concat([df7_health_age1.iloc[:20], temp, df7_health_age1.\n\u21aailoc[20:]],ignore_index=True)\ndf7_health_age1.iloc[20,0]=5\ndf7_health_age1.iloc[20,1]=1\ndf7_health_age1.iloc[20,2]=0\ntemp=pd.DataFrame({'GEN_015':5,'GEN_015':1,'proportion':0},index=[20])\ndf7_health_age2=pd.concat([df7_health_age2.iloc[:20], temp, df7_health_age2.\n\u21aailoc[20:]],ignore_index=True)\ndf7_health_age2.iloc[20,0]=5\ndf7_health_age2.iloc[20,1]=1\ndf7_health_age2.iloc[20,2]=0\ntemp=pd.DataFrame({'GEN_015':5,'GEN_015':1,'proportion':0},index=[20])\ndf7_health_age4=pd.concat([df7_health_age4.iloc[:20], temp, df7_health_age4.\n\u21aailoc[20:]],ignore_index=True)\ndf7_health_age4.iloc[20,0]=5\ndf7_health_age4.iloc[20,1]=1\ndf7_health_age4.iloc[20,2]=0\n40"
    },
    {
        "page": 43,
        "text": "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0,0].bar(df7_health_age1['GEN_015'].unique()-0.\n\u21aa2,df7_health_age1['proportion'][::5],width=0.1,label='Not at all stressful')\naxes[0,0].bar(df7_health_age1['GEN_015'].unique()-0.\n\u21aa1,df7_health_age1['proportion'][1::5],width=0.1, label='Not very stressful')\naxes[0,0].bar(df7_health_age1['GEN_015'].\n\u21aaunique(),df7_health_age1['proportion'][2::5],width=0.1, label='A bit\u2423\n\u21aastressful')\naxes[0,0].bar(df7_health_age1['GEN_015'].unique()+0.\n\u21aa1,df7_health_age1['proportion'][3::5],width=0.1, label='Quite a bit\u2423\n\u21aastressful')\naxes[0,0].bar(df7_health_age1['GEN_015'].unique()+0.\n\u21aa2,df7_health_age1['proportion'][4::5],width=0.1, label='Extremely stressful')\naxes[0,0].set_xticks(df7_health_age1['GEN_015'].unique(),('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[0,0].set_xlabel('Stress Level by Mental Health Status 18-34')\naxes[0,0].set_ylabel('Percent')\naxes[0,0].legend()\naxes[0,1].bar(df7_health_age2['GEN_015'].unique()-0.\n\u21aa2,df7_health_age2['proportion'][::5],width=0.1,label='Not at all stressful')\naxes[0,1].bar(df7_health_age2['GEN_015'].unique()-0.\n\u21aa1,df7_health_age2['proportion'][1::5],width=0.1, label='Not very stressful')\naxes[0,1].bar(df7_health_age2['GEN_015'].\n\u21aaunique(),df7_health_age2['proportion'][2::5],width=0.1, label='A bit\u2423\n\u21aastressful')\naxes[0,1].bar(df7_health_age2['GEN_015'].unique()+0.\n\u21aa1,df7_health_age2['proportion'][3::5],width=0.1, label='Quite a bit\u2423\n\u21aastressful')\naxes[0,1].bar(df7_health_age2['GEN_015'].unique()+0.\n\u21aa2,df7_health_age2['proportion'][4::5],width=0.1, label='Extremely stressful')\naxes[0,1].set_xticks(df7_health_age2['GEN_015'].unique(),('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[0,1].set_xlabel('Stress Level by Mental Health Status 34-49')\naxes[0,1].set_ylabel('Percent')\naxes[0,1].legend()\naxes[1,0].bar(df7_health_age3['GEN_015'].unique()-0.\n\u21aa2,df7_health_age3['proportion'][::5],width=0.1,label='Not at all stressful')\naxes[1,0].bar(df7_health_age3['GEN_015'].unique()-0.\n\u21aa1,df7_health_age3['proportion'][1::5],width=0.1, label='Not very stressful')\naxes[1,0].bar(df7_health_age3['GEN_015'].\n\u21aaunique(),df7_health_age3['proportion'][2::5],width=0.1, label='A bit\u2423\n\u21aastressful')\n41"
    },
    {
        "page": 44,
        "text": "axes[1,0].bar(df7_health_age3['GEN_015'].unique()+0.\n\u21aa1,df7_health_age3['proportion'][3::5],width=0.1, label='Quite a bit\u2423\n\u21aastressful')\naxes[1,0].bar(df7_health_age3['GEN_015'].unique()+0.\n\u21aa2,df7_health_age3['proportion'][4::5],width=0.1, label='Extremely stressful')\naxes[1,0].set_xticks(df7_health_age3['GEN_015'].unique(),('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[1,0].set_xlabel('Stress Level by Mental Health Status 50-64')\naxes[1,0].set_ylabel('Percent')\naxes[1,0].legend()\naxes[1,1].bar(df7_health_age4['GEN_015'].unique()-0.\n\u21aa2,df7_health_age4['proportion'][0::5],width=0.1,label='Not at all stressful')\naxes[1,1].bar(df7_health_age4['GEN_015'].unique()-0.\n\u21aa1,df7_health_age4['proportion'][1::5],width=0.1, label='Not very stressful')\naxes[1,1].bar(df7_health_age4['GEN_015'].\n\u21aaunique(),df7_health_age4['proportion'][2::5],width=0.1, label='A bit\u2423\n\u21aastressful')\naxes[1,1].bar(df7_health_age4['GEN_015'].unique()+0.\n\u21aa1,df7_health_age4['proportion'][3::5],width=0.1, label='Quite a bit\u2423\n\u21aastressful')\naxes[1,1].bar(df7_health_age4['GEN_015'].unique()+0.\n\u21aa2,df7_health_age4['proportion'][4::5],width=0.1, label='Extremely stressful')\naxes[1,1].set_xticks(df7_health_age4['GEN_015'].unique(),('Excellent','Very\u2423\n\u21aaGood','Good','Fair','Poor'))\naxes[1,1].set_xlabel('Stress Level by Mental Health Status 65 plus')\naxes[1,1].set_ylabel('Percent')\naxes[1,1].legend()\nplt.show()\n0.4\nDiscussion\nFrom our analysis of how alcohol consumption affected mental health among different age groups,\nwe saw that individuals who moderately consume alcohol may report a better self-perceived mental\nhealth and life satisfaction. However, if alcohol consumption becomes excessive, especially in the\ncase of frequent binge drinking, we can see a noticeable decrease in self-perceived mental health\nand life satisfaction. This trend is noticeable across all age groups to varying degrees. It should\nbe noted that weak correlation between alcohol consumption and mental health variables indicates\nthat there are additional variables that play a greater role in affecting these mental health variables.\nBased on our investigation of how cannabis use impact people\u2019s perceived stress level, we found a\nweak trend that using cannabis will slightly increase perceived life stress and work stress. Due to\nthe medicinal nature of cannabis, it can affect one\u2019s nerve system in some degree. It is di\ufb00icult to\nsay how strong that effect is, as we see people who used cannabis and feeling stressful or not at\nall stressful are evenly spread out on the graph. Therefore we conclude that using cannabis can\nslightly increase the stress level in both life and work aspect, but the increase is not obvious.\n42"
    },
    {
        "page": 45,
        "text": "Overall we infer that smoking has a major impact on physical health upon our analysis against\ngeography & gender, however overweight/obese doesn\u2019t have as strong of a relationship to smoking.\nPeople who stopped smoking show greater improvements in health levels. We can also infer that\nthe BMI is greatly improved for the people with higher physical activity, and it declines for people\nwith no physical activity. Hence we conclude that Smoking is directly related to Physical health in\nconverse to BMI.\nFrom our exploration of which demographic groups are mostly affected health barriers, we found\nthat females tend to use cannabis more and smoke more frequently by approximately 8% compared\nto males. However, males consumed alcohol more often than females and binge drank morecom-\npared to females. When broken down by age, we found that a lot more teenagers (12 to 17 years)\nuse cannabis than middle aged groups, then the trend shows an immediate decrease on 18 to 34\nyears group. After that we see a increasing trend on each age group until 65 years and older reaches\nthe maximum cannabis use. This might be because some senior people need cannabis for medical\ntreatment. In terms of smoking frequency, both 12 to 17 years and 65 years and older smoke most\nfrequently, and other age groups smoke slightly less frequently than the former two age groups.\nWhen looking at different regions, there are almost equal amounts of cannabis use and smoking\nfrequency across each province except territories. This may be due to a lower population in these\nterritories. In terms of alcohol consumptions, our analysis revealed that people in Quebec and\nBritish Columbia consume alcohol most often, while people in territories consume more alcohol on\none occasion compared to the other provinces.\nWhen broken down to household education level, people with no secondary school education use\nmore cannabis than secondary and post-secondary educated people, but the differences between\nthese groups are not obvious. We found that people with post-secondary education smoke slightly\nmore often than the other two groups.\nOur analysis revealed that people with post-secondary\neducation drink alcohol most frequently, with the secondary education and no-secondary education\ngroup following closely behind. When looking at binge drinking, people with secondary and post-\nsecondary education binge drank more compared to no-secondary education group.\nFrom our investigation of how physical activity affects reported health we saw that there is a\nnoticeable trend that people who reported they had better health were more likely to be above\nthe recommended physical activity guideline. When broken down by age group we could see that\nthat trend was stronger in older populations than younger populations. Similarly, our analysis of\nhow stress affects reported mental health showed that once again respondents who reported mental\nhealth were more likely to report they had lower stress levels. While the trend continued to show\nill all age groups older age groups appeared to be more likely to select the 2 extremes to not at all\nstressed or extremely stressed. Additionally, a respondent\u2019s self perception of their weight did seem\nto relate to their mental health. To conclude these two questions, we have shown that there is at\nleast a relationship between these factors and how a respondent reported their general and mental\nhealth state.\nSome limitations of our results are that they only utilized visual inspection and correlation statistics.\nWhile correlation statistics are useful at identifying a relationship between variables, we cannot\nguarantee causation. The combination of visual inspection with correlation statistics should give\nus a better idea of this relationship, we still cannot guarantee that to be the case.\nTo better\ninvestigate these relationships, future testing could see the introduction of regression models, most\nlikely of the ordinal variety, as well as interaction terms.\n43"
    },
    {
        "page": 46,
        "text": "0.5\nConclusion\nBased on the results of our analysis we saw that alcohol consumption and smoking both resulted\nin worsening some key variables related to quality of life, those being mental and physical health.\nAlthough the strengths of these relationships varied, we can still suggest that reducing the frequency\nof drinking and smoking is likely to lead to an improved quality of life.\nWe found that cannabis use showed a weak trend indicating that cannabis slightly increases per-\nceived life and work stress, potentially due to its effects on the nervous system.\nWhile females tend to use cannabis and smoke more frequently than males, males more frequently\nconsume alcohol and binge drink. Teenagers exhibit higher cannabis use, declining in older age\ngroups, except for seniors (65+), where we see an increase, possibly due to medical use. Smoking\nand alcohol consumption also varied by region, with Quebec and British Columbia leading in alcohol\nconsumption frequency, while territories showed a higher frequency of binge drinking.\nWe can also see that regular physical activity is related to improved self-reported physical and men-\ntal health, especially in an older population. Therefore, encouraging regular exercise, particularly\nin an older population should lead to an improved quality of life.\nTo summarize, although the exact strength of our relationships is unknown, it is likely that pro-\nmoting reductions in the frequency of alcohol and smoking, in addition to encouraging physical\nactivity, may contribute to an increase in mental and physical health, and therefore improve ones\nquality of life.\n0.6\nParticipant Contributions\nParticipants contributed in the following way:\n\u2022 Aaron Gelfand: in charge of answering question 1, led team meetings, developed templates\nfor submissions, and collaborated members work into final document.\n\u2022 David Gri\ufb00in: in charge of answering questions 5 and 6, provided essential code for certain\nvisualizations.\n\u2022 David Li: in charge of answering question 2 and 4, provided valuable opinions during meetings\non guiding questions.\n\u2022 Venkateshwaran Balu Soundararajan: in charge of answering question 3, provided insight on\nmultiple fields during team meetings, including data wrangling and question development.\n0.7\nReferences\n1. Canadian\nCommunity\nHealth\nSurvey\n\u2013\nAnnual\nComponent\n(CCHS).\nGovermnent\nof\nCanada,\nStatistics\nCanada.\n(2023,\nDecember\n29).\nhttps://www23.statcan.gc.ca/imdb/p2SV.pl?Function=getSurvey&Id=1531795\n2. Canadian\nCommunity\nHealth\nSurvey:\nPublic\nUse\nMicrodata\nFile.\nGovermnent\nof\nCanada,\nStatistics\nCanada\n(2024,\nSeptember\n22).\nhttps://www150.statcan.gc.ca/n1/en/catalogue/82M0013x\n3. Haskell, W. L., Lee, I. M., Pate, R. R., Powell, K. E., Benjamin, G. A., & Flegal, K. M.\n(2007). Physical activity and public health: Updated recommendation for adults from the\n44"
    },
    {
        "page": 47,
        "text": "American College of Sports Medicine and the American Heart Association.\nCirculation,\n116(9), 1081-1093.\n4. Rehm, J., Taylor, B., & Room, R. (2006). Global burden of disease from alcohol, illicit drugs\nand tobacco. Drug and Alcohol Review, 25(6), 503-513.\n45"
    },
    {
        "page": 48,
        "text": "R Notebook\nIntroduction\nWildfires in Canada are a significant threat to ecosystems, human safety, and property, necessitating a\ncomprehensive understanding of the factors that influence their behavior. Although the prevalence of forest\nfires varies year to year, there has been an observed trend over the past years indicating an overall increase\nin the occurence of forest fires. There is also evidence of an increase in severe forest fires in recent years,\ncompared to the last few decades1.\nPart of our motivation for our questions is aimed towards looking at predicting the size and spread rate of\nfires. The 2023 and 2024 fire seasons have been some of the worst on record in terms of area burned. Being\nable to predict wildfire sizes may help in allocations of resources to fight fires. As instances such as Fort\nMcMurray and Jasper become more common with populated areas finding themselves in the way of these\nfires understanding spread rates of fires may be important to inform evacuation decisions and keep more\npeople safe.\nThe main objective of our project is to examine multiple variables related to forest fires, to determine if they\nreduce or increase the severity of forest fires. Based on this information we can make suggestions to help\ndecrease the increasing threats of forest fires.\nObjective Questions\nTo answer our main objective, we examine four main questions:\n1. Explore the relationship between wind speed and fire spread rate. Understanding this relationship can\ninform firefighting strategies and preparedness efforts.\n2. Examine the relationship between temperature and fire spread rate. Analyzing this relationship will\nprovide insights into how varying temperature levels affect wildfire behavior.\n3. Examine the relationship between temperature and fire spread rate. Analyzing this relationship will\nprovide insights into how varying temperature levels affect wildfire behavior.\n4. Examine whether fire size relates to the weather conditions when the fire starts.\nBefore any analyses, we want to make sure to load all appropriate libraries, as well as the data set.\nlibrary(mosaic)\n## Registered S3 method overwritten by \u2019mosaic\u2019:\n##\nmethod\nfrom\n##\nfortify.SpatialPolygonsDataFrame ggplot2\n##\n## The \u2019mosaic\u2019 package masks several functions from core packages in order to add\n## additional features.\nThe original behavior of these functions should not be affected by this.\n1"
    },
    {
        "page": 49,
        "text": "##\n## Attaching package: \u2019mosaic\u2019\n## The following objects are masked from \u2019package:dplyr\u2019:\n##\n##\ncount, do, tally\n## The following object is masked from \u2019package:Matrix\u2019:\n##\n##\nmean\n## The following object is masked from \u2019package:ggplot2\u2019:\n##\n##\nstat\n## The following objects are masked from \u2019package:stats\u2019:\n##\n##\nbinom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n##\nquantile, sd, t.test, var\n## The following objects are masked from \u2019package:base\u2019:\n##\n##\nmax, mean, min, prod, range, sample, sum\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)\nlibrary(leaflet)\nfire_data<-read.csv(\"https://raw.githubusercontent.com/aarongelf/data602-data/refs/heads/main/fp-histor\n#We have also include the dataset in our submission, in case there is an error accessing the URL.\nQuestion 1 - What is the Relationship Between Wind Speed and Fire Spread Rate\nTo examine the relationship between wind speed and fire spread rate, we can perform a few visual functions,\nto get an idea of the dataset itself. It is also important to note that data dictionary provided should be used\nto help interpret the columns based on their names, as well as what the values represent. The data dic-\ntionary can be found at https://open.alberta.ca/dataset/a221e7a0-4f46-4be7-9c5a-e29de9a3447e/resource/\n1b635b8b-a937-4be4-857e-8aeef77365d2/download/fp-historical-wildfire-data-dictionary-2006-2023.pdf.\nhead(fire_data)\n##\nfire_year fire_number fire_name current_size size_class\n## 1\n2006\nPWF001\n<NA>\n0.10\nA\n## 2\n2006\nEWF002\n<NA>\n0.20\nB\n## 3\n2006\nEWF001\n<NA>\n0.50\nB\n## 4\n2006\nEWF003\n<NA>\n0.01\nA\n## 5\n2006\nPWF002\n<NA>\n0.10\nA\n## 6\n2006\nCWF001\n<NA>\n0.20\nB\n##\nfire_location_latitude fire_location_longitude\nfire_origin\n2"
    },
    {
        "page": 50,
        "text": "## 1\n56.24996\n-117.1820\nPrivate Land\n## 2\n53.60637\n-115.9157\nProvincial Land\n## 3\n53.61093\n-115.5943\nProvincial Land\n## 4\n53.60887\n-115.6095\nProvincial Land\n## 5\n56.24996\n-117.0502\nProvincial Land\n## 6\n51.15293\n-115.0346 Indian Reservation\n##\ngeneral_cause_desc industry_identifier_desc\nresponsible_group_desc\n## 1\nResident\n<NA>\nResident\n## 2\nIncendiary\n<NA> Others (explain in remarks)\n## 3\nIncendiary\n<NA> Others (explain in remarks)\n## 4\nIncendiary\n<NA> Others (explain in remarks)\n## 5\nOther Industry\nWaste Disposal\nEmployees\n## 6\nResident\n<NA>\nResident\n##\nactivity_class\ntrue_cause\nfire_start_date det_agent_type det_agent\n## 1\nGrass\nPermit Related 2006-04-02 12:00\nUNP\n310\n## 2 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\n310\n## 3 Lighting Fires Arson Suspected 2006-04-03 12:15\nUNP\n310\n## 4 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\nPUB\n## 5\nRefuse\nPermit Related 2006-04-03 17:00\nUNP\nLFS\n## 6\nUnclassified\nUnsafe Fire 2006-04-02 14:25\nUNP\n310\n##\ndiscovered_date discovered_size\nreported_date dispatched_resource\n## 1\n<NA>\nNA 2006-04-02 20:46\nFPD Staff\n## 2\n<NA>\nNA 2006-04-03 12:27\nFPD Staff\n## 3\n<NA>\nNA 2006-04-03 12:36\nFPD Staff\n## 4\n<NA>\nNA 2006-04-03 13:23\nFPD Staff\n## 5 2006-04-03 19:11\nNA 2006-04-03 19:12\nFPD Staff\n## 6 2006-04-02 14:27\nNA 2006-04-02 14:30\nFPD Staff\n##\ndispatch_date start_for_fire_date assessment_resource assessment_datetime\n## 1 2006-04-02 21:10\n2006-04-02 21:20\nIA Forces\n2006-04-02 22:00\n## 2 2006-04-03 12:33\n2006-04-03 12:35\nIA Forces\n2006-04-03 13:20\n## 3 2006-04-03 12:36\n2006-04-03 12:42\nIA Forces\n2006-04-03 13:23\n## 4 2006-04-03 13:50\n2006-04-03 13:50\nIA Forces\n2006-04-03 14:08\n## 5 2006-04-03 19:19\n2006-04-03 19:22\nOther\n2006-04-03 19:57\n## 6 2006-04-02 14:40\n2006-04-02 14:45\nIA Forces\n2006-04-02 16:00\n##\nassessment_hectares fire_spread_rate fire_type fire_position_on_slope\n## 1\n0.01\n0.0\nSurface\nFlat\n## 2\n0.20\n0.0\nSurface\nLower 1/3\n## 3\n0.50\n0.0\nSurface\nBottom\n## 4\n0.01\n0.0\nSurface\nFlat\n## 5\n0.10\n0.1\nSurface\nFlat\n## 6\n0.20\n0.0\nSurface\nFlat\n##\nweather_conditions_over_fire temperature relative_humidity wind_direction\n## 1\nClear\n18\n10\nSW\n## 2\nClear\n12\n22\nSW\n## 3\nClear\n12\n22\nSW\n## 4\nClear\n12\n22\nSW\n## 5\nClear\n6\n37\nSW\n## 6\nClear\n11\n32\nS\n##\nwind_speed fuel_type initial_action_by ia_arrival_at_fire_date ia_access\n## 1\n2\nO1a\nLand Owner\n<NA>\n<NA>\n## 2\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 3\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 4\n10\nO1b\nIndustry\n<NA>\n<NA>\n## 5\n2\n<NA>\nFire Department\n<NA>\n<NA>\n3"
    },
    {
        "page": 51,
        "text": "## 6\n20\nO1b\nFire Department\n<NA>\n<NA>\n##\nfire_fighting_start_date fire_fighting_start_size bucketing_on_fire\n## 1\n<NA>\nNA\n<NA>\n## 2\n<NA>\nNA\n<NA>\n## 3\n<NA>\nNA\n<NA>\n## 4\n<NA>\nNA\n<NA>\n## 5\n<NA>\nNA\n<NA>\n## 6\n<NA>\nNA\n<NA>\n##\ndistance_from_water_source first_bucket_drop_date\nbh_fs_date\n## 1\nNA\n<NA> 2006-04-02 22:00\n## 2\nNA\n<NA> 2006-04-03 13:20\n## 3\nNA\n<NA> 2006-04-03 13:23\n## 4\nNA\n<NA> 2006-04-03 14:08\n## 5\nNA\n<NA> 2006-04-03 19:57\n## 6\nNA\n<NA> 2006-04-02 16:00\n##\nbh_hectares\nuc_fs_date uc_hectares\nto_fs_date to_hectares\n## 1\n0.01 2006-04-02 22:00\n0.01\n<NA>\nNA\n## 2\n0.20 2006-04-03 13:20\n0.20\n<NA>\nNA\n## 3\n0.50 2006-04-03 13:23\n0.50\n<NA>\nNA\n## 4\n0.01 2006-04-03 14:08\n0.01\n<NA>\nNA\n## 5\n0.10 2006-04-03 20:19\n0.10 2006-04-03 20:20\n0.1\n## 6\n0.20 2006-04-02 16:00\n0.20\n<NA>\nNA\n##\nex_fs_date ex_hectares\n## 1 2006-04-03 10:20\n0.10\n## 2 2006-04-03 14:00\n0.20\n## 3 2006-04-03 15:00\n0.50\n## 4 2006-04-03 15:05\n0.01\n## 5 2006-04-05 10:18\n0.10\n## 6 2006-04-03 18:00\n0.20\ncolnames(fire_data)\n##\n[1] \"fire_year\"\n\"fire_number\"\n##\n[3] \"fire_name\"\n\"current_size\"\n##\n[5] \"size_class\"\n\"fire_location_latitude\"\n##\n[7] \"fire_location_longitude\"\n\"fire_origin\"\n##\n[9] \"general_cause_desc\"\n\"industry_identifier_desc\"\n## [11] \"responsible_group_desc\"\n\"activity_class\"\n## [13] \"true_cause\"\n\"fire_start_date\"\n## [15] \"det_agent_type\"\n\"det_agent\"\n## [17] \"discovered_date\"\n\"discovered_size\"\n## [19] \"reported_date\"\n\"dispatched_resource\"\n## [21] \"dispatch_date\"\n\"start_for_fire_date\"\n## [23] \"assessment_resource\"\n\"assessment_datetime\"\n## [25] \"assessment_hectares\"\n\"fire_spread_rate\"\n## [27] \"fire_type\"\n\"fire_position_on_slope\"\n## [29] \"weather_conditions_over_fire\" \"temperature\"\n## [31] \"relative_humidity\"\n\"wind_direction\"\n## [33] \"wind_speed\"\n\"fuel_type\"\n## [35] \"initial_action_by\"\n\"ia_arrival_at_fire_date\"\n## [37] \"ia_access\"\n\"fire_fighting_start_date\"\n## [39] \"fire_fighting_start_size\"\n\"bucketing_on_fire\"\n## [41] \"distance_from_water_source\"\n\"first_bucket_drop_date\"\n## [43] \"bh_fs_date\"\n\"bh_hectares\"\n4"
    },
    {
        "page": 52,
        "text": "## [45] \"uc_fs_date\"\n\"uc_hectares\"\n## [47] \"to_fs_date\"\n\"to_hectares\"\n## [49] \"ex_fs_date\"\n\"ex_hectares\"\nBased on initial inspection, we are only interested in a few columns, therefore we will create a new data\nframe, focusing on variables that will help examine the relationship between wind speed and fire spread rate.\nfire_data_1=fire_data %>%\nselect(wind_speed,fire_spread_rate)\nsum(is.na(fire_data_1))\n## [1] 5575\nsum(sapply(fire_data_1[c(\"fire_spread_rate\",\"wind_speed\")],is.na))\n## [1] 5575\n#Remove rows with NA values.\nWe won't include fuel_type for this part, as we are not initially concern\nfire_clean=na.omit(fire_data_1[,c(\"wind_speed\",\"fire_spread_rate\")])\nsum(is.na(fire_clean))\n## [1] 0\nsummary(fire_clean)\n##\nwind_speed\nfire_spread_rate\n##\nMin.\n: 0.000\nMin.\n: -1.0000\n##\n1st Qu.: 3.000\n1st Qu.:\n0.0000\n##\nMedian : 6.000\nMedian :\n0.0000\n##\nMean\n: 8.813\nMean\n:\n0.8962\n##\n3rd Qu.:12.000\n3rd Qu.:\n1.0000\n##\nMax.\n:90.000\nMax.\n:100.0000\nBased on our summary statistics, we can see that the minimum value for fire_spread_rate is -1. This seems\npeculiar, and warrants a bit of further investigation, therefore we will go back to the original data set and\ninspect any rows where fire_spread_rate is -1.\nnegative_fire_spread_data <- fire_data[!is.na(fire_data$fire_spread_rate) & fire_data$fire_spread_rate \nhead(negative_fire_spread_data)\n##\nfire_year fire_number fire_name current_size size_class\n## 12962\n2014\nRWF022\n<NA>\n0.20\nB\n## 13717\n2015\nHWF100\n<NA>\n0.02\nA\n## 18213\n2017\nMWF091\n<NA>\n0.10\nA\n## 20474\n2020\nCWF038\n<NA>\n0.01\nA\n## 20778\n2019\nSWF063\n<NA>\n0.01\nA\n## 21355\n2019\nSWF092\n<NA>\n0.04\nA\n##\nfire_location_latitude fire_location_longitude\nfire_origin\n## 12962\n52.37868\n-115.6254\nProvincial Land\n## 13717\n58.48312\n-114.4696 Indian Reservation\n5"
    },
    {
        "page": 53,
        "text": "## 18213\n56.83563\n-111.7322\nProvincial Land\n## 20474\n51.10135\n-115.3091\nProvincial Land\n## 20778\n55.94107\n-113.7807 Indian Reservation\n## 21355\n56.79307\n-114.7125\nProvincial Land\n##\ngeneral_cause_desc industry_identifier_desc\nresponsible_group_desc\n## 12962\nUndetermined\n<NA>\n<NA>\n## 13717\nIncendiary\n<NA>\n<NA>\n## 18213\nRecreation\n<NA> Others (explain in remarks)\n## 20474\nRecreation\n<NA>\nCampers\n## 20778\nIncendiary\n<NA>\n<NA>\n## 21355\nLightning\n<NA>\n<NA>\n##\nactivity_class\ntrue_cause\nfire_start_date det_agent_type\n## 12962\n<NA>\n<NA>\n2014-05-24 4:00\nLKT\n## 13717\nUnclassified\n<NA> 2015-05-18 10:48\nLKT\n## 18213\nOHV Operation Burning Substance 2017-09-16 16:38\nAIR\n## 20474 Cooking and Warming\nUnsafe Fire 2020-06-27 18:00\nUNP\n## 20778\nArson\n<NA>\n2019-05-22 7:00\nUNP\n## 21355\n<NA>\n<NA> 2019-06-02 15:40\nUNP\n##\ndet_agent\ndiscovered_date discovered_size\nreported_date\n## 12962\nRA\n2014-05-24 7:28\nNA\n2014-05-24 7:32\n## 13717\nFG 2015-05-18 10:48\nNA 2015-05-18 10:50\n## 18213\nHAC 2017-09-16 16:45\nNA 2017-09-16 16:45\n## 20474\n310\n<NA>\nNA 2020-06-28 10:09\n## 20778\nLFS\n<NA>\nNA\n2019-05-22 7:14\n## 21355\nPUB 2019-06-02 15:50\nNA 2019-06-02 15:50\n##\ndispatched_resource\ndispatch_date start_for_fire_date\n## 12962\nHAC\n2014-05-24 8:14\n2014-05-24 8:29\n## 13717\nFPD Staff 2015-05-18 11:00\n2015-05-18 11:00\n## 18213\nHAC 2017-09-16 16:45\n2017-09-16 16:45\n## 20474\nHAC 2020-06-28 14:30\n2020-06-28 15:15\n## 20778\nHAC\n2019-05-22 9:27\n2019-05-22 9:49\n## 21355\nHAC 2019-06-02 15:59\n2019-06-02 16:00\n##\nassessment_resource assessment_datetime assessment_hectares\n## 12962\nIA Forces\n2014-05-24 10:35\n0.20\n## 13717\nOther\n2015-05-18 11:16\n0.02\n## 18213\nIA Forces\n2017-09-16 16:45\n0.10\n## 20474\nIA Forces\n2020-06-28 18:00\n0.01\n## 20778\nIA Forces\n2019-05-22 10:06\n0.01\n## 21355\nIA Forces\n2019-06-02 16:05\n0.04\n##\nfire_spread_rate fire_type fire_position_on_slope\n## 12962\n-1\nGround\nFlat\n## 13717\n-1\nSurface\nFlat\n## 18213\n-1\nSurface\nBottom\n## 20474\n-1\nSurface\nBottom\n## 20778\n-1\nGround\nFlat\n## 21355\n-1\nSurface\nFlat\n##\nweather_conditions_over_fire temperature relative_humidity wind_direction\n## 12962\nRainshowers\n10.5\n73\nN\n## 13717\nClear\n20.0\n22\nSE\n## 18213\nClear\n15.6\n32\nS\n## 20474\nCloudy\n11.0\n75\nW\n## 20778\nClear\n17.0\n30\nE\n## 21355\nCloudy\n18.0\n41\nW\n##\nwind_speed fuel_type initial_action_by ia_arrival_at_fire_date\n6"
    },
    {
        "page": 54,
        "text": "## 12962\n1\nS1\nIndustry\n<NA>\n## 13717\n10\nD1\nFPD Staff\n2015-05-18 11:14\n## 18213\n20\nO1b\nHAC\n2017-09-16 16:51\n## 20474\n2\n<NA>\nHAC\n2020-06-28 17:53\n## 20778\n5\nM2\nHAC\n2019-05-22 10:06\n## 21355\n15\nC2\nHAC\n2019-06-02 16:05\n##\nia_access fire_fighting_start_date fire_fighting_start_size\n## 12962\n<NA>\n<NA>\nNA\n## 13717\n<NA>\n2015-05-18 11:16\n0.02\n## 18213 Conventional R/W\n2017-09-16 16:57\n0.10\n## 20474\nGround\n2020-06-28 18:00\n0.01\n## 20778\nGround\n2019-05-22 10:13\n0.01\n## 21355 Conventional R/W\n2019-06-02 16:10\n1.00\n##\nbucketing_on_fire distance_from_water_source first_bucket_drop_date\n## 12962\n<NA>\nNA\n<NA>\n## 13717\nY\n0.2\n2015-05-18 11:16\n## 18213\nN\nNA\n<NA>\n## 20474\nN\nNA\n<NA>\n## 20778\nN\nNA\n<NA>\n## 21355\nY\n0.1\n2019-06-02 16:25\n##\nbh_fs_date bh_hectares\nuc_fs_date uc_hectares to_fs_date\n## 12962 2014-05-24 10:35\n0.20 2014-05-24 11:40\n0.20\n<NA>\n## 13717 2015-05-18 11:16\n0.02 2015-05-18 11:50\n0.02\n<NA>\n## 18213 2017-09-16 17:09\n0.10 2017-09-16 17:55\n0.20\n<NA>\n## 20474 2020-06-28 18:00\n0.01 2020-06-28 18:00\n0.01\n<NA>\n## 20778 2019-05-22 10:06\n0.01 2019-05-22 10:06\n0.01\n<NA>\n## 21355 2019-06-02 16:05\n0.04 2019-06-02 19:21\n0.04\n<NA>\n##\nto_hectares\nex_fs_date ex_hectares\n## 12962\nNA 2014-05-24 14:31\n0.20\n## 13717\nNA 2015-05-18 13:00\n0.02\n## 18213\nNA 2017-09-17 10:50\n0.10\n## 20474\nNA 2020-06-28 19:56\n0.01\n## 20778\nNA 2019-05-22 10:25\n0.01\n## 21355\nNA 2019-06-02 19:50\n0.04\nUpon visual inspection there does not seem to be any pattern related to the fire_spread_rate being -1.\nBased on this, and the definition provided in the data dictionary, with fire_spread_rate being \u2018The rate of\nspread of the wildfire at the time of initial assessment, capture in metres per minute\u2019, we felt it was safe to\nremove these rows, as this is most likely an error with these entries. For the fire to have a negative spread\nrate, would mean that the fire is retreating instead of spreading, and given that this rate of spread is a\nmeasure of how fast the fire moves from a point of origin, this seems counter intuitive to how forest fires\nwork. Given more time, we could reach out to the providers of the data, to try to clarify this area, but for\nthe time being, and since there are only 6 data points, we will remove them.\nAfter we remove the rows with a fire spread rate of -1, we can plot the data for a preliminary visualization.\nfire_clean_no_neg=fire_clean[fire_clean['fire_spread_rate']>=0,]\nggplot(fire_clean_no_neg, aes(x = fire_spread_rate)) +\ngeom_histogram(color='red',fill='red')\n## \u2018stat_bin()\u2018 using \u2018bins = 30\u2018. Pick better value with \u2018binwidth\u2018.\n7"
    },
    {
        "page": 55,
        "text": "0\n5000\n10000\n15000\n20000\n0\n25\n50\n75\n100\nfire_spread_rate\ncount\nBased on our histogram, we can see that the data for fire_spread_rate is heavily skewed.\nfire_clean_no_neg=fire_clean[fire_clean['fire_spread_rate']>=0,]\nggplot(fire_clean_no_neg, aes(x = wind_speed, y = fire_spread_rate)) +\ngeom_point(color='red',alpha = 0.5) +\nlabs(title = \"Relationship between Fire Spread Rate and Wind Speed\",\nx = \"Wind Speed (kilometers per hour)\",\n# Replace with actual units if known\ny = \"Fire Spread Rate (metres per minute)\")\n8"
    },
    {
        "page": 56,
        "text": "0\n25\n50\n75\n100\n0\n25\n50\n75\nWind Speed (kilometers per hour)\nFire Spread Rate (metres per minute)\nRelationship between Fire Spread Rate and Wind Speed\nInitial inspection of the scatterplot is difficult to arrive to any meaningful conclusion without further\nanalysis.\nAdditionally, we will look at the correlation coefficient between fire_spread_rate and wind_speed.\nfire_corr=cor(fire_clean_no_neg,use=\"pairwise.complete.obs\")\nprint(fire_corr)\n##\nwind_speed fire_spread_rate\n## wind_speed\n1.0000000\n0.1346716\n## fire_spread_rate\n0.1346716\n1.0000000\nBased on our output we can see a very weak positive relationship between fire spread rate and wind speed.\nfire_no_neg_model=lm(fire_spread_rate ~ wind_speed, data = fire_clean_no_neg)\nsummary(fire_no_neg_model)\n##\n## Call:\n## lm(formula = fire_spread_rate ~ wind_speed, data = fire_clean_no_neg)\n##\n## Residuals:\n##\nMin\n1Q Median\n3Q\nMax\n## -4.263 -0.863 -0.597\n0.054 99.261\n##\n9"
    },
    {
        "page": 57,
        "text": "## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept) 0.531266\n0.024815\n21.41\n<2e-16 ***\n## wind_speed\n0.041468\n0.002035\n20.38\n<2e-16 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.571 on 22478 degrees of freedom\n## Multiple R-squared:\n0.01814,\nAdjusted R-squared:\n0.01809\n## F-statistic: 415.2 on 1 and 22478 DF,\np-value: < 2.2e-16\nIntercept: The expected value of fire_spread_rate when wind_speed is zero is 0.53130. As our p-value is\n<0.05, this indicates that we can reject the H0 that \u03b20=0. Therefore, we accept the H1 that \u03b20 \u0338= 0 and\nconclude that the intercept is statistically significant.\nSlope: For each additional kilometer per hour in wind_speed, the fire_spread_rate is expected to increase\nby approximately 0.04150 meters per minute. As our p-value is <0.05, this indicates that we can reject the\nH0 that \u03b21=0. Therefore, we accept the H1 that \u03b21 \u0338= 0 and conclude that there is a significant relationship\nbetween wind_speed and fire_spread_rate.\nBased on our output table, the equation for our model can be written out as, fire_spread_rate = 0.53130+\n(0.04150 \u2217 wind_speed)\nOur R-squared value indicates that approximately 1.81% of the variance in fire_spread_rate is explained by\nwind_speed. This low value suggests that there are other factors affecting fire spread that are not included\nin our model.\nWe can plot this model using the following code:\nggplot(fire_clean_no_neg, aes(x = wind_speed, y = fire_spread_rate)) +\ngeom_point(alpha = 0.5,color='red') +\nstat_smooth(method = \"lm\", formula = y~x) +\n# Add regression line\nlabs(title = \"Relationship between Fire Spread Rate and Wind Speed\",\nx = \"Wind Speed (km/h)\",\ny = \"Fire Spread Rate (m/min)\") +\ntheme_minimal()\n10"
    },
    {
        "page": 58,
        "text": "0\n25\n50\n75\n100\n0\n25\n50\n75\nWind Speed (km/h)\nFire Spread Rate (m/min)\nRelationship between Fire Spread Rate and Wind Speed\nFrom our output, we can see a weak relationship between wind_speed and fire_spread_rate. as suggested\nby our correlation statistics.\nTo determine whether the assumptions of independency and normality are met, we can plot the residual and\nQQ plots.\n#Residual plot\nplot(fire_no_neg_model, which =1)\n11"
    },
    {
        "page": 59,
        "text": "1\n2\n3\n4\n0\n20\n40\n60\n80\n100\nFitted values\nResiduals\nlm(fire_spread_rate ~ wind_speed)\nResiduals vs Fitted\n3130\n18649\n10693\n#QQ-plot\nplot(fire_no_neg_model, which =2)\n12"
    },
    {
        "page": 60,
        "text": "\u22124\n\u22122\n0\n2\n4\n0\n10\n20\n30\n40\nTheoretical Quantiles\nStandardized residuals\nlm(fire_spread_rate ~ wind_speed)\nQ\u2212Q Residuals\n3130\n18649\n10693\nFor our residual plot we see a large cluster of points on the upper left side of the figure, as well as some\noutliers. For our QQ plot, we see that the points stray off far from the line towards the right side of the\nfigure. Based on these observation, we can suggest that both assumptions of independency and normality\nof residuals fails.\nBy failing both of these assumptions, it suggests issues in our model that can lead to unreliable results.\nSome potential solutions are to transform the data, or to include interaction terms.\nQuestion 2 - What is the Relationship Between Temperature and Fire Spread Rate\nIn this project, we explored the relationship between temperature and fire spread rate in Canada. We visu-\nalized the distribution of fire spread rate and temperature, calculated the correlation coefficient, performed\na linear regression analysis, and conducted a hypothesis test to determine whether the observed relationship\nis statistically significant. Additionally, we created a geospatial representation of fire spread rate along with\ntemperature to better understand the spatial patterns and relationships between these variables.\nThe results of the hypothesis test indicate whether there is a statistically significant relationship between\ntemperature and fire spread rate.\ndata=fire_data\n# Filter out rows where Fire Spread Rate is negative\ndata<- data %>%\nfilter(fire_spread_rate >= 0)\n# Create a scatter plot\nggplot(data, aes(x = temperature, y = fire_spread_rate)) +\ngeom_point(color = \"red\", size = 2) +\n13"
    },
    {
        "page": 61,
        "text": "#geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\nlabs(title = \"Relationship between Fire Spread Rate and Temperature\",\nx = \"Fire Spread Rate\",\ny = \"Temperature\") +\ntheme_classic()\n## Warning: Removed 80 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n0\n25\n50\n75\n100\n\u221240\n\u221220\n0\n20\n40\nFire Spread Rate\nTemperature\nRelationship between Fire Spread Rate and Temperature\nEDA\n# Check the structure of the data\nstr(data)\n## \u2019data.frame\u2019:\n22562 obs. of\n50 variables:\n##\n$ fire_year\n: int\n2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 ...\n##\n$ fire_number\n: chr\n\"PWF001\" \"EWF002\" \"EWF001\" \"EWF003\" ...\n##\n$ fire_name\n: chr\nNA NA NA NA ...\n##\n$ current_size\n: num\n0.1 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ size_class\n: chr\n\"A\" \"B\" \"B\" \"A\" ...\n##\n$ fire_location_latitude\n: num\n56.2 53.6 53.6 53.6 56.2 ...\n14"
    },
    {
        "page": 62,
        "text": "##\n$ fire_location_longitude\n: num\n-117 -116 -116 -116 -117 ...\n##\n$ fire_origin\n: chr\n\"Private Land\" \"Provincial Land\" \"Provincial Land\" \"Provincial\n##\n$ general_cause_desc\n: chr\n\"Resident\" \"Incendiary\" \"Incendiary\" \"Incendiary\" ...\n##\n$ industry_identifier_desc\n: chr\nNA NA NA NA ...\n##\n$ responsible_group_desc\n: chr\n\"Resident\" \"Others (explain in remarks)\" \"Others (explain in r\n##\n$ activity_class\n: chr\n\"Grass\" \"Lighting Fires\" \"Lighting Fires\" \"Lighting Fires\" ...\n##\n$ true_cause\n: chr\n\"Permit Related\" \"Arson Suspected\" \"Arson Suspected\" \"Arson Su\n##\n$ fire_start_date\n: chr\n\"2006-04-02 12:00\" \"2006-04-03 12:10\" \"2006-04-03 12:15\" \"2006\n##\n$ det_agent_type\n: chr\n\"UNP\" \"UNP\" \"UNP\" \"UNP\" ...\n##\n$ det_agent\n: chr\n\"310\" \"310\" \"310\" \"PUB\" ...\n##\n$ discovered_date\n: chr\nNA NA NA NA ...\n##\n$ discovered_size\n: num\nNA NA NA NA NA NA NA NA NA NA ...\n##\n$ reported_date\n: chr\n\"2006-04-02 20:46\" \"2006-04-03 12:27\" \"2006-04-03 12:36\" \"2006\n##\n$ dispatched_resource\n: chr\n\"FPD Staff\" \"FPD Staff\" \"FPD Staff\" \"FPD Staff\" ...\n##\n$ dispatch_date\n: chr\n\"2006-04-02 21:10\" \"2006-04-03 12:33\" \"2006-04-03 12:36\" \"2006\n##\n$ start_for_fire_date\n: chr\n\"2006-04-02 21:20\" \"2006-04-03 12:35\" \"2006-04-03 12:42\" \"2006\n##\n$ assessment_resource\n: chr\n\"IA Forces\" \"IA Forces\" \"IA Forces\" \"IA Forces\" ...\n##\n$ assessment_datetime\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ assessment_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ fire_spread_rate\n: num\n0 0 0 0 0.1 0 0 0 0 0 ...\n##\n$ fire_type\n: chr\n\"Surface\" \"Surface\" \"Surface\" \"Surface\" ...\n##\n$ fire_position_on_slope\n: chr\n\"Flat\" \"Lower 1/3\" \"Bottom\" \"Flat\" ...\n##\n$ weather_conditions_over_fire: chr\n\"Clear\" \"Clear\" \"Clear\" \"Clear\" ...\n##\n$ temperature\n: num\n18 12 12 12 6 11 11 16 11 11 ...\n##\n$ relative_humidity\n: int\n10 22 22 22 37 32 25 17 35 44 ...\n##\n$ wind_direction\n: chr\n\"SW\" \"SW\" \"SW\" \"SW\" ...\n##\n$ wind_speed\n: int\n2 10 10 10 2 20 10 2 7 4 ...\n##\n$ fuel_type\n: chr\n\"O1a\" \"O1a\" \"O1a\" \"O1b\" ...\n##\n$ initial_action_by\n: chr\n\"Land Owner\" \"Fire Department\" \"Fire Department\" \"Industry\" ..\n##\n$ ia_arrival_at_fire_date\n: chr\nNA NA NA NA ...\n##\n$ ia_access\n: chr\nNA NA NA NA ...\n##\n$ fire_fighting_start_date\n: chr\nNA NA NA NA ...\n##\n$ fire_fighting_start_size\n: num\nNA NA NA NA NA NA NA 0.01 NA 0.6 ...\n##\n$ bucketing_on_fire\n: chr\nNA NA NA NA ...\n##\n$ distance_from_water_source\n: num\nNA NA NA NA NA NA NA NA NA NA ...\n##\n$ first_bucket_drop_date\n: chr\nNA NA NA NA ...\n##\n$ bh_fs_date\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ bh_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ uc_fs_date\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ uc_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ to_fs_date\n: chr\nNA NA NA NA ...\n##\n$ to_hectares\n: num\nNA NA NA NA 0.1 NA NA 0.01 0.2 NA ...\n##\n$ ex_fs_date\n: chr\n\"2006-04-03 10:20\" \"2006-04-03 14:00\" \"2006-04-03 15:00\" \"2006\n##\n$ ex_hectares\n: num\n0.1 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n# View unique values in Temperature and FireSpreadRate\nunique(data$temperature)\n##\n[1]\n18.0\n12.0\n6.0\n11.0\n16.0\n28.0\n26.0\n25.0\n35.0\n15.0\n10.0\n13.2\n##\n[13]\n27.0\n20.0\n17.0\n22.0\n24.0\n23.0\n21.0\n29.0\n19.0\n21.4\n14.5\n14.0\n##\n[25]\n33.0\n31.0\n32.0\n4.0\n24.5\n30.0\n27.5\n22.5\n21.5\n6.5\n9.0\n13.0\n##\n[37]\n25.5\n2.0\n30.6\n17.5\n5.0\n12.5\n22.4\n36.0\n26.4\n18.7\n16.1\n21.6\n##\n[49]\n16.5\n19.5\n7.0\n24.6\n-0.6\n20.5\n23.5\n24.3\n22.1\n18.4\n23.7\n26.5\n##\n[61]\n8.0\n15.6\n22.8\n12.8\n13.5\n29.5\n14.2\n8.3\n11.5\n8.5\n13.3\n17.6\n15"
    },
    {
        "page": 63,
        "text": "##\n[73]\n26.2\n19.6\n9.4\n7.5\n16.7\n21.2\n5.4\n3.0\n1.0\n18.5\n23.2\n16.3\n##\n[85]\n24.8\n26.7\n19.8\n31.5\n25.6\n28.2\n28.7\n20.2\n22.7\n17.3\n30.8\n24.7\n##\n[97]\n23.6\n17.4\n22.3\n28.6 -18.0\n0.1\n-1.0\n30.9\n-8.0\n-7.0\n11.2\n0.0\n## [109]\n19.4 -10.0\n-5.0\n6.2\n11.7\n-2.0\n14.4\n21.1\n25.7\n26.6\n10.8\n15.3\n## [121]\n0.2\n-7.3\n-4.0\n7.6\n8.9 -11.0\n28.5\n13.6\n30.7\n-6.0 -15.0\n-3.0\n## [133]\n5.5\n26.1\n9.5\n14.9\n15.5\n13.4\n16.9\n24.4\n17.8\n19.9\n15.4\n26.8\n## [145]\n25.4\n21.8\n18.2\n27.2\n-3.5 -12.0 -14.0 -13.0\n20.9\n2.5\n14.6\n12.1\n## [157]\n16.2\n12.2\n19.2\n19.1\n10.6\n-9.0\n8.2\n6.6\n20.6\n17.7\n3.2\n20.7\n## [169]\n18.6\n8.6\n20.1\n15.2\n11.8\n22.6\n28.3\n3.5\n-2.7\n6.1\n4.8\n13.8\n## [181]\n10.5\n2.4\n14.3\n28.9\n34.0 -21.0\n26.3 -18.5\n7.4\n22.2\n11.9\n24.1\n## [193]\n14.8\n20.3\n18.3\n29.1\n8.7\n7.2\n5.1\n27.8\n15.7 -20.0\n31.6\n2.6\n## [205]\n5.7\n13.7\n12.4\n27.4 -33.0\n8.1\n9.1\nNA -25.0\n8.8\n30.5\n14.1\n## [217]\n19.7\n6.4\n4.5\n10.4\n24.9\n24.2 -17.0\n10.2\n13.9\n18.1\n23.9 -22.0\n## [229]\n10.3 -19.0\n8.4 -16.0 -30.0\n14.7\n23.4\n20.4\n19.3\n32.1\n16.6\n12.3\n## [241]\n18.8\n25.8\n37.0\n1.5\n16.4\n-1.5\n25.2\n5.6\n29.4\n21.3\n27.6\n28.1\n## [253]\n27.1\n9.3\n18.9\n12.7\n6.3\n27.7\n32.5\n23.8\n33.2\n25.3\n20.8\n17.9\n## [265]\n23.3\n27.9\n29.3\n15.9\n21.9\n17.1\n21.7\n28.4\n17.2\n15.1\n16.8\n0.5\n## [277]\n32.4\n29.8\n30.4\n39.0 -35.0 -34.0\n2.3\n13.1\n28.8\n3.3\n6.7\n23.1\n## [289]\n2.2\n29.2\n11.6\n5.3\n-2.5\n29.6\n33.4\n27.3\n11.1\n1.7\n31.8\n22.9\n## [301]\n15.8\n9.7\n29.7\n9.2\n31.7\n-3.4\n35.4\n31.2\n7.3\n30.1\n1.3\n12.6\n## [313]\n5.8\n25.9\n1.9\n31.3\n6.8\n0.7\n3.6\n3.7\n26.9 -23.0\n25.1\n38.0\n## [325]\n0.3\n34.5\n35.3\n29.9\n39.9\n9.9\n38.1\n1.6\n11.3\n32.6\n5.9\n32.7\n## [337]\n37.5\n4.7\n12.9 -39.0\n-0.2\n10.1\n7.9\n9.8\nunique(data$fire_spread_rate)\n##\n[1]\n0.0\n0.1\n1.0\n12.0\n0.5\n1.5\n3.0\n2.0\n5.0\n10.0\n0.2\n35.0\n## [13]\n4.0\n30.0\n0.4\n50.0\n0.9\n8.0\n20.0\n3.5\n7.0\n0.3\n6.0\n11.0\n## [25]\n17.0\n9.0\n25.0\n18.0\n40.0\n2.5 100.0\n0.7\n15.0\n0.8\n1.2\n1.1\n## [37]\n1.8\n1.9\n13.0\n4.5\n1.7\n8.5\n70.0\n0.6\n2.2\n4.9\n16.0\n65.0\n## [49]\n21.0\n5.7\n34.0\n71.0\n19.0\n60.0\n22.0\n5.5\nWe will also look at a summary of the data\nsummary(data$temperature)\n##\nMin. 1st Qu.\nMedian\nMean 3rd Qu.\nMax.\nNA\u2019s\n##\n-39.00\n14.00\n19.00\n17.85\n23.00\n39.90\n80\nsummary(data$fire_spread_rate)\n##\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n##\n0.0000\n0.0000\n0.0000\n0.8962\n1.0000 100.0000\n# Plot a histogram\nggplot(data, aes(x = fire_spread_rate)) +\ngeom_histogram(binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.7) +\nlabs(title = \"Distribution of Fire Spread Rate\",\nx = \"Fire Spread Rate\",\ny = \"Frequency\") +\ntheme_classic()\n16"
    },
    {
        "page": 64,
        "text": "0\n5000\n10000\n15000\n0\n25\n50\n75\n100\nFire Spread Rate\nFrequency\nDistribution of Fire Spread Rate\n# Alternatively, plot a density plot\nggplot(data, aes(x = fire_spread_rate)) +\ngeom_density(fill = \"blue\", alpha = 0.5) +\nlabs(title = \"Density Plot of Fire Spread Rate\",\nx = \"Fire Spread Rate\",\ny = \"Density\") +\ntheme_classic()\n17"
    },
    {
        "page": 65,
        "text": "0.0\n0.5\n1.0\n1.5\n2.0\n0\n25\n50\n75\n100\nFire Spread Rate\nDensity\nDensity Plot of Fire Spread Rate\n# Calculate skewness\nspread_rate_skewness <- skewness(data$fire_spread_rate)\nprint(paste(\"Skewness of Fire Spread Rate:\", spread_rate_skewness))\n## [1] \"Skewness of Fire Spread Rate: 11.2632058482397\"\nThe value 11.22 suggests that the distribution of fire spread rates is heavily skewed to the right, meaning\nthat most of the fire spread rates are relatively low, but there are a few extremely high values (outliers) that\npull the tail of the distribution to the right.\nBefore performing a regression analysis, it is important to investigate the correlation between both variables.\nTo normalize the distribution of the fire spread rate we can attempt a log transformation\n# Log Transformation\ndata$log_fire_spread_rate <- log(data$fire_spread_rate + 1)\nlog_spread_rate_skewness <- skewness(data$log_fire_spread_rate)\nprint(paste(\"Skewness of Log Transformed Fire Spread Rate:\", log_spread_rate_skewness))\n## [1] \"Skewness of Log Transformed Fire Spread Rate: 1.92868885555048\"\nggplot(data, aes(x = log_fire_spread_rate)) +\ngeom_density(fill = \"blue\", alpha = 0.5) +\nlabs(title = \"Log Transformed Density Plot of Fire Spread Rate\",\n18"
    },
    {
        "page": 66,
        "text": "x = \"Log Fire Spread Rate\",\ny = \"Density\") +\ntheme_classic()\n0\n1\n2\n3\n0\n1\n2\n3\n4\nLog Fire Spread Rate\nDensity\nLog Transformed Density Plot of Fire Spread Rate\nFollowing this, we will calculate Pearson correlation between log-transformed fire spread rate and temperature\ncorrelation_log <- cor(data$log_fire_spread_rate, data$temperature, use = \"complete.obs\")\nprint(paste(\"Pearson correlation coefficient (Log Fire Spread Rate and Temperature):\", correlation_log)\n## [1] \"Pearson correlation coefficient (Log Fire Spread Rate and Temperature): 0.250089700065433\"\nWe will now rechecking correlation fire spread rate and temperature to see if the relationship has improved\nnow.\nThe regression model can be coded as:\n# Fit a linear regression model\nlog_model <- lm(log_fire_spread_rate ~ temperature, data = data)\nsummary(log_model)\n##\n## Call:\n## lm(formula = log_fire_spread_rate ~ temperature, data = data)\n##\n## Residuals:\n19"
    },
    {
        "page": 67,
        "text": "##\nMin\n1Q\nMedian\n3Q\nMax\n## -0.7690 -0.3839 -0.2119\n0.2326\n4.0652\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept) 0.0389989\n0.0096034\n4.061\n4.9e-05 ***\n## temperature 0.0191612\n0.0004948\n38.727\n< 2e-16 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 0.5651 on 22480 degrees of freedom\n##\n(80 observations deleted due to missingness)\n## Multiple R-squared:\n0.06254,\nAdjusted R-squared:\n0.0625\n## F-statistic:\n1500 on 1 and 22480 DF,\np-value: < 2.2e-16\nggplot(data, aes(x = temperature, y = log_fire_spread_rate)) +\ngeom_point(color = \"red\") +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\nlabs(title = \"Log Fire Spread Rate vs Temperature\",\nx = \"Temperature\",\ny = \"Log Fire Spread Rate\")\n## \u2018geom_smooth()\u2018 using formula = \u2019y ~ x\u2019\n## Warning: Removed 80 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 80 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n20"
    },
    {
        "page": 68,
        "text": "0\n1\n2\n3\n4\n\u221240\n\u221220\n0\n20\n40\nTemperature\nLog Fire Spread Rate\nLog Fire Spread Rate vs Temperature\nThe graph shows the relationship between temperature and the rate of fire spread. The red dots represent\ndata points where the x-axis represents the temperature and the y-axis represents the log of the fire spread\nrate. The blue line represents a line of best fit, indicating that there is a positive correlation between the two\nvariables. This means that as temperature increases, the fire spread rate also increases, and this is illustrated\nby the upward trend of the blue line.\nQ-Q Plot and Residuals Plot to check the normality\n# Q-Q plot to check normality of residuals\nqqnorm(residuals(log_model))\nqqline(residuals(log_model), col = \"blue\")\n21"
    },
    {
        "page": 69,
        "text": "\u22124\n\u22122\n0\n2\n4\n0\n1\n2\n3\n4\nNormal Q\u2212Q Plot\nTheoretical Quantiles\nSample Quantiles\n# Residuals vs Fitted plot\nplot(log_model, which = 1)\n22"
    },
    {
        "page": 70,
        "text": "\u22120.5\n0.0\n0.5\n\u22121\n0\n1\n2\n3\n4\nFitted values\nResiduals\nlm(log_fire_spread_rate ~ temperature)\nResiduals vs Fitted\n16449\n3129\n17097\n# Histogram of residuals\nhist(residuals(log_model), main = \"Histogram of Residuals\", xlab = \"Residuals\", col = \"lightblue\")\n23"
    },
    {
        "page": 71,
        "text": "Histogram of Residuals\nResiduals\nFrequency\n\u22121\n0\n1\n2\n3\n4\n0\n2000\n6000\n10000\nResidual plot: This plot suggests that the linear model is not a good fit for the data, and alternative models\nmight be considered.\nQ-Q Residual: This plot indicates that the data is not normally distributed because it deviates from a\nstraight line. The points are curved and have a few outliers. The data is skewed to the right as it deviates\nfrom the straight line on the right-hand side.\n# Get model summary and extract statistics\nlog_model_summary <- summary(log_model)\nr_squared_log <- log_model_summary$r.squared\nadj_r_squared_log <- log_model_summary$adj.r.squared\np_value_log <- log_model_summary$coefficients[2, 4]\nprint(paste(\"R-squared (log model):\", r_squared_log))\n## [1] \"R-squared (log model): 0.0625448580788178\"\nprint(paste(\"Adjusted R-squared (log model):\", adj_r_squared_log))\n## [1] \"Adjusted R-squared (log model): 0.062503156337629\"\nprint(paste(\"p-value for temperature (log model):\", p_value_log))\n## [1] \"p-value for temperature (log model): 1.1251791730511e-317\"\n24"
    },
    {
        "page": 72,
        "text": "To avoid the complexity by temperature variable Adjusted R-squared -the relationship between the temper-\nature and fire spread rate is weak, meaning temperature does not explain much of the variation in fire spread\nrate.P Value suggests that the temperature is likely statistically significant, meaning it has a real effect on\nthe dependent variable.\nTo check this since the p-value only reflects the significance of the relationship, it\u2019s also worth considering if\nother variables (like humidity , wind speed , vegetation_type\n# Extra work--Consider including other relevant features\ndata$humidity <- data$relative_humidity\ndata$wind_speed <- data$wind_speed\ndata$vegetation_type <- data$fuel_type\n# Update the model to include additional features\nmodel <- lm(fire_spread_rate ~ temperature + humidity + wind_speed + vegetation_type, data = data)\n# Summarize the updated model\nsummary(model)\n##\n## Call:\n## lm(formula = fire_spread_rate ~ temperature + humidity + wind_speed +\n##\nvegetation_type, data = data)\n##\n## Residuals:\n##\nMin\n1Q Median\n3Q\nMax\n## -4.849 -1.028 -0.475\n0.221 98.075\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n1.241233\n0.161542\n7.684 1.63e-14 ***\n## temperature\n0.031667\n0.003164\n10.009\n< 2e-16 ***\n## humidity\n-0.017181\n0.001239 -13.866\n< 2e-16 ***\n## wind_speed\n0.044757\n0.002431\n18.413\n< 2e-16 ***\n## vegetation_typeC2\n0.223939\n0.121777\n1.839 0.065942 .\n## vegetation_typeC3\n-0.246459\n0.156810\n-1.572 0.116036\n## vegetation_typeC4\n-0.443133\n0.259062\n-1.711 0.087185 .\n## vegetation_typeC7\n-0.845929\n0.621074\n-1.362 0.173202\n## vegetation_typeD1\n-0.759180\n0.176130\n-4.310 1.64e-05 ***\n## vegetation_typeM1\n-0.559852\n0.160891\n-3.480 0.000503 ***\n## vegetation_typeM2\n-0.699145\n0.130445\n-5.360 8.44e-08 ***\n## vegetation_typeM3\n-1.718422\n1.577507\n-1.089 0.276024\n## vegetation_typeM4\n-0.739332\n2.726585\n-0.271 0.786274\n## vegetation_typeO1a -0.760666\n0.127503\n-5.966 2.48e-09 ***\n## vegetation_typeO1b -0.719319\n0.134277\n-5.357 8.57e-08 ***\n## vegetation_typeS1\n-0.892669\n0.173143\n-5.156 2.55e-07 ***\n## vegetation_typeS2\n-1.039302\n0.174980\n-5.940 2.91e-09 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.724 on 17918 degrees of freedom\n##\n(4627 observations deleted due to missingness)\n25"
    },
    {
        "page": 73,
        "text": "## Multiple R-squared:\n0.07169,\nAdjusted R-squared:\n0.07086\n## F-statistic: 86.48 on 16 and 17918 DF,\np-value: < 2.2e-16\nThis thing concludes that not only one factor is responsible for the fire spread these other factors combiningly\nalso affect that.\nCreating a new column according to the size_class to represnt that in the visualization as HIgh or low spread\nin the particular area.\nstr(data)\n## \u2019data.frame\u2019:\n22562 obs. of\n53 variables:\n##\n$ fire_year\n: int\n2006 2006 2006 2006 2006 2006 2006 2006 2006 2006 ...\n##\n$ fire_number\n: chr\n\"PWF001\" \"EWF002\" \"EWF001\" \"EWF003\" ...\n##\n$ fire_name\n: chr\nNA NA NA NA ...\n##\n$ current_size\n: num\n0.1 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ size_class\n: chr\n\"A\" \"B\" \"B\" \"A\" ...\n##\n$ fire_location_latitude\n: num\n56.2 53.6 53.6 53.6 56.2 ...\n##\n$ fire_location_longitude\n: num\n-117 -116 -116 -116 -117 ...\n##\n$ fire_origin\n: chr\n\"Private Land\" \"Provincial Land\" \"Provincial Land\" \"Provincial\n##\n$ general_cause_desc\n: chr\n\"Resident\" \"Incendiary\" \"Incendiary\" \"Incendiary\" ...\n##\n$ industry_identifier_desc\n: chr\nNA NA NA NA ...\n##\n$ responsible_group_desc\n: chr\n\"Resident\" \"Others (explain in remarks)\" \"Others (explain in r\n##\n$ activity_class\n: chr\n\"Grass\" \"Lighting Fires\" \"Lighting Fires\" \"Lighting Fires\" ...\n##\n$ true_cause\n: chr\n\"Permit Related\" \"Arson Suspected\" \"Arson Suspected\" \"Arson Su\n##\n$ fire_start_date\n: chr\n\"2006-04-02 12:00\" \"2006-04-03 12:10\" \"2006-04-03 12:15\" \"2006\n##\n$ det_agent_type\n: chr\n\"UNP\" \"UNP\" \"UNP\" \"UNP\" ...\n##\n$ det_agent\n: chr\n\"310\" \"310\" \"310\" \"PUB\" ...\n##\n$ discovered_date\n: chr\nNA NA NA NA ...\n##\n$ discovered_size\n: num\nNA NA NA NA NA NA NA NA NA NA ...\n##\n$ reported_date\n: chr\n\"2006-04-02 20:46\" \"2006-04-03 12:27\" \"2006-04-03 12:36\" \"2006\n##\n$ dispatched_resource\n: chr\n\"FPD Staff\" \"FPD Staff\" \"FPD Staff\" \"FPD Staff\" ...\n##\n$ dispatch_date\n: chr\n\"2006-04-02 21:10\" \"2006-04-03 12:33\" \"2006-04-03 12:36\" \"2006\n##\n$ start_for_fire_date\n: chr\n\"2006-04-02 21:20\" \"2006-04-03 12:35\" \"2006-04-03 12:42\" \"2006\n##\n$ assessment_resource\n: chr\n\"IA Forces\" \"IA Forces\" \"IA Forces\" \"IA Forces\" ...\n##\n$ assessment_datetime\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ assessment_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ fire_spread_rate\n: num\n0 0 0 0 0.1 0 0 0 0 0 ...\n##\n$ fire_type\n: chr\n\"Surface\" \"Surface\" \"Surface\" \"Surface\" ...\n##\n$ fire_position_on_slope\n: chr\n\"Flat\" \"Lower 1/3\" \"Bottom\" \"Flat\" ...\n##\n$ weather_conditions_over_fire: chr\n\"Clear\" \"Clear\" \"Clear\" \"Clear\" ...\n##\n$ temperature\n: num\n18 12 12 12 6 11 11 16 11 11 ...\n##\n$ relative_humidity\n: int\n10 22 22 22 37 32 25 17 35 44 ...\n##\n$ wind_direction\n: chr\n\"SW\" \"SW\" \"SW\" \"SW\" ...\n##\n$ wind_speed\n: int\n2 10 10 10 2 20 10 2 7 4 ...\n##\n$ fuel_type\n: chr\n\"O1a\" \"O1a\" \"O1a\" \"O1b\" ...\n##\n$ initial_action_by\n: chr\n\"Land Owner\" \"Fire Department\" \"Fire Department\" \"Industry\" ..\n##\n$ ia_arrival_at_fire_date\n: chr\nNA NA NA NA ...\n##\n$ ia_access\n: chr\nNA NA NA NA ...\n##\n$ fire_fighting_start_date\n: chr\nNA NA NA NA ...\n##\n$ fire_fighting_start_size\n: num\nNA NA NA NA NA NA NA 0.01 NA 0.6 ...\n##\n$ bucketing_on_fire\n: chr\nNA NA NA NA ...\n##\n$ distance_from_water_source\n: num\nNA NA NA NA NA NA NA NA NA NA ...\n##\n$ first_bucket_drop_date\n: chr\nNA NA NA NA ...\n26"
    },
    {
        "page": 74,
        "text": "##\n$ bh_fs_date\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ bh_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ uc_fs_date\n: chr\n\"2006-04-02 22:00\" \"2006-04-03 13:20\" \"2006-04-03 13:23\" \"2006\n##\n$ uc_hectares\n: num\n0.01 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ to_fs_date\n: chr\nNA NA NA NA ...\n##\n$ to_hectares\n: num\nNA NA NA NA 0.1 NA NA 0.01 0.2 NA ...\n##\n$ ex_fs_date\n: chr\n\"2006-04-03 10:20\" \"2006-04-03 14:00\" \"2006-04-03 15:00\" \"2006\n##\n$ ex_hectares\n: num\n0.1 0.2 0.5 0.01 0.1 0.2 0.01 0.01 0.2 0.6 ...\n##\n$ log_fire_spread_rate\n: num\n0 0 0 0 0.0953 ...\n##\n$ humidity\n: int\n10 22 22 22 37 32 25 17 35 44 ...\n##\n$ vegetation_type\n: chr\n\"O1a\" \"O1a\" \"O1a\" \"O1b\" ...\n# Create SpreadCategory based on size_class\ndata$SpreadCategory <- ifelse(data$size_class %in% c(\"D\", \"E\"), \"High\", \"Low\")\nhead(data)\n##\nfire_year fire_number fire_name current_size size_class\n## 1\n2006\nPWF001\n<NA>\n0.10\nA\n## 2\n2006\nEWF002\n<NA>\n0.20\nB\n## 3\n2006\nEWF001\n<NA>\n0.50\nB\n## 4\n2006\nEWF003\n<NA>\n0.01\nA\n## 5\n2006\nPWF002\n<NA>\n0.10\nA\n## 6\n2006\nCWF001\n<NA>\n0.20\nB\n##\nfire_location_latitude fire_location_longitude\nfire_origin\n## 1\n56.24996\n-117.1820\nPrivate Land\n## 2\n53.60637\n-115.9157\nProvincial Land\n## 3\n53.61093\n-115.5943\nProvincial Land\n## 4\n53.60887\n-115.6095\nProvincial Land\n## 5\n56.24996\n-117.0502\nProvincial Land\n## 6\n51.15293\n-115.0346 Indian Reservation\n##\ngeneral_cause_desc industry_identifier_desc\nresponsible_group_desc\n## 1\nResident\n<NA>\nResident\n## 2\nIncendiary\n<NA> Others (explain in remarks)\n## 3\nIncendiary\n<NA> Others (explain in remarks)\n## 4\nIncendiary\n<NA> Others (explain in remarks)\n## 5\nOther Industry\nWaste Disposal\nEmployees\n## 6\nResident\n<NA>\nResident\n##\nactivity_class\ntrue_cause\nfire_start_date det_agent_type det_agent\n## 1\nGrass\nPermit Related 2006-04-02 12:00\nUNP\n310\n## 2 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\n310\n## 3 Lighting Fires Arson Suspected 2006-04-03 12:15\nUNP\n310\n## 4 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\nPUB\n## 5\nRefuse\nPermit Related 2006-04-03 17:00\nUNP\nLFS\n## 6\nUnclassified\nUnsafe Fire 2006-04-02 14:25\nUNP\n310\n##\ndiscovered_date discovered_size\nreported_date dispatched_resource\n## 1\n<NA>\nNA 2006-04-02 20:46\nFPD Staff\n## 2\n<NA>\nNA 2006-04-03 12:27\nFPD Staff\n## 3\n<NA>\nNA 2006-04-03 12:36\nFPD Staff\n## 4\n<NA>\nNA 2006-04-03 13:23\nFPD Staff\n## 5 2006-04-03 19:11\nNA 2006-04-03 19:12\nFPD Staff\n## 6 2006-04-02 14:27\nNA 2006-04-02 14:30\nFPD Staff\n##\ndispatch_date start_for_fire_date assessment_resource assessment_datetime\n## 1 2006-04-02 21:10\n2006-04-02 21:20\nIA Forces\n2006-04-02 22:00\n27"
    },
    {
        "page": 75,
        "text": "## 2 2006-04-03 12:33\n2006-04-03 12:35\nIA Forces\n2006-04-03 13:20\n## 3 2006-04-03 12:36\n2006-04-03 12:42\nIA Forces\n2006-04-03 13:23\n## 4 2006-04-03 13:50\n2006-04-03 13:50\nIA Forces\n2006-04-03 14:08\n## 5 2006-04-03 19:19\n2006-04-03 19:22\nOther\n2006-04-03 19:57\n## 6 2006-04-02 14:40\n2006-04-02 14:45\nIA Forces\n2006-04-02 16:00\n##\nassessment_hectares fire_spread_rate fire_type fire_position_on_slope\n## 1\n0.01\n0.0\nSurface\nFlat\n## 2\n0.20\n0.0\nSurface\nLower 1/3\n## 3\n0.50\n0.0\nSurface\nBottom\n## 4\n0.01\n0.0\nSurface\nFlat\n## 5\n0.10\n0.1\nSurface\nFlat\n## 6\n0.20\n0.0\nSurface\nFlat\n##\nweather_conditions_over_fire temperature relative_humidity wind_direction\n## 1\nClear\n18\n10\nSW\n## 2\nClear\n12\n22\nSW\n## 3\nClear\n12\n22\nSW\n## 4\nClear\n12\n22\nSW\n## 5\nClear\n6\n37\nSW\n## 6\nClear\n11\n32\nS\n##\nwind_speed fuel_type initial_action_by ia_arrival_at_fire_date ia_access\n## 1\n2\nO1a\nLand Owner\n<NA>\n<NA>\n## 2\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 3\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 4\n10\nO1b\nIndustry\n<NA>\n<NA>\n## 5\n2\n<NA>\nFire Department\n<NA>\n<NA>\n## 6\n20\nO1b\nFire Department\n<NA>\n<NA>\n##\nfire_fighting_start_date fire_fighting_start_size bucketing_on_fire\n## 1\n<NA>\nNA\n<NA>\n## 2\n<NA>\nNA\n<NA>\n## 3\n<NA>\nNA\n<NA>\n## 4\n<NA>\nNA\n<NA>\n## 5\n<NA>\nNA\n<NA>\n## 6\n<NA>\nNA\n<NA>\n##\ndistance_from_water_source first_bucket_drop_date\nbh_fs_date\n## 1\nNA\n<NA> 2006-04-02 22:00\n## 2\nNA\n<NA> 2006-04-03 13:20\n## 3\nNA\n<NA> 2006-04-03 13:23\n## 4\nNA\n<NA> 2006-04-03 14:08\n## 5\nNA\n<NA> 2006-04-03 19:57\n## 6\nNA\n<NA> 2006-04-02 16:00\n##\nbh_hectares\nuc_fs_date uc_hectares\nto_fs_date to_hectares\n## 1\n0.01 2006-04-02 22:00\n0.01\n<NA>\nNA\n## 2\n0.20 2006-04-03 13:20\n0.20\n<NA>\nNA\n## 3\n0.50 2006-04-03 13:23\n0.50\n<NA>\nNA\n## 4\n0.01 2006-04-03 14:08\n0.01\n<NA>\nNA\n## 5\n0.10 2006-04-03 20:19\n0.10 2006-04-03 20:20\n0.1\n## 6\n0.20 2006-04-02 16:00\n0.20\n<NA>\nNA\n##\nex_fs_date ex_hectares log_fire_spread_rate humidity vegetation_type\n## 1 2006-04-03 10:20\n0.10\n0.00000000\n10\nO1a\n## 2 2006-04-03 14:00\n0.20\n0.00000000\n22\nO1a\n## 3 2006-04-03 15:00\n0.50\n0.00000000\n22\nO1a\n## 4 2006-04-03 15:05\n0.01\n0.00000000\n22\nO1b\n## 5 2006-04-05 10:18\n0.10\n0.09531018\n37\n<NA>\n## 6 2006-04-03 18:00\n0.20\n0.00000000\n32\nO1b\n28"
    },
    {
        "page": 76,
        "text": "##\nSpreadCategory\n## 1\nLow\n## 2\nLow\n## 3\nLow\n## 4\nLow\n## 5\nLow\n## 6\nLow\nGeospatial Visualization: Map the fire occurrences geographically with spread rate and temperature as visual\nlayers\n#For the following code, the output is an HTML output, therefore exporting it in a PDF format is not po\n#data <- data[!is.na(data$fire_location_longitude) & !is.na(data$fire_location_latitude), ]\n# Ensure data is not NULL and has the required columns\n#if (!is.null(data) && all(c(\"fire_location_longitude\", \"fire_location_latitude\", \"fire_spread_rate\", \"\n# Print debugging information\n# Filter out rows with missing lat/long\n#data <- data[!is.na(data$fire_location_longitude) & !is.na(data$fire_location_latitude), ]\n#leaflet(data) %>%\n#addTiles() %>%\n#addCircleMarkers(~fire_location_longitude, ~fire_location_latitude,\n#radius = ~fire_spread_rate * 0.1,\n#color = ~ifelse(SpreadCategory == \"High\", \"red\", \"green\"),\n#fillOpacity = 0.5,\n#popup = ~paste(\"Spread Rate:\", fire_spread_rate, \"<br>\",\n#\"Temperature:\", temperature)) %>%\n#setView(lng = mean(data$fire_location_longitude, na.rm = TRUE),\n#lat = mean(data$fire_location_latitude, na.rm = TRUE),\n#zoom = 6) %>%\n#addLegend(\"bottomright\",\n#colors = c(\"red\", \"green\"),\n#labels = c(\"High Spread Rate\", \"Low Spread Rate\"),\n#title = \"Spread Rate Category\")\n#} else {\n#print(\"Data is NULL or required columns are missing.\")\n#}\nAdded clustering Points for Better Performance on this geospatial visualization. This will interactively shows\nthe area where the fire spread rate and temperature is high or low.\nHypothesis testing:\n# Extract the p-value for temperature in the transformed model\np_value_log <- summary(log_model)$coefficients[2, 4]\n# Hypothesis testing\nif (p_value_log < 0.05) {\nprint(\"Reject the null hypothesis. There is a statistically significant relationship between temperat\n} else {\nprint(\"Fail to reject the null hypothesis. There is no statistically significant relationship between\n}\n29"
    },
    {
        "page": 77,
        "text": "## [1] \"Reject the null hypothesis. There is a statistically significant relationship between temperatu\n# Print p-value for confirmation\nprint(paste(\"P-value:\", p_value_log))\n## [1] \"P-value: 1.1251791730511e-317\"\nThere is a statistically significant relationship between temperature and fire spread rate (log-transformed).\nThe analysis of the residual plot and Q-Q residual plot suggests that the linear regression model is not an\nappropriate fit for the data. The residual plot indicates a poor linear relationship, and the Q-Q residual plot\nreveals that the data is not normally distributed, with a right-skew and the presence of outliers. Given these\nobservations, alternative modeling approaches, such as non-linear regression or robust regression techniques,\nshould be considered to better capture the underlying patterns in the data and avoid the reliance on normality\nassumptions.\nQuestion 3 - What is the Relationship Between Fire Size to Response and Extinguished Time\nIntroduction\nAnalyze the relationship between the Fire Size against the Response /Extinguished Time\nResponse Time\nDuration between the detection of a forest fire and the arrival of firefighting resources at the scene.\nExtinguished Time\nDuration between the detection of a forest fire the moment when the fire is completely put out and no longer\nposes a threat.\nData Cleaning and Transformation\nReading the file and converting the blank into NA\ndata1=fire_data\nStep 1: Identifying the targetted Variable:\nBelow are the list of variables which we are focusing to continue our statistical analysis for this problem\nstatement\n\u2022 Fire_Size ( in Hectares)\n\u2022 fire_start_date\n\u2022 discovered date\n\u2022 ex_fs_date -(Extinguished time)\n\u2022 assessment_datetime-(Actual Response Starts)\n30"
    },
    {
        "page": 78,
        "text": "Step 2: Mapping the NA assessment_datetime,ex_fs_date\nIdentified the blank dates with the next earliest date for fire_start_date with Discovered_date, assessment\ndate with the arrival date in the fire control date,bh_fs_date. Main Objective of this transformation is not\nto eliminate the data which will be really supporting our regression\ndata2 <- data1 %>%\nmutate(discovered_date = ifelse(discovered_date == 'NA', reported_date, discovered_date),\nex_fs_date = ifelse(ex_fs_date == 'NA', bh_fs_date, ex_fs_date),\nfire_start_date = ifelse(fire_start_date == 'NA', discovered_date, fire_start_date),\nassessment_datetime = ifelse(assessment_datetime == 'NA', ia_arrival_at_fire_date, assessment\ndiscovered_date = ifelse(is.na(discovered_date), reported_date, discovered_date),\nex_fs_date = ifelse(is.na(ex_fs_date), bh_fs_date, ex_fs_date),\nfire_start_date = ifelse(is.na(fire_start_date), discovered_date, fire_start_date),\nassessment_datetime = ifelse(is.na(assessment_datetime), ia_arrival_at_fire_date, assessment_\nStep 3: Converting the date variables into date format\ndata2$ex_fs_date <- as.POSIXct(data2$ex_fs_date, format = \"%Y-%m-%d %H:%M\", tz = \"America/Edmonton\")\ndata2$discovered_date <- as.POSIXct(data2$discovered_date, format = \"%Y-%m-%d %H:%M\", tz = \"America/Edm\ndata2$fire_start_date <- as.POSIXct(data2$fire_start_date, format = \"%Y-%m-%d %H:%M\", tz = \"America/Edm\ndata2$assessment_datetime <- as.POSIXct(data2$assessment_datetime, format = \"%Y-%m-%d %H:%M\", tz = \"Ame\n# Check the classes after conversion\nclass(data2$ex_fs_date)\n## [1] \"POSIXct\" \"POSIXt\"\nclass(data2$fire_start_date)\n## [1] \"POSIXct\" \"POSIXt\"\nStep 4: Calculating the response and extinguished time in minutes\n\u2022 Responsetime = assessment_datetime-fire_start_date\n\u2022 Exttime= ex_fs_date-fire_start_date\n# Calculate response time and ext time in minutes\ndata2$Responsetime <- as.numeric(difftime(data2$assessment_datetime, data2$fire_start_date, units = \"mi\ndata2$Exttime <- as.numeric(difftime(data2$ex_fs_date, data2$fire_start_date, units = \"mins\"))\nStep 5: Logarthimic Transformation\n\u2022 Performed a log linear transformation on the dependent variables(current_size,Responsetime,Exttime)\ndata2$current_size=log10(data2$current_size)\ndata2$Responsetime=log10(data2$Responsetime)\ndata2$Exttime=log10(data2$Exttime)\n31"
    },
    {
        "page": 79,
        "text": "Step 6: Filtering Variables\n-Filtering Out the required variables to continue with the further regression analysis of the size and the times\nwhich we are targetting to produce the correlation.\u2018\ndata_filtered<- data2 %>%\nselect(fire_number, fire_year, current_size, Responsetime, Exttime)\nhead(data_filtered)\n##\nfire_number fire_year current_size Responsetime\nExttime\n## 1\nPWF001\n2006\n-1.00000\n2.778151 3.127105\n## 2\nEWF002\n2006\n-0.69897\n1.845098 2.041393\n## 3\nEWF001\n2006\n-0.30103\n1.832509 2.217484\n## 4\nEWF003\n2006\n-2.00000\n2.071882 2.243038\n## 5\nPWF002\n2006\n-1.00000\n2.247973 3.394101\n## 6\nCWF001\n2006\n-0.69897\n1.977724 3.218798\nStep 6: Calulating the mean response time and extinguished time\n\u2022 To continue analysis of the newly derived variables and conducting analysis, we performed grouping\nthe time with respect to the affected size and storing it into different data frames\n\u2022 result1 - (Response Time)\n\u2022 result2 - (Extinguished Time)\nBelow are the cleaned data set which are going to be in our further regression Analysis\nresult1 <- data2 %>%\ngroup_by(current_size) %>%\nsummarise(\nmeanresponsetime = mean(Responsetime),\n)\nresult1 <- result1 %>%\nfilter(!is.na(meanresponsetime), !is.infinite(meanresponsetime),\n!is.na(current_size), !is.infinite(current_size))\nresult2 <- data2 %>%\ngroup_by(current_size) %>%\nsummarise(\nmeanExttime = mean(Exttime),\n)%>%\narrange(meanExttime)\nresult2 <- result2 %>%\nfilter(!is.na(meanExttime), !is.infinite(meanExttime),\n!is.na(current_size), !is.infinite(current_size))\ncat(\"Data Frame for Response Time\")\n## Data Frame for Response Time\n32"
    },
    {
        "page": 80,
        "text": "head(result1)\n## # A tibble: 6 x 2\n##\ncurrent_size meanresponsetime\n##\n<dbl>\n<dbl>\n## 1\n-1.52\n2.34\n## 2\n-1.40\n2.41\n## 3\n-1.22\n2.57\n## 4\n-1.10\n2.38\n## 5\n-1.05\n2.45\n## 6\n-0.959\n2.25\nsummary(result1)\n##\ncurrent_size\nmeanresponsetime\n##\nMin.\n:-1.5229\nMin.\n:0.4771\n##\n1st Qu.: 0.7846\n1st Qu.:1.7482\n##\nMedian : 1.5428\nMedian :2.1533\n##\nMean\n: 1.7219\nMean\n:2.3599\n##\n3rd Qu.: 2.5152\n3rd Qu.:2.9807\n##\nMax.\n: 5.7617\nMax.\n:6.0263\ncat(\".\\n\")\n## .\ncat(\"Data Frame for Extinquished Time\")\n## Data Frame for Extinquished Time\nhead(result2)\n## # A tibble: 6 x 2\n##\ncurrent_size meanExttime\n##\n<dbl>\n<dbl>\n## 1\n4.08\n1.99\n## 2\n0.420\n2.05\n## 3\n0.897\n2.06\n## 4\n0.185\n2.11\n## 5\n0.246\n2.13\n## 6\n0.117\n2.14\nsummary(result2)\n##\ncurrent_size\nmeanExttime\n##\nMin.\n:-1.6990\nMin.\n:1.987\n##\n1st Qu.: 0.7645\n1st Qu.:3.650\n##\nMedian : 1.5358\nMedian :4.191\n##\nMean\n: 1.7056\nMean\n:4.200\n##\n3rd Qu.: 2.5110\n3rd Qu.:4.759\n##\nMax.\n: 5.7617\nMax.\n:6.031\n33"
    },
    {
        "page": 81,
        "text": "Regression Analysis\nPlotted the scattered plots to validate the density of the variables being analyzed\nggplot1= (ggplot(result1, aes(x=meanresponsetime, y = current_size))+\ngeom_point (color = \"red\") +\nstat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\")+\nlabs (title = \"Scatter plot of Average Response Time and current_size\",\nx = \"Response Time\", y =\"Average current_size\"))\nggplot1\n0\n2\n4\n6\n2\n4\n6\nResponse Time\nAverage current_size\nScatter plot of Average Response Time and current_size\nInference:\n-Positive linear trend. Response time increases, the current size also slightly increases, though the relationship\nseems weak (the slope of the line is small).\n-Data points (in red) are densely scattered, with many points clustering around lower response times and a\ncurrent size. However, there are a few points that deviate significantly from this dense cluster, especially for\nhigher response times and current sizes.\nggplot2 =(ggplot(result2, aes(x=meanExttime, y = current_size))+\ngeom_point (color = \"red\") +\nstat_smooth(method = \"lm\", formula = y ~ x, geom = \"smooth\")+\nlabs (title = \"Scatter plot of Average Extinguished time and Average current_size\",\n34"
    },
    {
        "page": 82,
        "text": "x = \"Extinguished time\", y =\"Average current_size\"))\nggplot2\n\u22122\n0\n2\n4\n6\n2\n3\n4\n5\n6\nExtinguished time\nAverage current_size\nScatter plot of Average Extinguished time and Average current_size\n### Inference:\n-Strong Positive Trend -Slope of the line is noticeably steeper compared to the response time plot, indicating\nthat changes in extinguished time have a more pronounced effect on the average current size.\nGenerating Models and Intreprating the Model Summary with Hypothesis Test\nMean Response Time\n-Null Hypothesis (H0): There is no relationship between the Response time and Average current_size.\n-Alternative Hypothesis (Ha): There is a relationship between the Response time and Average current_size.\nmodel_clean1 = lm(current_size ~ meanresponsetime, data = result1)\nsummary(model_clean1)\n##\n## Call:\n## lm(formula = current_size ~ meanresponsetime, data = result1)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -3.2433 -0.9376 -0.1591\n0.7704\n3.9218\n35"
    },
    {
        "page": 83,
        "text": "##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n1.52463\n0.09867\n15.451\n<2e-16 ***\n## meanresponsetime\n0.08359\n0.03937\n2.123\n0.0339 *\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 1.274 on 1465 degrees of freedom\n## Multiple R-squared:\n0.003068,\nAdjusted R-squared:\n0.002388\n## F-statistic: 4.509 on 1 and 1465 DF,\np-value: 0.03388\nInterpretation:\np-value of 0.03388 relationship between meanresponsetime and current_size is statistically significant at the\n5% significance level (p < 0.05)\nModel Fit:\nR-squared value of 0.002388 indicates a Low Fit.\nExtinquished Time\n-Null Hypothesis (H0): There is no relationship between the Extinguished time and Average current_size.\n-Alternative Hypothesis (Ha): There is a relationship between the Extinguished time and Average cur-\nrent_size.\nmodel_clean2 = lm(current_size ~ meanExttime, data = result2)\nsummary(model_clean2)\n##\n## Call:\n## lm(formula = current_size ~ meanExttime, data = result2)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -2.9340 -0.5563 -0.0782\n0.4719\n5.1584\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept) -3.57474\n0.12930\n-27.65\n<2e-16 ***\n## meanExttime\n1.25715\n0.03031\n41.48\n<2e-16 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 0.8741 on 1480 degrees of freedom\n## Multiple R-squared:\n0.5376, Adjusted R-squared:\n0.5373\n## F-statistic:\n1721 on 1 and 1480 DF,\np-value: < 2.2e-16\n36"
    },
    {
        "page": 84,
        "text": "Interpretation:\np-value of 2.2e-16 is extremely low, indicating strong evidence against the null hypothesis means that Re-\nsponse time has significant effect on the Size of the forest impact\nModel Fit:\nR-squared: 0.5373 of the variance in current_size which indicates a moderate fit\nAnalyzing the Normal Distribution - QQ Plot for Response Time\nqqnorm(model_clean1$residuals)\nqqline(model_clean1$residuals, col = \"red\")\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\nNormal Q\u2212Q Plot\nTheoretical Quantiles\nSample Quantiles\n### Interpretation: - Model appears to follow a normal distribution reasonably well. - Some deviations\nat the extreme ends (in the tails) may indicate slight departures from normality, possibly showing mild\noutliers or heavier tails than a normal distribution\nAnalyzing the Normal Distribution - QQ Plot for Extinquished Time\nqqnorm(model_clean2$residuals)\nqqline(model_clean2$residuals, col = \"red\")\n37"
    },
    {
        "page": 85,
        "text": "\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22122\n0\n2\n4\nNormal Q\u2212Q Plot\nTheoretical Quantiles\nSample Quantiles\nInterpretation:\n\u2022 Model is approximately normal\n\u2022 Model may have heavier tails or more extreme values than would be expected in a normal distribution-\nWhen the extinguished time increases there will be huge impact on the affected area\nAnalyzing the Normal Distribution - Residual Plot for Response Time\nplot(model_clean1$fitted.values, model_clean1$residuals)\nabline(h = 0, col = \"red\")\n38"
    },
    {
        "page": 86,
        "text": "1.6\n1.7\n1.8\n1.9\n2.0\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\nmodel_clean1$fitted.values\nmodel_clean1$residuals\n### Inference: -Residuals seem to be randomly scattered around the horizontal line at zero though there\nare many outliers in the data set\nAnalyzing the Normal Distribution - Residual Plot for Extinguished Time\nplot(model_clean2$fitted.values, model_clean2$residuals)\nabline(h = 0, col = \"red\")\n39"
    },
    {
        "page": 87,
        "text": "\u22121\n0\n1\n2\n3\n4\n\u22122\n0\n2\n4\nmodel_clean2$fitted.values\nmodel_clean2$residuals\n### Inference: -Residuals are centered around zero, which is a good sign for a well-fitted model and shows\nthe strong relationship between time and size\nConclusion\n-Overall, Models of Response and Extinguished time has a significant relationship between the affected area\n- size.\n-However, the Extinguished Time Model which shows the strong relationship with the relatively low p-value\n-Hence improving the Response and extinguished time of a forest fire involving with following strategies\nideally impact the size of the forest being affected\n-Early detection,Rapid response,Effective firefighting techniques,Training and Coordination & Public aware-\nness\nQuestion 4 - Does Fire Size Relate To The Weather Conditions When The Fire Starts\nThe relationship I will be investigating is if there a potential relationship between the recorded fires size\nonce under control and the weather that was present during the fires start. I will have uc_hectares as my\ndependent variable and I will use a multivariable regression with the temperature, relative humidity, and\nwind speed as the independent variables.\n#assign initial dataset to a new dataset, to prevent errors with preexisting code for the other questio\ndata1=fire_data\n40"
    },
    {
        "page": 88,
        "text": "EDA\nhead(data1)\n##\nfire_year fire_number fire_name current_size size_class\n## 1\n2006\nPWF001\n<NA>\n0.10\nA\n## 2\n2006\nEWF002\n<NA>\n0.20\nB\n## 3\n2006\nEWF001\n<NA>\n0.50\nB\n## 4\n2006\nEWF003\n<NA>\n0.01\nA\n## 5\n2006\nPWF002\n<NA>\n0.10\nA\n## 6\n2006\nCWF001\n<NA>\n0.20\nB\n##\nfire_location_latitude fire_location_longitude\nfire_origin\n## 1\n56.24996\n-117.1820\nPrivate Land\n## 2\n53.60637\n-115.9157\nProvincial Land\n## 3\n53.61093\n-115.5943\nProvincial Land\n## 4\n53.60887\n-115.6095\nProvincial Land\n## 5\n56.24996\n-117.0502\nProvincial Land\n## 6\n51.15293\n-115.0346 Indian Reservation\n##\ngeneral_cause_desc industry_identifier_desc\nresponsible_group_desc\n## 1\nResident\n<NA>\nResident\n## 2\nIncendiary\n<NA> Others (explain in remarks)\n## 3\nIncendiary\n<NA> Others (explain in remarks)\n## 4\nIncendiary\n<NA> Others (explain in remarks)\n## 5\nOther Industry\nWaste Disposal\nEmployees\n## 6\nResident\n<NA>\nResident\n##\nactivity_class\ntrue_cause\nfire_start_date det_agent_type det_agent\n## 1\nGrass\nPermit Related 2006-04-02 12:00\nUNP\n310\n## 2 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\n310\n## 3 Lighting Fires Arson Suspected 2006-04-03 12:15\nUNP\n310\n## 4 Lighting Fires Arson Suspected 2006-04-03 12:10\nUNP\nPUB\n## 5\nRefuse\nPermit Related 2006-04-03 17:00\nUNP\nLFS\n## 6\nUnclassified\nUnsafe Fire 2006-04-02 14:25\nUNP\n310\n##\ndiscovered_date discovered_size\nreported_date dispatched_resource\n## 1\n<NA>\nNA 2006-04-02 20:46\nFPD Staff\n## 2\n<NA>\nNA 2006-04-03 12:27\nFPD Staff\n## 3\n<NA>\nNA 2006-04-03 12:36\nFPD Staff\n## 4\n<NA>\nNA 2006-04-03 13:23\nFPD Staff\n## 5 2006-04-03 19:11\nNA 2006-04-03 19:12\nFPD Staff\n## 6 2006-04-02 14:27\nNA 2006-04-02 14:30\nFPD Staff\n##\ndispatch_date start_for_fire_date assessment_resource assessment_datetime\n## 1 2006-04-02 21:10\n2006-04-02 21:20\nIA Forces\n2006-04-02 22:00\n## 2 2006-04-03 12:33\n2006-04-03 12:35\nIA Forces\n2006-04-03 13:20\n## 3 2006-04-03 12:36\n2006-04-03 12:42\nIA Forces\n2006-04-03 13:23\n## 4 2006-04-03 13:50\n2006-04-03 13:50\nIA Forces\n2006-04-03 14:08\n## 5 2006-04-03 19:19\n2006-04-03 19:22\nOther\n2006-04-03 19:57\n## 6 2006-04-02 14:40\n2006-04-02 14:45\nIA Forces\n2006-04-02 16:00\n##\nassessment_hectares fire_spread_rate fire_type fire_position_on_slope\n## 1\n0.01\n0.0\nSurface\nFlat\n## 2\n0.20\n0.0\nSurface\nLower 1/3\n## 3\n0.50\n0.0\nSurface\nBottom\n## 4\n0.01\n0.0\nSurface\nFlat\n## 5\n0.10\n0.1\nSurface\nFlat\n## 6\n0.20\n0.0\nSurface\nFlat\n41"
    },
    {
        "page": 89,
        "text": "##\nweather_conditions_over_fire temperature relative_humidity wind_direction\n## 1\nClear\n18\n10\nSW\n## 2\nClear\n12\n22\nSW\n## 3\nClear\n12\n22\nSW\n## 4\nClear\n12\n22\nSW\n## 5\nClear\n6\n37\nSW\n## 6\nClear\n11\n32\nS\n##\nwind_speed fuel_type initial_action_by ia_arrival_at_fire_date ia_access\n## 1\n2\nO1a\nLand Owner\n<NA>\n<NA>\n## 2\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 3\n10\nO1a\nFire Department\n<NA>\n<NA>\n## 4\n10\nO1b\nIndustry\n<NA>\n<NA>\n## 5\n2\n<NA>\nFire Department\n<NA>\n<NA>\n## 6\n20\nO1b\nFire Department\n<NA>\n<NA>\n##\nfire_fighting_start_date fire_fighting_start_size bucketing_on_fire\n## 1\n<NA>\nNA\n<NA>\n## 2\n<NA>\nNA\n<NA>\n## 3\n<NA>\nNA\n<NA>\n## 4\n<NA>\nNA\n<NA>\n## 5\n<NA>\nNA\n<NA>\n## 6\n<NA>\nNA\n<NA>\n##\ndistance_from_water_source first_bucket_drop_date\nbh_fs_date\n## 1\nNA\n<NA> 2006-04-02 22:00\n## 2\nNA\n<NA> 2006-04-03 13:20\n## 3\nNA\n<NA> 2006-04-03 13:23\n## 4\nNA\n<NA> 2006-04-03 14:08\n## 5\nNA\n<NA> 2006-04-03 19:57\n## 6\nNA\n<NA> 2006-04-02 16:00\n##\nbh_hectares\nuc_fs_date uc_hectares\nto_fs_date to_hectares\n## 1\n0.01 2006-04-02 22:00\n0.01\n<NA>\nNA\n## 2\n0.20 2006-04-03 13:20\n0.20\n<NA>\nNA\n## 3\n0.50 2006-04-03 13:23\n0.50\n<NA>\nNA\n## 4\n0.01 2006-04-03 14:08\n0.01\n<NA>\nNA\n## 5\n0.10 2006-04-03 20:19\n0.10 2006-04-03 20:20\n0.1\n## 6\n0.20 2006-04-02 16:00\n0.20\n<NA>\nNA\n##\nex_fs_date ex_hectares\n## 1 2006-04-03 10:20\n0.10\n## 2 2006-04-03 14:00\n0.20\n## 3 2006-04-03 15:00\n0.50\n## 4 2006-04-03 15:05\n0.01\n## 5 2006-04-05 10:18\n0.10\n## 6 2006-04-03 18:00\n0.20\nBased off of the potential relationship I wish to investigate I will create a new data frame with the columns\nof interest.\nreg_df=data.frame(uc_hectares=data1$uc_hectares,temperature=data1$temperature,relative_humidity=data1$r\nsummary(reg_df)\n##\nuc_hectares\ntemperature\nrelative_humidity\nwind_speed\n##\nMin.\n:\n0.0\nMin.\n:-39.00\nMin.\n:\n0.00\nMin.\n: 0.000\n##\n1st Qu.:\n0.0\n1st Qu.: 14.00\n1st Qu.: 31.00\n1st Qu.: 3.000\n##\nMedian :\n0.0\nMedian : 19.00\nMedian : 40.00\nMedian : 6.000\n42"
    },
    {
        "page": 90,
        "text": "##\nMean\n:\n245.6\nMean\n: 17.85\nMean\n: 45.38\nMean\n: 8.813\n##\n3rd Qu.:\n0.3\n3rd Qu.: 23.00\n3rd Qu.: 56.00\n3rd Qu.:12.000\n##\nMax.\n:707648.0\nMax.\n: 39.90\nMax.\n:100.00\nMax.\n:90.000\n##\nNA\u2019s\n:2820\nNA\u2019s\n:2822\nNA\u2019s\n:2823\nBased off of the summary stats the dependent variable uc_hectares seems to be strongly skewed. To explore\nthe data I will create histograms to display the data\u2019s distributuion.\nggplot(data = reg_df, aes(x = uc_hectares)) +\ngeom_histogram(color='blue')\n## \u2018stat_bin()\u2018 using \u2018bins = 30\u2018. Pick better value with \u2018binwidth\u2018.\n0\n5000\n10000\n15000\n20000\n25000\n0e+00\n2e+05\n4e+05\n6e+05\nuc_hectares\ncount\nggplot(data = reg_df, aes(x = temperature)) +\ngeom_histogram(color='blue')\n## \u2018stat_bin()\u2018 using \u2018bins = 30\u2018. Pick better value with \u2018binwidth\u2018.\n## Warning: Removed 2820 rows containing non-finite outside the scale range\n## (\u2018stat_bin()\u2018).\n43"
    },
    {
        "page": 91,
        "text": "0\n1000\n2000\n3000\n4000\n\u221225\n0\n25\ntemperature\ncount\nggplot(data = reg_df, aes(x = relative_humidity)) +\ngeom_histogram(color='blue')\n## \u2018stat_bin()\u2018 using \u2018bins = 30\u2018. Pick better value with \u2018binwidth\u2018.\n## Warning: Removed 2822 rows containing non-finite outside the scale range\n## (\u2018stat_bin()\u2018).\n44"
    },
    {
        "page": 92,
        "text": "0\n1000\n2000\n0\n25\n50\n75\n100\nrelative_humidity\ncount\nggplot(data = reg_df, aes(x = wind_speed)) +\ngeom_histogram(color='blue')\n## \u2018stat_bin()\u2018 using \u2018bins = 30\u2018. Pick better value with \u2018binwidth\u2018.\n## Warning: Removed 2823 rows containing non-finite outside the scale range\n## (\u2018stat_bin()\u2018).\n45"
    },
    {
        "page": 93,
        "text": "0\n2000\n4000\n0\n25\n50\n75\nwind_speed\ncount\nUnsurprisingly the distribution of the uc_hectares variable is greatly skewed confirming what could be seen\nin the summary statistics.\nRegression Analysis\nreglarge=lm(formula=reg_df$uc_hectares~reg_df$temperature+reg_df$relative_humidity+reg_df$wind_speed)\nsummary(reglarge)\n##\n## Call:\n## lm(formula = reg_df$uc_hectares ~ reg_df$temperature + reg_df$relative_humidity +\n##\nreg_df$wind_speed)\n##\n## Residuals:\n##\nMin\n1Q Median\n3Q\nMax\n##\n-3902\n-479\n-220\n29 705357\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n-279.842\n229.400\n-1.220 0.222521\n## reg_df$temperature\n23.514\n7.070\n3.326 0.000883 ***\n## reg_df$relative_humidity\n-5.442\n2.927\n-1.860 0.062956 .\n## reg_df$wind_speed\n42.551\n6.231\n6.829 8.76e-12 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n46"
    },
    {
        "page": 94,
        "text": "##\n## Residual standard error: 7734 on 22492 degrees of freedom\n##\n(2825 observations deleted due to missingness)\n## Multiple R-squared:\n0.003188,\nAdjusted R-squared:\n0.003055\n## F-statistic: 23.98 on 3 and 22492 DF,\np-value: 1.733e-15\nTo start I\u2019ll state the hypothesis test for all coefficients, H0: B=0 The coefficient in question is not statistically\nsignificant from 0 H1: B!=0 The coefficient is statistically significant from 0\nThe intercept is -279.8 hectares if all other variables are 0 which does not make sense and it is unsurprisingly\nnot significant with a P value of 0.22 so we cannot reject the null hypothesis. The coefficient for temperature,\n23.5 hectares per degree increase in temperature has a P value of 0.00088 allowing us to state that is significant\nand we can reject the null hypothesis. The coefficient for relative humidity is -5.44 hectares per %increase\nin humidity, with a P value of 0.062 the coefficient is not significant at the 5% level but is close. Under the\nstated hypothesis test we cannot reject the null hypothesis. Lastly The coefficient for wind speed is 42.5\nhectares burned per 1 kilometer per hour increase. With a P value of near 0 the coefficient is significant and\nwe can reject the null hypothesis.\nWith an adj R-squared of 0.003055 indicating the regression only explains 0.32% of variation in uc_hectares\nis explained by this model, this indicates that the estimated model is likely a very poor fit for the data.\nreg_cor= round(cor(reg_df,use = \"complete.obs\"), 2)\nreg_cor\n##\nuc_hectares temperature relative_humidity wind_speed\n## uc_hectares\n1.00\n0.03\n-0.03\n0.05\n## temperature\n0.03\n1.00\n-0.28\n-0.02\n## relative_humidity\n-0.03\n-0.28\n1.00\n-0.17\n## wind_speed\n0.05\n-0.02\n-0.17\n1.00\nFrom the correlation matrix we can see that there is very little correlation between uc_hectares and any of\nthe independent variables. To better visualize this I will create scatter plots with estimated models for each\nindependent variable and uc_hectares as visualizing a multivariate regression model is difficult.\nggplot(reg_df, aes(x=temperature, y=uc_hectares )) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n## Warning: Removed 2820 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2820 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n47"
    },
    {
        "page": 95,
        "text": "0e+00\n2e+05\n4e+05\n6e+05\n\u221240\n\u221220\n0\n20\n40\ntemperature\nuc_hectares\nggplot(reg_df, aes(x=relative_humidity, y=uc_hectares )) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n## Warning: Removed 2822 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2822 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n48"
    },
    {
        "page": 96,
        "text": "0e+00\n2e+05\n4e+05\n6e+05\n0\n25\n50\n75\n100\nrelative_humidity\nuc_hectares\nggplot(reg_df, aes(x=wind_speed, y=uc_hectares)) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n## Warning: Removed 2823 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2823 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n49"
    },
    {
        "page": 97,
        "text": "0e+00\n2e+05\n4e+05\n6e+05\n0\n25\n50\n75\nwind_speed\nuc_hectares\nplot(lm(formula=data1$uc_hectares~data1$temperature+data1$relative_humidity+data1$wind_speed))\n50"
    },
    {
        "page": 98,
        "text": "\u22121000\n0\n1000\n2000\n3000\n4000\n0e+00\n4e+05\nFitted values\nResiduals\nlm(data1$uc_hectares ~ data1$temperature + data1$relative_humidity + data1$ ...\nResiduals vs Fitted\n8026\n15583\n20777\n51"
    },
    {
        "page": 99,
        "text": "\u22124\n\u22122\n0\n2\n4\n0\n20\n40\n60\n80\n100\nTheoretical Quantiles\nStandardized residuals\nlm(data1$uc_hectares ~ data1$temperature + data1$relative_humidity + data1$ ...\nQ\u2212Q Residuals\n8026\n15583\n20777\n52"
    },
    {
        "page": 100,
        "text": "\u22121000\n0\n1000\n2000\n3000\n4000\n0\n2\n4\n6\n8\nFitted values\nStandardized residuals\nlm(data1$uc_hectares ~ data1$temperature + data1$relative_humidity + data1$ ...\nScale\u2212Location\n8026\n15583\n20777\n53"
    },
    {
        "page": 101,
        "text": "0.000\n0.001\n0.002\n0.003\n0.004\n0\n20\n40\n60\n80\n100\nLeverage\nStandardized residuals\nlm(data1$uc_hectares ~ data1$temperature + data1$relative_humidity + data1$ ...\nCook's distance\n0.5\n1\nResiduals vs Leverage\n8026\n20777\n15583\nThe plots above show that the residuals are not independent or normally distributed. The first plot shows\nthat there are some large outliers in the data and the normality plot also shows that the the residuals are\nnot normally distributed. This indicates that the model is likely unreliable. A log log transformation could\nperhaps improve this somewhat.\nreg_dflog=data.frame(uc_hectares=log10(data1$uc_hectares),temperature=data1$temperature,relative_humidi\nreglog=lm(formula=reg_dflog$uc_hectares~reg_dflog$temperature+reg_dflog$relative_humidity+reg_dflog$win\nsummary(reglog)\n##\n## Call:\n## lm(formula = reg_dflog$uc_hectares ~ reg_dflog$temperature +\n##\nreg_dflog$relative_humidity + reg_dflog$wind_speed)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -3.1129 -0.8514 -0.2214\n0.6036\n6.4842\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n-1.2367098\n0.0328594\n-37.64\n<2e-16 ***\n## reg_dflog$temperature\n0.0193262\n0.0010127\n19.08\n<2e-16 ***\n## reg_dflog$relative_humidity -0.0079249\n0.0004192\n-18.91\n<2e-16 ***\n## reg_dflog$wind_speed\n0.0256644\n0.0008925\n28.76\n<2e-16 ***\n## ---\n54"
    },
    {
        "page": 102,
        "text": "## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 1.108 on 22492 degrees of freedom\n##\n(2825 observations deleted due to missingness)\n## Multiple R-squared:\n0.08394,\nAdjusted R-squared:\n0.08382\n## F-statistic:\n687 on 3 and 22492 DF,\np-value: < 2.2e-16\nTransforming the data in log format did improve the regression and the P values of all coefficients are below\n0.05 allowing us to state that all 3 coefficients significant along with the intercept but still does not make\nsense in the context of the data. With an adj R-squared of 0.083 the fit of the model does improve in this\ncase drastically from the first model.\nggplot(reg_dflog, aes(x=temperature, y=uc_hectares )) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n## Warning: Removed 2820 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2820 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n\u22122\n0\n2\n4\n6\n\u221240\n\u221220\n0\n20\n40\ntemperature\nuc_hectares\nggplot(reg_dflog, aes(x=relative_humidity, y=uc_hectares )) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n55"
    },
    {
        "page": 103,
        "text": "## Warning: Removed 2822 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2822 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n\u22122\n0\n2\n4\n6\n0\n25\n50\n75\n100\nrelative_humidity\nuc_hectares\nggplot(reg_dflog, aes(x=wind_speed, y=uc_hectares)) +\ngeom_point(color = \"red\") + stat_smooth(method = \"lm\",\nformula = y ~ x, geom = \"smooth\")\n## Warning: Removed 2823 rows containing non-finite outside the scale range\n## (\u2018stat_smooth()\u2018).\n## Warning: Removed 2823 rows containing missing values or values outside the scale range\n## (\u2018geom_point()\u2018).\n56"
    },
    {
        "page": 104,
        "text": "\u22122\n0\n2\n4\n6\n0\n25\n50\n75\nwind_speed\nuc_hectares\nreg_corlog= round(cor(reg_dflog,use = \"complete.obs\"), 2)\nreg_corlog\n##\nuc_hectares temperature relative_humidity wind_speed\n## uc_hectares\n1.00\n0.16\n-0.20\n0.21\n## temperature\n0.16\n1.00\n-0.28\n-0.02\n## relative_humidity\n-0.20\n-0.28\n1.00\n-0.17\n## wind_speed\n0.21\n-0.02\n-0.17\n1.00\nWith the fire size transformed into log form the data shows slightly more of a linear relationship. Additionally,\nthe correlation matrix shows an improved correlation between the independent and dependent variables.\nplot(lm(formula=reg_dflog$uc_hectares~reg_dflog$temperature+reg_dflog$relative_humidity+reg_dflog$wind_\n57"
    },
    {
        "page": 105,
        "text": "\u22122\n\u22121\n0\n1\n\u22124\n\u22122\n0\n2\n4\n6\nFitted values\nResiduals\nlm(reg_dflog$uc_hectares ~ reg_dflog$temperature + reg_dflog$relative_humid ...\nResiduals vs Fitted\n10401\n15583\n24839\n58"
    },
    {
        "page": 106,
        "text": "\u22124\n\u22122\n0\n2\n4\n\u22122\n0\n2\n4\n6\nTheoretical Quantiles\nStandardized residuals\nlm(reg_dflog$uc_hectares ~ reg_dflog$temperature + reg_dflog$relative_humid ...\nQ\u2212Q Residuals\n10401\n15583\n24839\n59"
    },
    {
        "page": 107,
        "text": "\u22122\n\u22121\n0\n1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nFitted values\nStandardized residuals\nlm(reg_dflog$uc_hectares ~ reg_dflog$temperature + reg_dflog$relative_humid ...\nScale\u2212Location\n10401\n15583\n24839\n60"
    },
    {
        "page": 108,
        "text": "0.000\n0.001\n0.002\n0.003\n0.004\n\u22122\n0\n2\n4\n6\nLeverage\nStandardized residuals\nlm(reg_dflog$uc_hectares ~ reg_dflog$temperature + reg_dflog$relative_humid ...\nCook's distance\nResiduals vs Leverage\n11443\n9506\n8026\nWhile still not independent or normally distributed it is still a large improvement on the original model\nwith the untransformed data.\nOverall the model with the total dataset does not fit the data well and the visualizations showed that there\nwas almost no linear relationship between the variables and the fire size. However, once the fire size data\nis transformed so the model is in log linear format the model and the visualizations showed a much better\nrelationship between the weather and fire size. With the size of the dataset and other variables available\nperhaps a larger or non linear model could be built to better fit the fire size data but given the regressions\nabove temperature, wind speed, and relative humidity alone are not enough to sufficiently explain the data\nwhile there is at least something of a relationship between them.\nConclusion\nBoth questions investigating potential determinants of fire spread rates did have significant coefficients\ntheir R-squared values were low. The model looking at the relationship between fire size, response, and\nextinguished time did have significant coefficients and did show a good linear relationship between fire size\nand time to extinguishment time. The multivariable model looking at if weather conditions can predict fire\nsize did show a linear relationship in the variables that were all significant, but the R-squared value was also\nlow.\nOverall, many of our results came down to the fact that the skewed nature of much of the data affected our\nlinear model\u2019s ability to fit the data. Even with transformations and slightly larger multivariable models\nthey were not sufficient. Perhaps a more comprehensive model containing more of the dataset\u2019s variables or\nperhaps a non-linear model may fit the data better.\n61"
    },
    {
        "page": 109,
        "text": "References\n1. Friedman, E. S., Kauffman, M. J., Burch, J. W., & Cottam, R. (2019). Trends in conifer regeneration\nfollowing disturbance in a high-elevation forest: Implications for forest management and restoration.\nCanadian Journal of Forest Research, 49(5), 502-511. https://doi.org/10.1139/cjfr-2018-0293\n2. Government of Alberta. (n.d.). Wildfire data [Data set]. Open Alberta. https://open.alberta.ca/\nopendata/wildfire-data#summary\n62"
    },
    {
        "page": 110,
        "text": "torontocrimedrivers\nJuly 13, 2025\n1\nDynamics of Crime in Toronto\n1.1\nDATA 604 Group L01-05\nAaron Gelfand\nDavid Gri\ufb00in\nJackson Meier\nSteen Rasmussen\nVenkateshwaran Balu Soundararajan\n[1]: import pandas as pd\nimport sqlalchemy as sq\nfrom sqlalchemy import create_engine\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.stats import linregress\nimport geopandas as gpd\nfrom shapely.geometry import shape\nfrom shapely import wkt\nimport json\nimport re\nimport folium\nimport warnings\nimport mysql.connector\nfrom mysql.connector import Error\nimport plotly.express as px\nwarnings.filterwarnings('ignore')\nsq.__version__\n[1]: '2.0.36'\n[2]: USERNAME='project'\nPASSWORD='ErCjJdGXIcFTr'\n1"
    },
    {
        "page": 111,
        "text": "DATABASE='project'\nPORT=3306\nengine = sq.create_engine(f'''mysql+mysqlconnector://{USERNAME}:\n\u21aa{PASSWORD}@localhost:{PORT}/{DATABASE}''')\n[3]: # Function to run SQL queries\ndef SQL(query_string):\ntry:\nquery = pd.read_sql_query(query_string, con=engine)\nreturn query\nexcept Exception as e:\nprint(\"An error occurred:\", e)\nreturn None\n[4]: # Function to create mysql table using DDL\ndef create_table(db_name, table_name,create_table_query):\ntry:\nconnection = mysql.connector.connect(\nhost='localhost',\nuser=USERNAME,\npassword=PASSWORD,\nport=PORT,\ndatabase=db_name\n)\nif connection.is_connected():\ncursor = connection.cursor()\ndrop_table_query= f\"\"\"DROP TABLE IF EXISTS {table_name};\"\"\"\ncursor.execute(drop_table_query)\nprint(\"Table \"+table_name+\" dropped successfully.\")\ncursor.execute(create_table_query)\nconnection.commit()\nprint(\"Table \"+table_name+\" created successfully.\")\nexcept Error as e:\nprint(f\"Error while connecting to MySQL: {e}\")\nfinally:\nif connection.is_connected():\ncursor.close()\nconnection.close()\n[5]: SQL(\"SHOW TABLES\")\n[5]:\nTables_in_project\n0\ndata604_Bike_crime\n2"
    },
    {
        "page": 112,
        "text": "1\ndata604_address_points\n2\ndata604_converted_budget\n3\ndata604_converted_crime\n4\ndata604_daily_shelter_occupancy\n5\ndata604_daily_shelter_occupancy_points\n6\ndata604_daily_shelter_occupancy_raw\n7\ndata604_income\n2\nIntroduction\nThe goal of this project is to investigate the dynamics of crime-related trends and rates in Toronto,\nfocusing on the factors that drive these changes. By looking at how the severity of crimes varies\nacross Toronto neighbourhoods, we aim to help uncover contributors to crime and reveal high-risk\nareas with the hope of unveiling mitigation strategies. Specifically, our project will analyze yearly\npolice budgets, household income, household education levels, homeless shelter occupancy rates,\nand current bicycle parking racks to determine what factors most influence crime statistics.\nIn particular, we will compare trends in annual police funds to crime rates over the same time\nperiod. We will also link any correlations between household income and average education level\nto crime rates in the corresponding neighbourhood. Lastly, we will be looking at whether homeless\nshelter occupancy has any impact on surrounding petty crime and whether bike parking racks and\nbike concentrations have any effect on bike theft rates. By linking the trends of these variables,\nwe hope to answer the questions regarding crime contributors and how to address the tendencies.\nUltimately, we hope to reveal insights on Toronto crime that may be useful to governing parties to\nhelp make data-informed decisions. These discoveries and implementations may not only be useful\nfor Toronto neighbourhoods but also for other major Canadian cities with similar challenges.\nGuiding Questions\nOverall Objective of this project is to evaluate the crime statistics in Toronto\nneighbourhoods by analyzing the guiding Questions.\nQ1: Police Budget and Crime Data Trends: How does the Operating Budget impact the\ncrime statistics? Research indicates that police budgets have generally increased over the years,\neven as crime rates have fluctuated (Toronto Police Service 2024). We will be analyzing the Police\nBudget with the Crime Rates, so we can compare police budget data with crime trends to see if an\nincrease or decrease in funds plays a role in neighbourhood crime rates for different years.\nQ2: Household Income and Crime Rates: How does mean household income change crime\nstatistics? Higher Household income is generally associated with lower crime rates. Analyzing\nhousehold income data in relation to crime rates can reveal important trends. We will analyze\ncensus data from 2020 to see if lower household income results in higher crime rates.\nQ3: Shelter Occupancy and Neighbourhood Crime: How does shelter occupancy rate af-\nfect crime statistics per neighbourhood? Investigating the relationship between shelter occupancy\nrates and neighbourhood crime can provide insights into how homelessness and crime are intercon-\nnected. We will be Investigating the shelter occupancy rates and link them to crime patterns by\nneighbourhood and year.\nQ4: Bike racks and Bikes Theft: How does the presence of bike racks affect bike thefts per\nneighbourhood? Comparing crime data for areas with and without bike racks can help determine\n3"
    },
    {
        "page": 113,
        "text": "if the presence of bike racks reduces bike theft. Studies often show that well-placed bike racks,\ncombined with other security measures, can deter theft and improve overall safety in the area (Bike\nFinder, 2024). We want to examine whether a higher concentration of bike racks helps prevent bike\nthefts.\nQ5: Highest Education Level and Crime Rates: How does the highest certificate, diploma,\nor degree per household, affect crime rate per neighbourhood? Generally, higher education levels\nare associated with lower crime rates (Lochner & Moretti, 2004), as education can provide better\neconomic opportunities and reduce the likelihood of engaging in criminal activities. Analyzing crime\nrates in relation to education levels in each neighbourhood can highlight the impact of education\non crime.\n3\nIndividual Datasets\nWe are using 6 datasets for this project. All of these datasets are from toronto open data and all\nsubsequently hold the same open data license requiring use of the data to be acknowledged. You\nwill find links to the datasets in the references below. Our first dataset contains data on crime\nrates in various neighbourhoods in Toronto from 2014 to 2023. The dataset comes in the form of\na Geojson, csv, geopackage, and shapefile and contains 158 neighbourhoods and 18 separate crime\nstatistics for each neighbourhood. Our second dataset contains the toronto police operating budget\nfor 2014-2023 and is downloadable in csv, json, and xml format. The dataset contains 6 columns\nand 129 rows. The third dataset contains data on the neighbourhoods in Toronto. It contains\ninformation on each neighbourhood\u2019s demographic data such as income distribution and education\nlevels and many more. The dataset is released every 5 years alongside the census and we will be\nusing 2020 dataset. They can be downloaded in csv, json, and xml and xlsx. The 2020 dataset has\n158 columns and 2383 rows. The fourth dataset contains geographic data on bike rack locations\nin Toronto, the dataset has 27 columns and 241 rows. It can be downloaded as a Geojson, csv,\ngeopackage, and shapefile. The fifth dataset contains data on daily shelter occupancy in Toronto\nfrom 2017-2020. The dataset contains information such as shelter name, location and occupancy\non each day. Each year\u2019s dataset can be downloaded as a csv, json, and xml and the most recent\n2020 dataset contains 13 columns and 41061 rows. The final dataset contains data on obrt 500,000\naddresses in Toronto. The dataset contains information on street addresses, as well as geometric\ncoordinates for each address.\nBelow we have listed the team member that worked on the cleaning of each dataset and the variables\nof interest. The shelter occupancy and address points datasets were particularly di\ufb00icult to utilize\nas we needed to attach the geometric coordinates from the address points dataset to the shelter\noccupancy dataset.\nAs both datasets were very large, any process run on either dataset took\nconsiderable time. Therefore any mistakes in our code required that additional time when running,\nwhich considerably slowed the process here.\nToronto Crime Data - Jackson Meier - AREA_NAME: Name of the neighbourhood - POPULA-\nTION_2023: Population of neighbourhood - Crime Columns: CRIME_XXXX: the crime followed\nby the year (XXXX). Replace CRIME with the following:Assault (ASSAULT), Auto Theft (AU-\nTOTHEFT), Bike Theft (BIKETHEFT), Break & Enter (BREAKENTER), Homicide (HOMI-\nCIDE), Robbery (ROBBERY), Shooting (SHOOTING), Theft From Motor Vehicle (THEFT-\nFROMMV), Theft Over $5,000 (THEFTOVER). Years range from 2014-2023 - Each crime has\na rate column, which is the same crime, and same year, but as a rate per 100,000 people. It is the\ncrime, followed by RATE then the year (XXXX).Example: ASSAULT_RATE_XXXX\n4"
    },
    {
        "page": 114,
        "text": "Gross Operating Budget - Aaron Gelfand - YEAR: year of the value collected - COUNT: Value\nof the subtype - SUBTYPE: Factor that is being measured. Main interest in Gross operating\nbudget ($) and Absolute change ($)\nNeighbourhood Profiles - David Gri\ufb00in - Neighbourhood Name: name of the neighbourhood\n- Rest of the columns are specific neighbourhoods - Average total income among recipients ($):\nThe average income among recipients aged 15 and over, in a private household - Total - Highest\ncertificate, diploma or degree for the population aged 15 years and over in private households - 25%\nsample data: Data on the highest degree obtained in a private household\nBike Rack Data - Steen Rasmussen - ADDRESS_FULL: displays full address of the location of\nthe bike rack - POSTAL_CODE: postal code to go along with the address - WARD_NAME: ward\nthe bike rack is located in - CAPACITY: number of bikes that can fit on the rack - SHELTERED:\nwhether or not the bike rack is sheltered/covered - STATUS: whether the bike rack is installed,\napproved or planned. Only interested in installed.\nShelter\nOccupancy\n-\nVenkateshwaran\nBalu\nSoundararajan/Aaron\nGelfand\n-\nSHEL-\nTER_ADDRESS: Location of the shelter - OCCUPANCY: occupancy of the shelter - CAPACITY:\namount of people the shelter can host\nAddress Points - Venkateshwaran Balu Soundararajan/Aaron Gelfand - ADDRESS_FULL: The\nfull address of the location - geometry: The geometric coordinate of the respective address\n4\nData Exploration\n4.1\nData Cleaning and Uploading Datasets to Database Tables\n[6]: #TABLE DEFINITIONS\nCRIMETABLE=\"data604_converted_crime\"\nBUDGETTABLE=\"data604_converted_budget\"\nBIKECRIMETABLE=\"data604_Bike_crime\"\nINCOMETABLE=\"data604_income\"\nSHELTEROCCUPANCYTABLE=\"data604_daily_shelter_occupancy\"\n#ReferenceTable\nADDRESSTABLE=\"data604_address_points\"\nSHELTEROCCUPANCY_RAW=\"data604_daily_shelter_occupancy_raw\"\nSHELTEROCCUPANCY_POINTS=\"data604_daily_shelter_occupancy_points\"\nBudget cleaning and uploading to SQL table\n[7]: ### Creation of budget table\nBUDGETTABLE_CREATE = f\"\"\"CREATE TABLE {BUDGETTABLE} (\nID bigint NOT NULL AUTO_INCREMENT,\nyear bigint(20) DEFAULT NULL,\npercent_change double DEFAULT NULL,\nabsolute_budget_change double DEFAULT\u2423\n\u21aaNULL,\nChief double DEFAULT NULL,\n5"
    },
    {
        "page": 115,
        "text": "Communities_and_Neighbourhoods double\u2423\n\u21aaDEFAULT NULL,\nCommunity_Safety double DEFAULT NULL,\nCommunity_Safety_Command double DEFAULT\u2423\n\u21aaNULL,\nCorporate_Services double DEFAULT NULL,\nCorporate_Support double DEFAULT NULL,\nEquipment double DEFAULT NULL,\ngross_operating_budget double DEFAULT\u2423\n\u21aaNULL,\nHuman_Resources double DEFAULT NULL,\nInformation_Technology double DEFAULT\u2423\n\u21aaNULL,\nMaterials double DEFAULT NULL,\nOperational_Support double DEFAULT NULL,\nPriority_Response double DEFAULT NULL,\nSalaries_and_Benefits double DEFAULT NULL,\nServices_and_Rents double DEFAULT NULL,\nSpecialized_Operations double DEFAULT\u2423\n\u21aaNULL,\nPRIMARY KEY (ID)\n);\"\"\"\ncreate_table(DATABASE, BUDGETTABLE,BUDGETTABLE_CREATE)\nTable data604_converted_budget dropped successfully.\nTable data604_converted_budget created successfully.\n[8]: ### Budget cleaning to SQL table\n#First we want to upload our CSV\nbudgetdf=pd.read_csv(\"Gross Operating Budget.csv\")\n#Our csv is not in an ideal format, so we pivot the table so that columns are\u2423\n\u21aanow the subtypes, like gross operating budget ($), Absolute Change ($) etc.\nbudget_df_pivot=budgetdf.pivot_table(\nindex='YEAR',\ncolumns='SUBTYPE',\nvalues='COUNT_'\n).reset_index()\n# some generic cleaning replace NA values w/0.\nShouldn't be an issue as all\u2423\n\u21aathe NA values are for % values\nbudget_df_pivot.fillna(0, inplace=True)\n#strip and replace blanks with _, and & with and for readability\nbudget_df_pivot.columns= budget_df_pivot.columns.str.strip().str.replace('\u2423\n\u21aa','_').str.replace('&','and')\n6"
    },
    {
        "page": 116,
        "text": "#convert to csv so we have the changed csv for later\nbudget_df_pivot.to_csv('converted_budget.csv',index=False)\n#Change column names to prevent issues with SQL queries\nbudget_df=pd.read_csv(\"converted_budget.csv\")\nbudget_df.rename(columns={\"YEAR\":\"year\",\"%_Change\":\n\u21aa\"percent_change\",\"Absolute_Change_($)\":\"absolute_budget_change\",\n\"Gross_Operating_Budget_($)\":\n\u21aa\"gross_operating_budget\"},inplace=True)\nbudget_df['_id']= budget_df.index + 1\nbudget_df.rename(columns = {'_id':'ID'}, inplace = True)\n#Upload csv as SQL table\nbudget_df.to_sql(BUDGETTABLE,engine,index=False,if_exists='append')\nbudget_table_df=pd.read_sql_table(BUDGETTABLE,engine)\nbudget_table_df.head()\n[8]:\nID\nyear\npercent_change\nabsolute_budget_change\nChief\n\\\n0\n1\n2014\n0.062\n63610200.0\n0.006\n1\n2\n2015\n0.017\n17216200.0\n0.006\n2\n3\n2016\n0.026\n28666300.0\n0.008\n3\n4\n2017\n-0.003\n-3267300.0\n0.010\n4\n5\n2018\n0.007\n8209800.0\n0.012\nCommunities_and_Neighbourhoods\nCommunity_Safety\nCommunity_Safety_Command\n\\\n0\n0.000\n0.540\n0.0\n1\n0.000\n0.536\n0.0\n2\n0.311\n0.000\n0.0\n3\n0.317\n0.000\n0.0\n4\n0.295\n0.000\n0.0\nCorporate_Services\nCorporate_Support\nEquipment\ngross_operating_budget\n\\\n0\n0.068\n0.000\n0.002\n1.086002e+09\n1\n0.071\n0.000\n0.003\n1.103218e+09\n2\n0.000\n0.101\n0.002\n1.131884e+09\n3\n0.000\n0.103\n0.002\n1.128617e+09\n4\n0.000\n0.102\n0.002\n1.136827e+09\nHuman_Resources\nInformation_Technology\nMaterials\nOperational_Support\n\\\n0\n0.000\n0.0\n0.019\n0.191\n1\n0.000\n0.0\n0.018\n0.189\n2\n0.047\n0.0\n0.016\n0.000\n3\n0.044\n0.0\n0.016\n0.000\n4\n0.045\n0.0\n0.016\n0.000\n7"
    },
    {
        "page": 117,
        "text": "Priority_Response\nSalaries_and_Benefits\nServices_and_Rents\n\\\n0\n0.000\n0.890\n0.089\n1\n0.000\n0.890\n0.089\n2\n0.386\n0.895\n0.087\n3\n0.374\n0.891\n0.091\n4\n0.374\n0.884\n0.098\nSpecialized_Operations\n0\n0.196\n1\n0.198\n2\n0.147\n3\n0.152\n4\n0.172\nCrime data cleaning and uploading to SQL table\n[9]: ### Creation of crime table\nCRIMETABLE_CREATE = f\"\"\"CREATE TABLE {CRIMETABLE} (\nID BIGINT(20) NOT NULL AUTO_INCREMENT,\nAREA_NAME LONGTEXT DEFAULT NULL,\nHOOD_ID BIGINT(20) DEFAULT NULL,\nPOPULATION_2023 BIGINT(20) DEFAULT NULL,\nGEOMETRY LONGTEXT DEFAULT NULL,\nYEAR BIGINT(20) DEFAULT NULL,\nASSAULT BIGINT(20) DEFAULT NULL,\nASSAULT_RATE DOUBLE DEFAULT NULL,\nAUTOTHEFT DOUBLE DEFAULT NULL,\nAUTOTHEFT_RATE DOUBLE DEFAULT NULL,\nBIKETHEFT DOUBLE DEFAULT NULL,\nBIKETHEFT_RATE DOUBLE DEFAULT NULL,\nBREAKENTER BIGINT(20) DEFAULT NULL,\nBREAKENTER_RATE DOUBLE DEFAULT NULL,\nHOMICIDE DOUBLE DEFAULT NULL,\nHOMICIDE_RATE DOUBLE DEFAULT NULL,\nROBBERY DOUBLE DEFAULT NULL,\nROBBERY_RATE DOUBLE DEFAULT NULL,\nSHOOTING DOUBLE DEFAULT NULL,\nSHOOTING_RATE DOUBLE DEFAULT NULL,\nTHEFTFROMMV BIGINT(20) DEFAULT NULL,\nTHEFTFROMMV_RATE DOUBLE DEFAULT NULL,\nTHEFTOVER DOUBLE DEFAULT NULL,\nTHEFTOVER_RATE DOUBLE DEFAULT NULL,\nPRIMARY KEY (ID)\n);\"\"\"\ncreate_table(DATABASE, CRIMETABLE,CRIMETABLE_CREATE)\nTable data604_converted_crime dropped successfully.\nTable data604_converted_crime created successfully.\n8"
    },
    {
        "page": 118,
        "text": "[10]: #Loading Neighbour Crime Rates\npolygons_gdf=gpd.read_file('./neighbourhood-crime-rates.geojson')\npolygons_gdf.dtypes\npolygons_gdf = polygons_gdf.to_crs(epsg=4326)\npolygons_gdf.head(5)\npolygons_gdf.columns\npolygons_gdf.to_csv(\"neighbourhood-crime-rates.csv\")\n[11]: ### Crime cleaning to SQL table\n#Clean and add crimeset to database\ncrime_df=pd.read_csv(\"neighbourhood-crime-rates.csv\")\ncrime_df.head\ncrime_df = crime_df.loc[:, ~crime_df.columns.str.contains('^Unnamed')]\n#wide_to_long to squish the ASSAULT_2014, ASSAULT_2015, etc. into just ASSAULT\u2423\n\u21aaand add a Year column\nid_cols = ['_id', 'AREA_NAME', 'HOOD_ID', 'POPULATION_2023','geometry']\ncrime_long = pd.wide_to_long(\ncrime_df,\nstubnames=[\n'ASSAULT', 'ASSAULT_RATE', 'AUTOTHEFT', 'AUTOTHEFT_RATE',\n'BIKETHEFT', 'BIKETHEFT_RATE', 'BREAKENTER', 'BREAKENTER_RATE',\n'HOMICIDE', 'HOMICIDE_RATE', 'ROBBERY', 'ROBBERY_RATE',\n'SHOOTING', 'SHOOTING_RATE', 'THEFTFROMMV', 'THEFTFROMMV_RATE',\n'THEFTOVER', 'THEFTOVER_RATE'\n],\ni=id_cols,\nj='Year',\nsep='_'\n).reset_index()\n#store changes as new csv\ncrime_long.to_csv('converted_crime.csv',index=False)\n#Some general cleaning so the csv has no issues with SQL syntax\ncrime_df=pd.read_csv('converted_crime.csv')\ncrime_df.rename(columns={\"Year\":\"year\"},inplace=True)\ncrime_df['_id']= crime_df.index + 1\ncrime_df.rename(columns = {'_id':'ID'}, inplace = True)\n# Comfortable replacing the NAs with 0, because if you inspect the data you can\u2423\n\u21aasee that the NAs only appear in columns\n# where the totals are low values already.\nThis makes me believe that the\u2423\n\u21aamissing data may simply just be 0's\n9"
    },
    {
        "page": 119,
        "text": "crime_df[['ASSAULT', 'ASSAULT_RATE', 'AUTOTHEFT', 'AUTOTHEFT_RATE',\n'BIKETHEFT', 'BIKETHEFT_RATE', 'BREAKENTER', 'BREAKENTER_RATE',\n'HOMICIDE', 'HOMICIDE_RATE', 'ROBBERY', 'ROBBERY_RATE',\n'SHOOTING', 'SHOOTING_RATE', 'THEFTFROMMV', 'THEFTFROMMV_RATE',\n'THEFTOVER', 'THEFTOVER_RATE']]=crime_df[['ASSAULT', 'ASSAULT_RATE',\u2423\n\u21aa'AUTOTHEFT', 'AUTOTHEFT_RATE',\n'BIKETHEFT', 'BIKETHEFT_RATE', 'BREAKENTER', 'BREAKENTER_RATE',\n'HOMICIDE', 'HOMICIDE_RATE', 'ROBBERY', 'ROBBERY_RATE',\n'SHOOTING', 'SHOOTING_RATE', 'THEFTFROMMV', 'THEFTFROMMV_RATE',\n'THEFTOVER', 'THEFTOVER_RATE']].fillna(0)\n#Upload csv as SQL table\ncrime_df.to_sql(CRIMETABLE,engine,index=False,if_exists='append',chunksize=1000)\ncrime_table_df=pd.read_sql_table(CRIMETABLE,engine)\ncrime_table_df.head(10)\n[11]:\nID\nAREA_NAME\nHOOD_ID\nPOPULATION_2023\n\\\n0\n1\nSouth Eglinton-Davisville\n174\n21987\n1\n2\nSouth Eglinton-Davisville\n174\n21987\n2\n3\nSouth Eglinton-Davisville\n174\n21987\n3\n4\nSouth Eglinton-Davisville\n174\n21987\n4\n5\nSouth Eglinton-Davisville\n174\n21987\n5\n6\nSouth Eglinton-Davisville\n174\n21987\n6\n7\nSouth Eglinton-Davisville\n174\n21987\n7\n8\nSouth Eglinton-Davisville\n174\n21987\n8\n9\nSouth Eglinton-Davisville\n174\n21987\n9\n10\nSouth Eglinton-Davisville\n174\n21987\nGEOMETRY\nYEAR\nASSAULT\n\\\n0\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2014\n63\n1\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2015\n61\n2\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2016\n70\n3\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2017\n82\n4\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2018\n85\n5\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2019\n70\n6\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2020\n82\n7\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2021\n121\n8\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2022\n128\n9\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n2023\n101\nASSAULT_RATE\nAUTOTHEFT\nAUTOTHEFT_RATE\n\u2026\nHOMICIDE\nHOMICIDE_RATE\n\\\n0\n344.978638\n5.0\n27.379257\n\u2026\n0.0\n0.000000\n1\n332.135468\n4.0\n21.779375\n\u2026\n0.0\n0.000000\n2\n377.826965\n3.0\n16.192583\n\u2026\n1.0\n5.397528\n3\n429.454285\n8.0\n41.897980\n\u2026\n0.0\n0.000000\n4\n431.581635\n15.0\n76.161461\n\u2026\n0.0\n0.000000\n5\n345.866882\n8.0\n39.527645\n\u2026\n1.0\n4.940956\n10"
    },
    {
        "page": 120,
        "text": "6\n396.844605\n15.0\n72.593521\n\u2026\n1.0\n4.839568\n7\n577.455383\n15.0\n71.585381\n\u2026\n1.0\n4.772358\n8\n597.628174\n10.0\n46.689701\n\u2026\n0.0\n0.000000\n9\n459.362366\n21.0\n95.510986\n\u2026\n0.0\n0.000000\nROBBERY\nROBBERY_RATE\nSHOOTING\nSHOOTING_RATE\nTHEFTFROMMV\n\\\n0\n12.0\n65.710220\n1.0\n5.475852\n18\n1\n10.0\n54.448437\n0.0\n0.000000\n19\n2\n9.0\n48.577751\n1.0\n5.397528\n13\n3\n7.0\n36.660732\n0.0\n0.000000\n17\n4\n17.0\n86.316322\n1.0\n5.077431\n19\n5\n5.0\n24.704779\n1.0\n4.940956\n24\n6\n16.0\n77.433090\n1.0\n4.839568\n43\n7\n11.0\n52.495945\n0.0\n0.000000\n22\n8\n16.0\n74.703522\n1.0\n4.668970\n43\n9\n3.0\n13.644426\n0.0\n0.000000\n40\nTHEFTFROMMV_RATE\nTHEFTOVER\nTHEFTOVER_RATE\n0\n98.565331\n4.0\n21.903406\n1\n103.452034\n3.0\n16.334532\n2\n70.167862\n4.0\n21.590111\n3\n89.033203\n1.0\n5.237247\n4\n96.471184\n3.0\n15.232292\n5\n118.582932\n3.0\n14.822866\n6\n208.101440\n5.0\n24.197842\n7\n104.991890\n6.0\n28.634151\n8\n195.570110\n3.0\n14.006910\n9\n181.925690\n8.0\n36.385136\n[10 rows x 24 columns]\nShelter cleaning, addition of neighborhood and uploading to SQL table\n[12]: ### Cleaning Shelter Occupancy data and adding a neighborhood column\n#First we will upload our daily shelter occupancy data for all years\n#daily_shelter_occupancy='daily_shelter_occupancy'\ndf = pd.read_csv(\"Data_Shelter_Occupancy_Merged.csv\")\ndf=df.drop(columns=['FileUniqueID'],errors='ignore')\ndf['ID']= df.index + 1\n#To upload our df as an SQL we make sure to do it in chunks because the dataset\u2423\n\u21aais too large\ndf.\n\u21aato_sql(SHELTEROCCUPANCY_RAW,engine,index=False,if_exists='replace',chunksize=10000)\n[12]: 156977\n11"
    },
    {
        "page": 121,
        "text": "[13]: ### We then upload our address points dataset, so that we can attach geometric\u2423\n\u21aapoints to the addresses in our shelter data\nADDRESS_POINTS=pd.read_csv('Address Points_Neighbourhoods.csv')\n#Only intered in the id, full address, and geometric points\nADDRESS_POINTS2=ADDRESS_POINTS[['_id','ADDRESS_FULL','geometry']]\nADDRESS_POINTS2.rename(columns = {'_id':'ID',\"geometry\":\"GEOMETRY\"}, inplace =\u2423\n\u21aaTrue)\n#This needs to be done in chunks due to the size of the dataset\nADDRESS_POINTS2.\n\u21aato_sql(ADDRESSTABLE,engine,index=False,if_exists='replace',chunksize=10000)\n[13]: 525460\n[14]: ### Attempt to load our crime dataset geojson file to our SQL tables\n#geo_test=gpd.read_file('neighbourhood-crime-rates.geojson')\n#geo_test.to_postgis(name=\"geo_test\", con=engine, if_exists=\"replace\")\n# When running this code, we see that we are denied permissions to do this\n# Therefore we will simply do the merging outside of it, then convert it to a\u2423\n\u21aacsv and upload the merged\n# dataset to a SQL table\n[15]: #Normalizes the address line so that we can merge occupancy and neighbourhood\u2423\n\u21aageo data\ndef normalize_address(address):\nif isinstance(address, str):\nabbreviations = {\nr'\\bSt\\b': 'Street',\nr'\\bst\\b': 'Street',\nr'\\bAve\\b': 'Avenue',\nr'\\bAve.\\b': 'Avenue',\nr'\\bBlvd\\b': 'Boulevard',\nr'\\bBlvd.\\b': 'Boulevard',\nr'\\bRd\\b': 'Road',\nr'\\bRd.\\b': 'Road',\nr'\\bLn\\b': 'Lane',\nr'\\bLn.\\b': 'Lane',\nr'\\bDr\\b': 'Drive',\nr'\\bDr.\\b': 'Drive',\nr'\\bPkwy\\b': 'Parkway',\nr'\\bPkwy.\\b': 'Parkway',\nr'\\bPl\\b': 'Place',\nr'\\bPl.\\b': 'Place',\nr'\\bCt\\b': 'Court',\nr'\\bCt.\\b': 'Court',\n12"
    },
    {
        "page": 122,
        "text": "r'\\bCrt\\b': 'Court',\nr'\\bW\\b': 'West',\nr'\\bE\\b': 'East',\nr'\\bN\\b': 'North',\nr'\\bS\\b': 'South',\nr'\\bBathrust\\b': 'Bathurst',\nr'\\bToronto\\b': '',\nr'\\b2nd\\b': '',\nr'\\bfloor\\b': ''\n}\nfor abbr, full in abbreviations.items():\naddress = re.sub(abbr, full, address)\naddress = re.sub(r'[.,]','',address)\naddress = address.strip()\nreturn address\n[16]: #Normalize our shelter occupancy data\ndf=SQL(f\"\"\"SELECT * FROM {SHELTEROCCUPANCY_RAW};\"\"\")\ndf['Normalized_ADDRESS']=df['SHELTER_ADDRESS'].apply(lambda x:\u2423\n\u21aanormalize_address(str(x)))\n#Normalize our address points data\ndf2=SQL(f\"\"\"SELECT ADDRESS_FULL, GEOMETRY FROM {ADDRESSTABLE};\"\"\")\ndf2['Normalized_ADDRESS']=df2['ADDRESS_FULL'].apply(lambda x:\u2423\n\u21aanormalize_address(str(x)))\n#Merge the 2 datasets on the Normalized_ADDRESS column, this ensures that our\u2423\n\u21aashelter occupancy data now has\n#geometric points for all the shelter addresses\nmerged_df = df.merge(df2[['Normalized_ADDRESS', 'GEOMETRY']],\u2423\n\u21aaon='Normalized_ADDRESS', how='left')\n#Upload this dataset as an SQL table\nmerged_df = merged_df.loc[:, ~merged_df.columns.str.contains('^Unnamed')]\n#Write DataFrame to SQL in chunks\nmerged_df.\n\u21aato_sql(SHELTEROCCUPANCY_POINTS,engine,index=False,if_exists='replace',chunksize=10000)\n[16]: 159751\n[17]: #Now we will call upon this table, converting the geometric points to a\u2423\n\u21aageometric datatype to be used for merging later\ngeopoints = SQL(f\"\"\"SELECT * FROM {SHELTEROCCUPANCY_POINTS};\"\"\")\n13"
    },
    {
        "page": 123,
        "text": "geopoints = geopoints[(geopoints['OCCUPANCY'] != 0) | (geopoints['CAPACITY'] !=\u2423\n\u21aa0)]\ngeopoints = geopoints.dropna(subset=['GEOMETRY'])\ngeopoints['GEOMETRY'] = geopoints['GEOMETRY'].apply(lambda x: shape(json.\n\u21aaloads(x)))\ngeopoints['GEOMETRY'] = geopoints['GEOMETRY'].apply(shape)\npoints_gdf = gpd.GeoDataFrame(geopoints, geometry='GEOMETRY')\npoints_gdf.set_crs(epsg=4326, inplace=True)\n[17]:\nID OCCUPANCY_DATE\nORGANIZATION_NAME\n\\\n0\n1\n2017-01-01\nCOSTI Immigrant Services\n1\n2\n2017-01-01\nChristie Ossington Neighbourhood Centre\n2\n3\n2017-01-01\nChristie Ossington Neighbourhood Centre\n3\n4\n2017-01-01\nChristie Refugee Welcome Centre, Inc.\n4\n5\n2017-01-01\nCity of Toronto\n\u2026\n\u2026\n\u2026\n\u2026\n159744\n156971\n2020-12-31\nYWCA Toronto\n159747\n156974\n2020-12-31\nYouth Without Shelter\n159748\n156975\n2020-12-31\nYouth Without Shelter\n159749\n156976\n2020-12-31\nYouthLink\n159750\n156977\n2020-12-31\nYouthLink\nSHELTER_NAME\nSHELTER_ADDRESS SHELTER_CITY\n\\\n0\nCOSTI Reception Centre\n100 Lippincott Street\nToronto\n1\nChristie Ossington Men's Hostel\n973 Lansdowne Avenue\nToronto\n2\nChristie Ossington Men's Hostel\n973 Lansdowne Avenue\nToronto\n3\nChristie Refugee Welcome Centre\n43 Christie Street\nToronto\n4\nBirchmount Residence\n1673 Kingston Road\nToronto\n\u2026\n\u2026\n\u2026\n\u2026\n159744\nYWCA - First Stop Woodlawn\n80 Woodlawn Ave. East\nToronto\n159747\nYouth Without Shelter\n6 Warrendale Court\nEtobicoke\n159748\nYouth Without Shelter\n6 Warrendale Court\nEtobicoke\n159749\nYouthLink Shelter\n747 Warden Ave\nToronto\n159750\nYouthLink Shelter\n747 Warden Ave\nToronto\nSHELTER_PROVINCE SHELTER_POSTAL_CODE\nFACILITY_NAME\n\\\n0\nON\nM5S 2P1\nCOSTI Reception Centre\n1\nON\nM6H 3Z5\nChristie Ossington Men's Hostel\n2\nON\nM6H 3Z5\nChristie Ossington Men's Hostel\n3\nON\nM6G 3B1\nChristie Refugee Welcome Centre\n4\nON\nNone\nBirchmount Res 1673 Kingston Rd\n\u2026\n\u2026\n\u2026\n\u2026\n159744\nON\nM4T 1C1\nYWCA-80 Woodlawn Ave. E.-Youth\n159747\nON\nM9V 1P9\nYouth w/o Shelter Emerg Shelter\n159748\nON\nM9V 1P9\nYouth w/o Shltr Transitional Res\n159749\nON\nM1L 4A1\nYouthLink - 747 Warden Ave\n159750\nON\nM1L 4A1\nYouthLink - 747 Warden Ave\n14"
    },
    {
        "page": 124,
        "text": "PROGRAM_NAME\nSECTOR\n\\\n0\nCOSTI Reception Ctr CITY Program\nCo-ed\n1\nChristie Ossington Extreme Weather Program\nMen\n2\nChristie Ossington Men's Hostel\nMen\n3\nChristie Refugee Welcome Ctr - Settlement and \u2026\nFamilies\n4\nBirchmount Residence\nMen\n\u2026\n\u2026\n\u2026\n159744\nYWCA - Youth Shelter\nYouth\n159747\nYouth without Shelter Emergency Shelter Program\nYouth\n159748\nYouth without Shelter Stay In School Program\nYouth\n159749\nYouthLink Emergency Program\nCo-ed\n159750\nYouthLink Transitional Program\nCo-ed\nOCCUPANCY\nCAPACITY\nNormalized_ADDRESS\n\\\n0\n16\n16\n100 Lippincott Street\n1\n13\n17\n973 Lansdowne Avenue\n2\n63\n63\n973 Lansdowne Avenue\n3\n66\n70\n43 Christie Street\n4\n58\n60\n1673 Kingston Road\n\u2026\n\u2026\n\u2026\n\u2026\n159744\n21\n24\n80 Woodlawn Avenue East\n159747\n11\n17\n6 Warrendale Court\n159748\n11\n16\n6 Warrendale Court\n159749\n10\n10\n747 Warden Avenue\n159750\n29\n29\n747 Warden Avenue\nGEOMETRY\n0\nMULTIPOINT (-79.40716 43.65766)\n1\nMULTIPOINT (-79.44591 43.66618)\n2\nMULTIPOINT (-79.44591 43.66618)\n3\nMULTIPOINT (-79.41885 43.66516)\n4\nMULTIPOINT (-79.26399 43.69151)\n\u2026\n\u2026\n159744\nMULTIPOINT (-79.38971 43.68478)\n159747\nMULTIPOINT (-79.58027 43.73636)\n159748\nMULTIPOINT (-79.58027 43.73636)\n159749\nMULTIPOINT (-79.28283 43.71784)\n159750\nMULTIPOINT (-79.28283 43.71784)\n[148683 rows x 15 columns]\n[18]: ### Creation of shelter occupancy table\nSHELTERTABLE_CREATE = f\"\"\"CREATE TABLE {SHELTEROCCUPANCYTABLE} (\nID bigint(20) NOT NULL AUTO_INCREMENT,\nOCCUPANCY_DATE text DEFAULT NULL,\nORGANIZATION_NAME text DEFAULT NULL,\n15"
    },
    {
        "page": 125,
        "text": "SHELTER_NAME text DEFAULT NULL,\nSHELTER_ADDRESS text DEFAULT NULL,\nSHELTER_CITY text DEFAULT NULL,\nSHELTER_PROVINCE text DEFAULT NULL,\nSHELTER_POSTAL_CODE text DEFAULT NULL,\nFACILITY_NAME text DEFAULT NULL,\nPROGRAM_NAME text DEFAULT NULL,\nSECTOR text DEFAULT NULL,\nOCCUPANCY bigint(20) DEFAULT NULL,\nCAPACITY bigint(20) DEFAULT NULL,\nNormalized_ADDRESS text DEFAULT NULL,\nindex_right bigint(20) DEFAULT NULL,\nAREA_NAME text DEFAULT NULL,\nPRIMARY KEY (ID)\n);\"\"\"\ncreate_table(DATABASE, SHELTEROCCUPANCYTABLE,SHELTERTABLE_CREATE)\nTable data604_daily_shelter_occupancy dropped successfully.\nTable data604_daily_shelter_occupancy created successfully.\n[19]: # We will be merging this dataset to our crime dataset on the geometric field,\u2423\n\u21aato see any shelter locations\n# that fall within the geometric polygon of our crime dataset, therefore\u2423\n\u21aatelling us which neighborhood this shelter\n# is located\ncrime_geo=gpd.read_file('neighbourhood-crime-rates.geojson')\ncrime_geo=crime_geo[['AREA_NAME','geometry']]\n#Use a spatial join to determine if a geometric point flls within a certain\u2423\n\u21aageometric polygon\nmerged_df = gpd.sjoin(points_gdf, crime_geo, how='inner', predicate='within')\n#Drop our geometry data as it is no longer needed, and upload data to SQL table\nmerged_df=merged_df.drop(columns=['GEOMETRY'], errors='ignore')\nmerged_df['ID']= merged_df.index + 1\n#This needs to be done in chunks due to the size of the dataset\nmerged_df.\n\u21aato_sql(SHELTEROCCUPANCYTABLE,engine,index=False,if_exists='append',chunksize=10000)\n[19]: 148683\nIncome and education cleaning and uploading to SQL table\n[20]: #We begin by uploading our dataset\ndf20= pd.read_csv(\"neighbourhood-profiles-2021-158-model (1).csv\",header=None)\n16"
    },
    {
        "page": 126,
        "text": "#Select relevant columns\ndf20_clean=pd.DataFrame(columns=df20.columns, data=[df20.iloc[0,:],df20.\n\u21aailoc[67,:],df20.iloc[1981,:],df20.iloc[36,:]])\n#Make neighbourhoods rows\ndisplay(df20_clean)\npivot=df20_clean.T\nhead=pivot.iloc[0]\npivot=pivot[1:]\npivot.columns=head\n#Rename long column names\nincomedf=pivot.rename(columns={'Neighbourhood Name':'Neighbourhood',\n'Total - Highest certificate, diploma or\u2423\n\u21aadegree for the population aged 15 years and over in private households - 25%\u2423\n\u21aasample data':'Degree',\n'Total - Persons in private households - 25%\u2423\n\u21aasample data':'Population',\n'\nAverage after-tax income in 2020 among\u2423\n\u21aarecipients ($)':'Income'})\n#Create year variable\nincomedf['YEAR']=2020\n#Convert number columns to int\nincomedf=incomedf.astype({'Degree':'int'})\nincomedf=incomedf.astype({'Population':'int'})\nincomedf=incomedf.astype({'Income':'int'})\n#Create degree rate variable\nincomedf['Degree Rate']=incomedf['Degree']/incomedf['Population']\nincomedf.insert(0, 'ID',incomedf.index)\nincomedf.rename(columns = {'Degree Rate':'Degree_Rate'}, inplace = True)\nincomedf\n0\n\\\n0\nNeighbourhood Name\n67\nAverage after-tax income in 2020 among rec\u2026\n1981\nTotal - Highest certificate, diploma or degree\u2026\n36\nTotal - Persons in private households - 25% sa\u2026\n1\n2\n\\\n0\nWest Humber-Clairville\nMount Olive-Silverstone-Jamestown\n67\n35800\n31760\n1981\n29000\n25655\n36\n33300\n31345\n3\n4\n5\n\\\n0\nThistletown-Beaumond Heights\nRexdale-Kipling\nElms-Old Rexdale\n67\n36360\n36880\n36440\n1981\n8350\n8805\n7745\n36\n9850\n10375\n9355\n17"
    },
    {
        "page": 127,
        "text": "6\n7\n\\\n0\nKingsview Village-The Westway\nWillowridge-Martingrove-Richview\n67\n38640\n45160\n1981\n18090\n18945\n36\n22005\n22445\n8\n9\n\u2026\n\\\n0\nHumber Heights-Westmount\nEdenbridge-Humber Valley\n\u2026\n67\n50160\n68700\n\u2026\n1981\n8635\n13120\n\u2026\n36\n10005\n15190\n\u2026\n149\n150\n\\\n0\nHarbourfront-CityPlace\nSt Lawrence-East Bayfront-The Islands\n67\n57700\n61650\n1981\n26060\n29010\n36\n28135\n31285\n151\n152\n153\n\\\n0\nChurch-Wellesley\nDowntown Yonge East\nBay-Cloverhill\n67\n46360\n52900\n50960\n1981\n21420\n16640\n15930\n36\n22320\n17700\n16670\n154\n155\n156\n\\\n0\nYonge-Bay Corridor\nJunction-Wallace Emerson\nDovercourt Village\n67\n53050\n45480\n46200\n1981\n11675\n20105\n11015\n36\n12645\n23180\n12380\n157\n158\n0\nNorth Toronto\nSouth Eglinton-Davisville\n67\n48200\n54700\n1981\n14570\n20545\n36\n15885\n22735\n[4 rows x 159 columns]\n[20]: 0\nID\nNeighbourhood\nIncome\nDegree\nPopulation\nYEAR\n\\\n1\n1\nWest Humber-Clairville\n35800\n29000\n33300\n2020\n2\n2\nMount Olive-Silverstone-Jamestown\n31760\n25655\n31345\n2020\n3\n3\nThistletown-Beaumond Heights\n36360\n8350\n9850\n2020\n4\n4\nRexdale-Kipling\n36880\n8805\n10375\n2020\n5\n5\nElms-Old Rexdale\n36440\n7745\n9355\n2020\n..\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n154\n154\nYonge-Bay Corridor\n53050\n11675\n12645\n2020\n18"
    },
    {
        "page": 128,
        "text": "155\n155\nJunction-Wallace Emerson\n45480\n20105\n23180\n2020\n156\n156\nDovercourt Village\n46200\n11015\n12380\n2020\n157\n157\nNorth Toronto\n48200\n14570\n15885\n2020\n158\n158\nSouth Eglinton-Davisville\n54700\n20545\n22735\n2020\n0\nDegree_Rate\n1\n0.870871\n2\n0.818472\n3\n0.847716\n4\n0.848675\n5\n0.827900\n..\n\u2026\n154\n0.923290\n155\n0.867343\n156\n0.889742\n157\n0.917218\n158\n0.903673\n[158 rows x 7 columns]\n[21]: INCOMETABLE_CREATE = f\"\"\"CREATE TABLE {INCOMETABLE} (\nID bigint(20) NOT NULL AUTO_INCREMENT,\nNeighbourhood text DEFAULT NULL,\nIncome bigint(20) DEFAULT NULL,\nDegree bigint(20) DEFAULT NULL,\nPopulation bigint(20) DEFAULT NULL,\nYEAR bigint(20) DEFAULT NULL,\nDegree_Rate double DEFAULT NULL,\nPRIMARY KEY (ID)\n);\"\"\"\ncreate_table(DATABASE, INCOMETABLE,INCOMETABLE_CREATE)\nTable data604_income dropped successfully.\nTable data604_income created successfully.\n[22]: #Make table\nincomedf.to_sql(INCOMETABLE,engine,index=False,if_exists='append')\nincome_table=pd.read_sql_table(INCOMETABLE,engine)\ncrime_table_df=pd.read_sql_table(CRIMETABLE,engine)\nincomedf\n[22]: 0\nID\nNeighbourhood\nIncome\nDegree\nPopulation\nYEAR\n\\\n1\n1\nWest Humber-Clairville\n35800\n29000\n33300\n2020\n2\n2\nMount Olive-Silverstone-Jamestown\n31760\n25655\n31345\n2020\n3\n3\nThistletown-Beaumond Heights\n36360\n8350\n9850\n2020\n4\n4\nRexdale-Kipling\n36880\n8805\n10375\n2020\n5\n5\nElms-Old Rexdale\n36440\n7745\n9355\n2020\n..\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n19"
    },
    {
        "page": 129,
        "text": "154\n154\nYonge-Bay Corridor\n53050\n11675\n12645\n2020\n155\n155\nJunction-Wallace Emerson\n45480\n20105\n23180\n2020\n156\n156\nDovercourt Village\n46200\n11015\n12380\n2020\n157\n157\nNorth Toronto\n48200\n14570\n15885\n2020\n158\n158\nSouth Eglinton-Davisville\n54700\n20545\n22735\n2020\n0\nDegree_Rate\n1\n0.870871\n2\n0.818472\n3\n0.847716\n4\n0.848675\n5\n0.827900\n..\n\u2026\n154\n0.923290\n155\n0.867343\n156\n0.889742\n157\n0.917218\n158\n0.903673\n[158 rows x 7 columns]\nBike rack cleaning and uploading to SQL table\n[23]: ### Bike rack cleaning\n# cleann bike rack csv\n# read csv\nbikerack_df = pd.read_csv('Bicycle Parking Racks Data - 4326.csv')\n# drop columns that aren't needed\nbikerack_df = bikerack_df.drop(columns=['ADDRESS_POINT_ID',\u2423\n\u21aa'ADDRESS_NUMBER','LINEAR_NAME_FULL','CENTRELINE_ID','LO_NUM','LO_NUM_SUF','HI_NUM','HI_NUM_\nbikerack_df_cleaned = bikerack_df[bikerack_df['STATUS'].str.lower() ==\u2423\n\u21aa'installed'] # make sure the bike rack is installed\n# drop rows with missing values\nbikerack_df_cleaned = bikerack_df_cleaned.dropna(subset=['ADDRESS_FULL',\u2423\n\u21aa'POSTAL_CODE', 'WARD_NAME'])\n#make sure capacity is a number\nbikerack_df_cleaned['CAPACITY'] = pd.\n\u21aato_numeric(bikerack_df_cleaned['CAPACITY'], errors='coerce')\nbikerack_df_cleaned = bikerack_df_cleaned.dropna(subset=['CAPACITY']) # get rid\u2423\n\u21aaof rows with no capacity\nbikerack_df_cleaned = bikerack_df_cleaned.reset_index(drop=True)\n20"
    },
    {
        "page": 130,
        "text": "# save new csv\nbikerack_df_cleaned.to_csv('Cleaned_Bicycle_Parking_Racks_Data.csv', index =\u2423\n\u21aaFalse)\n### Crime data pivot and clean\n# read geojson\ncrime_df = gpd.read_file('neighbourhood-crime-rates.geojson')\n# set column names\nid_cols = ['_id', 'AREA_NAME', 'HOOD_ID', 'POPULATION_2023','geometry']\n#wide_to_long to squish the ASSAULT_2014, ASSAULT_2015, etc. into just ASSAULT\u2423\n\u21aaand add a Year column\ncrime_long = pd.wide_to_long(\ncrime_df,\nstubnames=[\n'ASSAULT', 'ASSAULT_RATE', 'AUTOTHEFT', 'AUTOTHEFT_RATE',\n'BIKETHEFT', 'BIKETHEFT_RATE', 'BREAKENTER', 'BREAKENTER_RATE',\n'HOMICIDE', 'HOMICIDE_RATE', 'ROBBERY', 'ROBBERY_RATE',\n'SHOOTING', 'SHOOTING_RATE', 'THEFTFROMMV', 'THEFTFROMMV_RATE',\n'THEFTOVER', 'THEFTOVER_RATE'],\ni=id_cols,\nj='Year',\n#makes it so Year is a column\nsep='_'\n).reset_index()\n# save new csv\ncrime_long.to_csv('crime_long2.csv', index=False)\n### clean crime_long2\n#read file\ncrime_df = pd.read_csv('crime_long2.csv')\n# drop rows with missing values\ncrime_df = crime_df.dropna(subset=['BIKETHEFT', 'BIKETHEFT_RATE', 'HOOD_ID'])\n# drop unnecessary columns\ncolumns_to_drop = ['ASSAULT', 'ASSAULT_RATE', 'AUTOTHEFT', 'AUTOTHEFT_RATE',\n'BREAKENTER', 'BREAKENTER_RATE',\n'HOMICIDE', 'HOMICIDE_RATE', 'ROBBERY', 'ROBBERY_RATE',\n'SHOOTING', 'SHOOTING_RATE', 'THEFTFROMMV', 'THEFTFROMMV_RATE',\n'THEFTOVER', 'THEFTOVER_RATE']\ncrime_df_cleaned = crime_df.drop(columns=columns_to_drop)\n21"
    },
    {
        "page": 131,
        "text": "# convert POPULATION_2023 column to numeric\ncrime_df_cleaned['POPULATION_2023'] = pd.\n\u21aato_numeric(crime_df_cleaned['POPULATION_2023'], errors='coerce')\ncrime_df_cleaned = crime_df_cleaned.dropna(subset=['POPULATION_2023'])\n# reset the index\ncrime_df_cleaned = crime_df_cleaned.reset_index(drop=True)\n# save new csv\ncrime_df_cleaned.to_csv('Cleaned_crime_long_Data.csv', index=False)\n### Merging Bike and crime\n# read cleaned files\nbikerack_df = pd.read_csv('Cleaned_Bicycle_Parking_Racks_Data.csv')\ncrime_stats_df = pd.read_csv('Cleaned_crime_long_Data.csv')\n# turn geometry to a string for bike racks\nif isinstance(bikerack_df['geometry'].iloc[0], str):\ntry:\nbikerack_df['geometry'] = bikerack_df['geometry'].apply(lambda x:\u2423\n\u21aashape(json.loads(x)))\nexcept (json.JSONDecodeError, TypeError):\nbikerack_df['geometry'] = gpd.GeoSeries.\n\u21aafrom_wkt(bikerack_df['geometry'])\n# turn geometry to a string for crime\nif isinstance(crime_stats_df['geometry'].iloc[0], str):\ntry:\ncrime_stats_df['geometry'] = crime_stats_df['geometry'].apply(lambda x:\u2423\n\u21aashape(json.loads(x)))\nexcept (json.JSONDecodeError, TypeError):\ncrime_stats_df['geometry'] = gpd.GeoSeries.\n\u21aafrom_wkt(crime_stats_df['geometry'])\n# turn dataframes into geodataframes\nbikerack_gdf = gpd.GeoDataFrame(bikerack_df, geometry='geometry')\ncrime_stats_gdf = gpd.GeoDataFrame(crime_stats_df, geometry='geometry')\n# set coordinate reference systen\nbikerack_gdf = bikerack_gdf.set_crs(epsg=4326)\ncrime_stats_gdf = crime_stats_gdf.set_crs(epsg=4326)\n# function to check if points are inside polygons\ndef check_points_in_polygons(points_gdf, polygons_gdf, output_file=None):\n22"
    },
    {
        "page": 132,
        "text": "if not isinstance(points_gdf, gpd.GeoDataFrame) or not\u2423\n\u21aaisinstance(polygons_gdf, gpd.GeoDataFrame):\nraise ValueError(\"Both inputs must be GeoDataFrames.\")\n# join on points and polygons\nresult = gpd.sjoin(points_gdf, polygons_gdf, how='inner',\u2423\n\u21aapredicate='within')\n# save file to right type\nif output_file:\nif output_file.endswith('.csv'):\nresult.to_csv(output_file, index=False)\nelif output_file.endswith('.geojson'):\nresult.to_file(output_file, driver='GeoJSON')\nelif output_file.endswith('.shp'):\nresult.to_file(output_file)\nelse:\nraise ValueError(\"Unsupported file format. Use .csv, .geojson, or .\n\u21aashp.\")\nreturn result\n# save new joined csv\noutput = check_points_in_polygons(bikerack_gdf, crime_stats_gdf,\u2423\n\u21aaoutput_file='Bike_crime.csv')\n[24]: BIKETABLE_CREATE = f\"\"\"CREATE TABLE {BIKECRIMETABLE} (\nID bigint(20) NOT NULL AUTO_INCREMENT,\nid_left bigint(20) DEFAULT NULL,\nADDRESS_FULL text DEFAULT NULL,\nPOSTAL_CODE text DEFAULT NULL,\nMUNICIPALITY text DEFAULT NULL,\nCITY text DEFAULT NULL,\nWARD_NAME text DEFAULT NULL,\nCAPACITY bigint(20) DEFAULT NULL,\nSHELTERED text DEFAULT NULL,\nSTATUS text DEFAULT NULL,\nMAP_CLASS text DEFAULT NULL,\ngeometry text DEFAULT NULL,\nindex_right bigint(20) DEFAULT NULL,\nid_right bigint(20) DEFAULT NULL,\nAREA_NAME text DEFAULT NULL,\nHOOD_ID double DEFAULT NULL,\nPOPULATION_2023 bigint(20) DEFAULT NULL,\nYear bigint(20) DEFAULT NULL,\nBIKETHEFT bigint(20) DEFAULT NULL,\nBIKETHEFT_RATE double DEFAULT NULL,\nPRIMARY KEY (ID)\n23"
    },
    {
        "page": 133,
        "text": ");\"\"\"\ncreate_table(DATABASE, BIKECRIMETABLE,BIKETABLE_CREATE)\nTable data604_Bike_crime dropped successfully.\nTable data604_Bike_crime created successfully.\n[25]: ### Create Bike_crime table\n# read file\ndf = pd.read_csv('Bike_crime.csv')\ndf.insert(0, 'ID',df.index+1)\ndf.rename(columns = {'_id_left':'id_left','_id_right':'id_right'}, inplace =\u2423\n\u21aaTrue)\n# save file to a table\ndf.to_sql(BIKECRIMETABLE, engine, if_exists='append', index=False)\n[25]: 1390\n[26]: SQL(\"SHOW TABLES\")\n[26]:\nTables_in_project\n0\ndata604_Bike_crime\n1\ndata604_address_points\n2\ndata604_converted_budget\n3\ndata604_converted_crime\n4\ndata604_daily_shelter_occupancy\n5\ndata604_daily_shelter_occupancy_points\n6\ndata604_daily_shelter_occupancy_raw\n7\ndata604_income\n24"
    },
    {
        "page": 134,
        "text": "4.1.1\nEntity relationship Diagram\n4.2\nData Exploration for Each Guiding Question\nPolice Budget vs Crime Statistics - Aaron Gelfand\n[27]: # query for inspecting relationship between police budget and crime stats\n# create a dataframe from our budget table and crime table which looks at all\u2423\n\u21aacrime total counts and avg crime rates\n#Also take our total budget in billions and budget change in billions\n# merges the two into one table using the year column\ncrime_query= f\"\"\"\nWITH CrimeTotals AS (\nSELECT\nyear,\nSUM(ASSAULT) as total_assault,\nSUM(AUTOTHEFT) as total_autotheft,\nSUM(BIKETHEFT) as total_biketheft,\nSUM(BREAKENTER) as total_breakenter,\nSUM(HOMICIDE) as total_homicide,\n25"
    },
    {
        "page": 135,
        "text": "SUM(ROBBERY) as total_robbery,\nSUM(SHOOTING) as total_shooting,\nSUM(THEFTFROMMV) as total_theftfrommv,\nSUM(THEFTOVER) as total_theftover,\nAVG(ASSAULT_RATE) as avg_assault_rate,\nAVG(AUTOTHEFT_RATE) as avg_autotheft_rate,\nAVG(BIKETHEFT_RATE) as avg_biketheft_rate,\nAVG(BREAKENTER_RATE) as avg_breakenter_rate,\nAVG(HOMICIDE_RATE) as avg_homicide_rate,\nAVG(ROBBERY_RATE) as avg_robbery_rate,\nAVG(SHOOTING_RATE) as avg_shooting_rate,\nAVG(THEFTFROMMV_RATE) as avg_theftfrommv_rate,\nAVG(THEFTOVER_RATE) as avg_theftover_rate\nFROM {CRIMETABLE}\nGROUP BY year\n),\nBudgetData AS (\nSELECT\nyear,\nCAST(SUM(gross_operating_budget) / 1000000000 AS FLOAT) AS\u2423\n\u21aatotal_budget_in_billions,\nCAST(SUM(absolute_budget_change) / 1000000000 AS FLOAT) AS\u2423\n\u21aabudget_change_in_billions,\npercent_change\nFROM {BUDGETTABLE}\nGROUP BY year\n)\nSELECT\nB.year,\nB.total_budget_in_billions,\nB.budget_change_in_billions,\nB.percent_change,\nC.total_assault,\nC.total_autotheft,\nC.total_biketheft,\nC.total_breakenter,\nC.total_homicide,\nC.total_robbery,\nC.total_shooting,\nC.total_theftfrommv,\nC.total_theftover,\nC.avg_assault_rate,\nC.avg_autotheft_rate,\nC.avg_biketheft_rate,\nC.avg_breakenter_rate,\n26"
    },
    {
        "page": 136,
        "text": "C.avg_homicide_rate,\nC.avg_robbery_rate,\nC.avg_shooting_rate,\nC.avg_theftfrommv_rate,\nC.avg_theftover_rate\nFROM BudgetData B\nJoin CrimeTotals C\nOn B.year=C.year\nORDER BY B.year;\n\"\"\"\nresult_df=SQL(crime_query)\nSQL(crime_query)\n[27]:\nyear\ntotal_budget_in_billions\nbudget_change_in_billions\npercent_change\n\\\n0\n2014\n1.08600\n0.063610\n0.062000\n1\n2015\n1.10322\n0.017216\n0.017000\n2\n2016\n1.13188\n0.028666\n0.026000\n3\n2017\n1.12862\n-0.003267\n-0.003000\n4\n2018\n1.13683\n0.008210\n0.007000\n5\n2019\n1.20194\n0.065113\n0.057000\n6\n2020\n1.22122\n0.019276\n0.016000\n7\n2021\n1.22000\n-0.001215\n-0.000995\n8\n2022\n1.26243\n0.042427\n0.033610\n9\n2023\n1.33063\n0.068197\n0.054020\ntotal_assault\ntotal_autotheft\ntotal_biketheft\ntotal_breakenter\n\\\n0\n16530.0\n3576.0\n3023.0\n7185.0\n1\n17875.0\n3262.0\n3294.0\n6911.0\n2\n18626.0\n3311.0\n3798.0\n6402.0\n3\n18928.0\n3568.0\n3856.0\n6883.0\n4\n19586.0\n4774.0\n3967.0\n7565.0\n5\n20602.0\n5285.0\n3704.0\n8438.0\n6\n17974.0\n5731.0\n3915.0\n6914.0\n7\n19012.0\n6579.0\n3154.0\n5671.0\n8\n21024.0\n9662.0\n2940.0\n6038.0\n9\n24376.0\n12013.0\n3008.0\n7632.0\ntotal_homicide\ntotal_robbery\n\u2026\ntotal_theftover\navg_assault_rate\n\\\n0\n58.0\n3668.0\n\u2026\n993.0\n610.401795\n1\n59.0\n3472.0\n\u2026\n1035.0\n656.997006\n2\n75.0\n3654.0\n\u2026\n1030.0\n670.169852\n3\n65.0\n4008.0\n\u2026\n1167.0\n673.530257\n4\n98.0\n3623.0\n\u2026\n1230.0\n680.128628\n5\n79.0\n3508.0\n\u2026\n1368.0\n706.708804\n6\n71.0\n2773.0\n\u2026\n1209.0\n605.126466\n7\n85.0\n2243.0\n\u2026\n1053.0\n637.650870\n27"
    },
    {
        "page": 137,
        "text": "8\n71.0\n2807.0\n\u2026\n1444.0\n703.030473\n9\n72.0\n3149.0\n\u2026\n1724.0\n806.310513\navg_autotheft_rate\navg_biketheft_rate\navg_breakenter_rate\n\\\n0\n122.894735\n127.664053\n268.183005\n1\n113.846110\n133.504008\n255.802629\n2\n113.862644\n149.693942\n234.708454\n3\n120.558686\n148.402819\n248.018052\n4\n159.456077\n150.450802\n264.205776\n5\n175.019654\n135.200693\n293.590049\n6\n186.981110\n142.251404\n241.965059\n7\n217.026578\n112.199870\n193.507392\n8\n316.850521\n105.244566\n205.555160\n9\n382.173675\n102.847534\n258.216067\navg_homicide_rate\navg_robbery_rate\navg_shooting_rate\n\\\n0\n2.112469\n136.057003\n6.274449\n1\n2.212523\n124.980440\n10.707640\n2\n2.696912\n131.174051\n14.310603\n3\n2.438607\n141.363362\n13.704951\n4\n3.630148\n127.874208\n14.227553\n5\n2.882434\n120.688364\n16.807147\n6\n2.220783\n92.661019\n14.691605\n7\n2.974706\n75.648864\n13.390221\n8\n2.344280\n92.461004\n12.401718\n9\n2.251066\n103.874248\n11.212980\navg_theftfrommv_rate\navg_theftover_rate\n0\n353.516855\n36.385880\n1\n320.224055\n37.096303\n2\n276.356090\n36.700544\n3\n294.381803\n40.144456\n4\n312.275849\n41.853118\n5\n339.260389\n46.163937\n6\n345.983044\n40.760462\n7\n270.300717\n34.193171\n8\n300.305751\n46.940361\n9\n277.921576\n54.832638\n[10 rows x 22 columns]\n[28]: #Plot out operating budget vs avg crime rates, with budget as bars and crime\u2423\n\u21aarates as lines, so you can see the change in both over time\nfig, axs = plt.subplots(2,2,figsize=(15,12))\naxs[0, 0].bar(result_df['year'], result_df['total_budget_in_billions'],\u2423\n\u21aacolor='tab:blue', alpha=0.4, label='Total Budget (Billions)')\n28"
    },
    {
        "page": 138,
        "text": "axs[0, 0].set_xlabel('Year')\naxs[0, 0].set_ylabel('Total Budget (in billions)', color='tab:blue')\naxs[0, 0].tick_params(axis='y', labelcolor='tab:blue')\nax2 = axs[0, 0].twinx()\nax2.plot(result_df['year'], result_df['avg_assault_rate'], label='Average\u2423\n\u21aaAssault Rates', color='magenta', linewidth=2)\nax2.set_ylabel('Crime Rates', color='green')\nax2.tick_params(axis='y', labelcolor='green')\naxs[0, 0].set_title('Operating Budget vs. Average Assault Rates per 100,000\u2423\n\u21aaPopulation')\naxs[0, 1].bar(result_df['year'], result_df['total_budget_in_billions'],\u2423\n\u21aacolor='tab:blue', alpha=0.4)\naxs[0, 1].set_xlabel('Year')\naxs[0, 1].set_ylabel('Total Budget (in billions)', color='tab:blue')\naxs[0, 1].tick_params(axis='y', labelcolor='tab:blue')\nax2 = axs[0, 1].twinx()\nax2.plot(result_df['year'], result_df['avg_autotheft_rate'], label='Average\u2423\n\u21aaAuto Theft Rates', color='tab:orange', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_breakenter_rate'], label='Average\u2423\n\u21aaBreak and Enter Rates', color='tab:green', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_biketheft_rate'], label='Average\u2423\n\u21aaBike Theft Rates', color='tab:red', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_robbery_rate'], label='Average\u2423\n\u21aaRobbery Rates', color='tab:blue', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_theftfrommv_rate'], label='Average\u2423\n\u21aaTheft from Moving Vehicle Rates', color='black', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_theftover_rate'], label='Average\u2423\n\u21aaThefts Over $5000 Rates', color='tab:brown', linewidth=2)\nax2.set_ylabel('Crime Rates', color='green')\nax2.tick_params(axis='y', labelcolor='green')\naxs[0, 1].set_title('Operating Budget vs. Average Rates for Multiple Crimes per\u2423\n\u21aa100,000 Population')\naxs[1, 0].bar(result_df['year'], result_df['total_budget_in_billions'],\u2423\n\u21aacolor='tab:blue', alpha=0.4)\naxs[1, 0].set_xlabel('Year')\naxs[1, 0].set_ylabel('Total Budget (in billions)', color='tab:blue')\naxs[1, 0].tick_params(axis='y', labelcolor='tab:blue')\nax2 = axs[1, 0].twinx()\nax2.plot(result_df['year'], result_df['avg_homicide_rate'], label='Average\u2423\n\u21aaHomicide Rates', color='purple', linewidth=2)\nax2.plot(result_df['year'], result_df['avg_shooting_rate'], label='Average\u2423\n\u21aaShooting Rates', color='teal', linewidth=2)\nax2.set_ylabel('Crime Rates', color='green')\nax2.tick_params(axis='y', labelcolor='green')\n29"
    },
    {
        "page": 139,
        "text": "axs[1, 0].set_title('Operating Budget vs. Average Homicide and Shooting Rates\u2423\n\u21aaper 100,000 Population')\naxs[1, 1].axis('off')\nfig.suptitle('Operating Budget vs. Average Crime Statistic Rates per 100,000\u2423\n\u21aaPopulation Over Time', fontsize=16)\nfig.tight_layout(rect=[0, 0, 1, 0.96])\n# Adjust layout to avoid overlap\nfig.legend(loc='upper left', bbox_to_anchor=(0.6, 0.4))\nplt.show()\n[29]: # Our sample size for budgetary data is too small (n=10) therefore we decided\u2423\n\u21aato run bootstrapping\ntarget_columns = [\n'avg_assault_rate', 'avg_autotheft_rate', 'avg_biketheft_rate',\u2423\n\u21aa'avg_breakenter_rate',\n'avg_homicide_rate', 'avg_robbery_rate', 'avg_shooting_rate',\u2423\n\u21aa'avg_theftfrommv_rate',\n30"
    },
    {
        "page": 140,
        "text": "'avg_theftover_rate'\n]\nn_iter=1000\nboot_results={}\nfor target in target_columns:\ny = result_df[target]\nX = result_df[['total_budget_in_billions']]\nX = sm.add_constant(X)\nboot_coefs=[]\nfor _ in range(n_iter):\nresample_indices=np.random.choice(len(result_df),\u2423\n\u21aalen(result_df),replace = True)\nX_resample=X.iloc[resample_indices]\ny_resample=y.iloc[resample_indices]\nmodel = sm.OLS(y_resample, X_resample).fit()\nboot_coefs.append(model.params.values)\nboot_coefs_df=pd.DataFrame(boot_coefs,columns=['Intercept', 'Slope'])\nboot_results[target]=boot_coefs_df\nintercept_mean = boot_coefs_df['Intercept'].mean()\nintercept_std = boot_coefs_df['Intercept'].std()\nslope_mean = boot_coefs_df['Slope'].mean()\nslope_std = boot_coefs_df['Slope'].std()\nprint(f\"Bootstrap Results for {target}:\")\nprint(f\"Intercept: Mean = {intercept_mean:.3f}, Std Dev = {intercept_std:.\n\u21aa3f}\")\nprint(f\"Slope: Mean = {slope_mean:.3f}, Std Dev = {slope_std:.3f}\")\nprint(f\"95% CI for Slope: {np.percentile(boot_coefs_df['Slope'], [2.5, 97.\n\u21aa5])}\")\nprint()\nBootstrap Results for avg_assault_rate:\nIntercept: Mean = 200.268, Std Dev = 295.023\nSlope: Mean = 399.191, Std Dev = 256.663\n95% CI for Slope: [-219.29111869\n723.56022077]\nBootstrap Results for avg_autotheft_rate:\nIntercept: Mean = -1089.792, Std Dev = 210.396\nSlope: Mean = 1080.958, Std Dev = 181.100\n95% CI for Slope: [ 587.81773088 1344.30914803]\nBootstrap Results for avg_biketheft_rate:\nIntercept: Mean = 323.732, Std Dev = 87.799\nSlope: Mean = -162.449, Std Dev = 74.914\n31"
    },
    {
        "page": 141,
        "text": "95% CI for Slope: [-290.01173223\n-5.53325095]\nBootstrap Results for avg_breakenter_rate:\nIntercept: Mean = 400.265, Std Dev = 151.882\nSlope: Mean = -130.903, Std Dev = 133.942\n95% CI for Slope: [-414.43424544\n80.83283158]\nBootstrap Results for avg_homicide_rate:\nIntercept: Mean = 3.224, Std Dev = 2.802\nSlope: Mean = -0.515, Std Dev = 2.346\n95% CI for Slope: [-5.15405213\n4.147113\n]\nBootstrap Results for avg_robbery_rate:\nIntercept: Mean = 384.456, Std Dev = 97.799\nSlope: Mean = -228.910, Std Dev = 84.969\n95% CI for Slope: [-433.54062493 -114.94065184]\nBootstrap Results for avg_shooting_rate:\nIntercept: Mean = 0.052, Std Dev = 19.554\nSlope: Mean = 10.984, Std Dev = 16.467\n95% CI for Slope: [-15.91244638\n48.76534223]\nBootstrap Results for avg_theftfrommv_rate:\nIntercept: Mean = 456.387, Std Dev = 157.016\nSlope: Mean = -125.196, Std Dev = 135.189\n95% CI for Slope: [-338.05800975\n187.33348451]\nBootstrap Results for avg_theftover_rate:\nIntercept: Mean = -23.802, Std Dev = 24.808\nSlope: Mean = 55.020, Std Dev = 21.780\n95% CI for Slope: [-2.67892237 79.21359648]\n[30]: #Plotted out our significant relationships from the output of our bootstrapped\u2423\n\u21aaanalysis (instances where 95% CI did not contain 0)\n#These were auto theft, bike theft, theft over $5000, and robbery\n#Plotted out 100 of the bootstrapped regression lines, as well as the mean\u2423\n\u21aaregression line\ndef add_bootstrap_lines(ax, x_data, boot_results, n_lines=100, color='gray',\u2423\n\u21aaalpha=0.2):\n\"\"\"Add bootstrapped regression lines to the plot\"\"\"\nfor i in range(min(n_lines, len(boot_results))):\nslope = boot_results.iloc[i]['Slope']\nintercept = boot_results.iloc[i]['Intercept']\nx_vals = np.array([x_data.min(), x_data.max()])\ny_vals = slope * x_vals + intercept\nax.plot(x_vals, y_vals, color=color, alpha=alpha)\n32"
    },
    {
        "page": 142,
        "text": "def add_mean_regression_line(ax, x_data, boot_results, data_color, label='Mean\u2423\n\u21aaRegression Line'):\n\"\"\"Add the mean regression line to the plot\"\"\"\nmean_slope = boot_results['Slope'].mean()\nmean_intercept = boot_results['Intercept'].mean()\nx_vals = np.array([x_data.min(), x_data.max()])\ny_vals = mean_slope * x_vals + mean_intercept\nax.plot(x_vals, y_vals, color=data_color, label=label, linewidth=2)\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\naxs[0].scatter(result_df['total_budget_in_billions'],\u2423\n\u21aaresult_df['avg_autotheft_rate'],\ncolor='tab:orange', label='Auto Theft Rates')\nadd_bootstrap_lines(axs[0], result_df['total_budget_in_billions'],\u2423\n\u21aaboot_results['avg_autotheft_rate'])\nadd_mean_regression_line(axs[0], result_df['total_budget_in_billions'],\nboot_results['avg_autotheft_rate'], data_color='tab:\n\u21aaorange')\naxs[0].scatter(result_df['total_budget_in_billions'],\u2423\n\u21aaresult_df['avg_biketheft_rate'],\ncolor='tab:green', label='Bike Theft Rates')\nadd_bootstrap_lines(axs[0], result_df['total_budget_in_billions'],\u2423\n\u21aaboot_results['avg_biketheft_rate'])\nadd_mean_regression_line(axs[0], result_df['total_budget_in_billions'],\nboot_results['avg_biketheft_rate'], data_color='tab:\n\u21aagreen')\naxs[0].set_xlabel('Total Budget (in billions)', fontsize=14)\naxs[0].set_ylabel('Average Crime Rates', fontsize=14)\naxs[0].set_title('Operating Budget vs. Average Rates of Auto and Bike\u2423\n\u21aaTheft',fontsize=14)\naxs[1].scatter(result_df['total_budget_in_billions'],\u2423\n\u21aaresult_df['avg_theftover_rate'],\ncolor='purple', label='Theft Over $5000 Rates')\nadd_bootstrap_lines(axs[1], result_df['total_budget_in_billions'],\u2423\n\u21aaboot_results['avg_theftover_rate'])\nadd_mean_regression_line(axs[1], result_df['total_budget_in_billions'],\nboot_results['avg_theftover_rate'],\u2423\n\u21aadata_color='purple')\naxs[1].scatter(result_df['total_budget_in_billions'],\u2423\n\u21aaresult_df['avg_robbery_rate'],\ncolor='teal', label='Robbery Rates')\nadd_bootstrap_lines(axs[1], result_df['total_budget_in_billions'],\u2423\n\u21aaboot_results['avg_robbery_rate'])\n33"
    },
    {
        "page": 143,
        "text": "add_mean_regression_line(axs[1], result_df['total_budget_in_billions'],\nboot_results['avg_robbery_rate'], data_color='teal')\naxs[1].set_xlabel('Total Budget (in billions)', fontsize=14)\naxs[1].set_ylabel('Average Crime Rates', fontsize=14)\naxs[1].set_title('Operating Budget vs. Theft Over $5000 and Robbery\u2423\n\u21aaRates',fontsize=14)\nfig.suptitle('Bootstrapped Regression Analysis of Operating Budget Vs. Crime\u2423\n\u21aaStatistic Rates', fontsize=16)\nfig.tight_layout(rect=[0, 0, 1, 0.96])\nfig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9), fontsize=12)\nplt.show()\n34"
    },
    {
        "page": 144,
        "text": "Income vs Crime Statistics - David Gri\ufb00in\n[31]: #join crime and income datasets and create dataframe\nquery= f\"\"\"\nWITH CrimeTotals AS (\nSELECT\nAREA_NAME,\n35"
    },
    {
        "page": 145,
        "text": "year,\nASSAULT,\nAUTOTHEFT,\nBIKETHEFT,\nBREAKENTER,\nHOMICIDE,\nROBBERY,\nSHOOTING,\nTHEFTFROMMV,\nTHEFTOVER,\nASSAULT_RATE,\nHOMICIDE_RATE,\nROBBERY_RATE,\nSHOOTING_RATE,\nAUTOTHEFT_RATE,\nBREAKENTER_RATE\nFROM {CRIMETABLE}\nWHERE year='2020'\n),\nIncomeData AS (\nSELECT\nNeighbourhood,\nIncome,\nYEAR,\nPopulation\nFROM {INCOMETABLE}\n)\nSELECT\nI.YEAR,\nI.Neighbourhood,\nI.Income,\nI.Population,\nC.ASSAULT,\nC.HOMICIDE,\nC.ROBBERY,\nC.AUTOTHEFT,\nC.ASSAULT_RATE,\nC.HOMICIDE_RATE,\nC.ROBBERY_RATE,\nC.AUTOTHEFT_RATE,\nC.BREAKENTER,\nC.BREAKENTER_RATE\nFROM IncomeData I\nJoin CrimeTotals C\nOn I.Neighbourhood=C.AREA_NAME\nORDER BY I.Neighbourhood;\n36"
    },
    {
        "page": 146,
        "text": "\"\"\"\ncrime_df=pd.read_sql_query(query, engine)\ncrime_df.describe()\n[31]:\nYEAR\nIncome\nPopulation\nASSAULT\nHOMICIDE\n\\\ncount\n151.0\n151.000000\n151.000000\n151.000000\n151.000000\nmean\n2020.0\n50083.642384\n17526.456954\n113.576159\n0.456954\nstd\n0.0\n18275.444527\n6225.065451\n98.205800\n0.745968\nmin\n2020.0\n30800.000000\n6260.000000\n11.000000\n0.000000\n25%\n2020.0\n37100.000000\n12395.000000\n55.500000\n0.000000\n50%\n2020.0\n45760.000000\n16670.000000\n85.000000\n0.000000\n75%\n2020.0\n55375.000000\n22325.000000\n133.000000\n1.000000\nmax\n2020.0\n141200.000000\n33300.000000\n756.000000\n3.000000\nROBBERY\nAUTOTHEFT\nASSAULT_RATE\nHOMICIDE_RATE\nROBBERY_RATE\n\\\ncount\n151.000000\n151.000000\n151.000000\n151.000000\n151.000000\nmean\n17.443709\n37.099338\n601.820149\n2.230829\n91.718967\nstd\n16.628744\n40.088694\n471.090176\n3.530964\n80.170084\nmin\n1.000000\n4.000000\n110.430679\n0.000000\n11.983224\n25%\n7.000000\n17.000000\n339.540222\n0.000000\n52.815395\n50%\n15.000000\n26.000000\n508.151611\n0.000000\n77.433090\n75%\n21.500000\n44.500000\n689.026428\n4.691375\n108.235043\nmax\n136.000000\n407.000000\n3582.768555\n15.490267\n644.519226\nAUTOTHEFT_RATE\nBREAKENTER\nBREAKENTER_RATE\ncount\n151.000000\n151.000000\n151.000000\nmean\n190.831371\n43.748344\n240.835126\nstd\n141.915053\n36.576170\n182.616767\nmin\n37.606354\n3.000000\n29.228371\n25%\n103.571892\n21.000000\n116.272907\n50%\n160.057831\n34.000000\n196.055359\n75%\n232.629288\n50.500000\n304.359421\nmax\n1139.545288\n204.000000\n1105.997681\n[32]: #create variable that sorts each neighbourhoods income into quantiles\ncrime_df['Income Group']=0\nfor i in range(len(crime_df)):\nif crime_df['Income'][i]<=37100:\ncrime_df.iloc[i,14]=1\nif crime_df['Income'][i]>37100 and crime_df['Income'][i]<=45760:\ncrime_df.iloc[i,14]=2\nif crime_df['Income'][i]>45760 and crime_df['Income'][i]<=55375:\ncrime_df.iloc[i,14]=3\nif crime_df['Income'][i]>55375:\ncrime_df.iloc[i,14]=4\n37"
    },
    {
        "page": 147,
        "text": "[33]: #make scatter plots for assault, robbery, autotheft and break and enter rates\u2423\n\u21aawith regressions for each plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0,0].scatter(crime_df['Income'],crime_df['ASSAULT_RATE'], color='tab:blue')\naxes[0,0].set_xlabel('Income')\naxes[0,0].set_ylabel('Assault Rate')\nslope, intercept,r1,p1,_ = linregress(crime_df['Income'],\u2423\n\u21aacrime_df['ASSAULT_RATE'])\nx_vals=np.array([min(crime_df['Income']), max(crime_df['Income'])])\ny_vals= slope*x_vals + intercept\naxes[0,0].plot(x_vals,y_vals, color='tab:blue', linestyle='-',\u2423\n\u21aalabel='Regression Line')\naxes[0,1].scatter(crime_df['Income'],crime_df['AUTOTHEFT_RATE'], color='tab:\n\u21aablue',)\naxes[0,1].set_xlabel('Income')\naxes[0,1].set_ylabel('Auto Rate')\nslope, intercept,r2,p2,_ = linregress(crime_df['Income'],\u2423\n\u21aacrime_df['AUTOTHEFT_RATE'])\nx_vals=np.array([min(crime_df['Income']), max(crime_df['Income'])])\ny_vals= slope*x_vals + intercept\naxes[0,1].plot(x_vals,y_vals, color='tab:blue', linestyle='-',\u2423\n\u21aalabel='Regression Line')\naxes[1,0].scatter(crime_df['Income'],crime_df['ROBBERY_RATE'], color='tab:\n\u21aablue',)\naxes[1,0].set_xlabel('Income')\naxes[1,0].set_ylabel('Robbery Rate')\nslope, intercept,r3,p3,_ = linregress(crime_df['Income'],\u2423\n\u21aacrime_df['ROBBERY_RATE'])\nx_vals=np.array([min(crime_df['Income']), max(crime_df['Income'])])\ny_vals= slope*x_vals + intercept\naxes[1,0].plot(x_vals,y_vals, color='tab:blue', linestyle='-',\u2423\n\u21aalabel='Regression Line')\naxes[1,1].scatter(crime_df['Income'],crime_df['BREAKENTER_RATE'], color='tab:\n\u21aablue',)\naxes[1,1].set_xlabel('Income')\naxes[1,1].set_ylabel('Break and Enter Rate')\nslope, intercept,r4,p4,_ = linregress(crime_df['Income'],\u2423\n\u21aacrime_df['BREAKENTER_RATE'])\nx_vals=np.array([min(crime_df['Income']), max(crime_df['Income'])])\ny_vals= slope*x_vals + intercept\naxes[1,1].plot(x_vals,y_vals, color='tab:blue', linestyle='-',\u2423\n\u21aalabel='Regression Line')\\\n38"
    },
    {
        "page": 148,
        "text": "plt.show()\nprint(f\"Assault R-squared:{r1**2}, coefficient P-value:{p1}\")\nprint(f\"Autotheft R-squared:{r2**2}, coefficient P-value:{p2}\")\nprint(f\"Robbery R-squared:{r3**2}, coefficient P-value:{p3}\")\nprint(f\"Break and Enter R-squared:{r4**2}, coefficient P-value:{p4}\")\nAssault R-squared:0.017645701711067212, coefficient P-value:0.1039539860617245\nAutotheft R-squared:0.021867904397415174, coefficient\nP-value:0.06998142980679437\nRobbery R-squared:0.0039613181600412035, coefficient P-value:0.4426404066165349\nBreak and Enter R-squared:0.06665997361769427, coefficient\nP-value:0.001370828658722553\n[34]: #plot crime rates against income quartiles\ncrime_dfg=crime_df.drop(columns=['YEAR','Neighbourhood'])\ngroup1=crime_dfg.groupby('Income Group',as_index=False).mean()\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes[0,0].bar(group1['Income Group'].unique()-0.\n\u21aa15,group1['ASSAULT_RATE'],width=0.1,label='Assault Rate')\n39"
    },
    {
        "page": 149,
        "text": "axes[0,0].bar(group1['Income Group'].unique()-0.\n\u21aa05,group1['AUTOTHEFT_RATE'],width=0.1,label='Autotheft Rate')\naxes[0,0].bar(group1['Income Group'].unique()+0.\n\u21aa05,group1['ROBBERY_RATE'],width=0.1,label='Robbery Rate')\naxes[0,0].bar(group1['Income Group'].unique()+0.\n\u21aa15,group1['BREAKENTER_RATE'],width=0.1,label='Break and Enter Rate')\naxes[0,0].set_xticks(group1['Income Group'].unique(),('<25% Income','25%-50%\u2423\n\u21aaIncome','50%-75% Income','75% Income'))\naxes[0,0].set_xlabel('Income Groups')\naxes[0,0].set_ylabel('Crime Rate Average')\nfig.delaxes(axes[0, 1])\nfig.delaxes(axes[1, 0])\nfig.delaxes(axes[1, 1])\nfig.legend(bbox_to_anchor=(0.63,0.84))\nplt.show()\n[35]: #plot monicide rates against income for each neighbourhood\nincome_sort = crime_df.sort_values(by='Income', ascending=False)\nfig, ax1 = plt.subplots(figsize=(14, 8))\nx = np.arange(len(income_sort['Neighbourhood']))\nwidth = 0.4\nremove=[]\nax1.bar(x - width/2, income_sort['Income'], width, color='tab:blue',\u2423\n\u21aalabel='Income')\nax1.set_xlabel('Neighbourhood', fontsize=14)\nax1.set_ylabel('Income', color='tab:blue', fontsize=14)\n40"
    },
    {
        "page": 150,
        "text": "ax1.tick_params(axis='y', labelcolor='tab:blue')\nax1.set_xticks(x)\nax1.set_xticklabels(remove)\nax2 = ax1.twinx()\nax2.bar(x + width/2, income_sort['HOMICIDE_RATE'], width, color='tab:red',\u2423\n\u21aalabel='Homicide Rate')\nax2.set_ylabel('Homicide rate', color='tab:blue', fontsize=14)\nax2.tick_params(axis='y', labelcolor='tab:blue')\nfig.suptitle('Income and Homicide rate by Neighbourhood', fontsize=16)\nfig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05), ncol=2, fontsize=12)\nplt.tight_layout()\nEducation Level vs Crime Statistics - Venkateshwaran Balu Soundararajan\nThis Anal-\nysis examines the relationship between the highest certificate, diploma, or degree per household\nand the crime rate per neighborhood in Toronto. It aims to highlight the impact of education on\ncrime rates across different neighborhoods in Toronto.\nTo begin our analysis we extracted the crime statistics from the database. In this step, we execute\nan SQL query to join two tables: one containing crime statistics (CRIMETABLE) and the other\ncontaining income and education data (INCOMETABLE). The objective is to create a comprehen-\nsive dataset that includes information on crime rates, education levels, and other relevant variables\nfor each neighborhood. This resulting dataset will be used for further analysis, including calculat-\ning correlation matrices, performing regression analysis, and creating visualizations to understand\n41"
    },
    {
        "page": 151,
        "text": "the relationship between education and crime in Toronto neighborhoods.\n[36]: degreeandcrimestats=pd.read_sql_query(f\"\"\"SELECT\nccr.area_name,\nccr.geometry,\nine.Degree_Rate,\nPopulation,\nASSAULT_RATE,\nAUTOTHEFT_RATE,\nBIKETHEFT_RATE,\nBREAKENTER_RATE,\nHOMICIDE_RATE,\nROBBERY_RATE,\nSHOOTING_RATE,\nTHEFTFROMMV_RATE,\nTHEFTOVER_RATE\nFROM\n{CRIMETABLE} ccr\nINNER JOIN\n{INCOMETABLE} ine ON ccr.AREA_NAME = ine.Neighbourhood\nAND ccr.year = ine.year;\"\"\",engine)\ndegreeandcrimestats.head(5)\n[36]:\narea_name\n\\\n0\nSouth Eglinton-Davisville\n1\nNorth Toronto\n2\nDovercourt Village\n3\nJunction-Wallace Emerson\n4\nYonge-Bay Corridor\ngeometry\nDegree_Rate\nPopulation\n\\\n0\nMULTIPOLYGON (((-79.3863542900264 43.697839855\u2026\n0.903673\n22735\n1\nMULTIPOLYGON (((-79.3974398976879 43.706938508\u2026\n0.917218\n15885\n2\nMULTIPOLYGON (((-79.4341164165158 43.660153857\u2026\n0.889742\n12380\n3\nMULTIPOLYGON (((-79.4387032547807 43.667669388\u2026\n0.867343\n23180\n4\nMULTIPOLYGON (((-79.3840431592607 43.644973675\u2026\n0.923290\n12645\nASSAULT_RATE\nAUTOTHEFT_RATE\nBIKETHEFT_RATE\nBREAKENTER_RATE\n\\\n0\n396.844605\n72.593521\n454.919434\n338.769775\n1\n525.969727\n80.356491\n365.256775\n642.851929\n2\n542.731018\n130.255447\n202.619583\n340.111450\n3\n661.992248\n172.353012\n211.524155\n430.882538\n4\n2842.933594\n103.919243\n1284.144897\n1105.997681\nHOMICIDE_RATE\nROBBERY_RATE\nSHOOTING_RATE\nTHEFTFROMMV_RATE\n\\\n0\n4.839568\n77.433090\n4.839568\n208.101440\n1\n7.305136\n21.915407\n0.000000\n175.323257\n42"
    },
    {
        "page": 152,
        "text": "2\n7.236413\n108.546204\n0.000000\n484.839722\n3\n0.000000\n86.176506\n19.585569\n544.478821\n4\n0.000000\n408.254150\n14.845606\n1002.078369\nTHEFTOVER_RATE\n0\n24.197842\n1\n43.830814\n2\n21.709240\n3\n35.254025\n4\n259.798096\nCorrelation Analysis\nWe then created a correlation matrix to examine the strength and direction of the relationships\nbetween the degree rate and various crime rates. The correlation matrix provides valuable insights\ninto how education impacts different types of crime.\n[37]: import seaborn as sns\nimport matplotlib.pyplot as plt\ncorrelation_matrix = degreeandcrimestats[['Degree_Rate', 'ASSAULT_RATE',\u2423\n\u21aa'AUTOTHEFT_RATE', 'BIKETHEFT_RATE', 'BREAKENTER_RATE',\n'HOMICIDE_RATE', 'ROBBERY_RATE',\u2423\n\u21aa'SHOOTING_RATE', 'THEFTFROMMV_RATE',\n'THEFTOVER_RATE']].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\u2423\n\u21aalinewidths=0.5)\nplt.title('Correlation Between Education and TheftRates')\nplt.show()\n43"
    },
    {
        "page": 153,
        "text": "From the correlation matrix we infer that SHOOTING_RATE has a weaker negative correlation (-\n0.26) with education, indicating that higher education levels may be associated with lower shooting\nincidents. Conversely, metrics like BIKETHEFT_RATE (0.55) and BREAKENTER_RATE (0.54)\ndemonstrate moderate positive correlations, suggesting these crimes might be more prevalent in\nareas with higher education levels, possibly due to economic factors or population density\nRegression Analysis\nWe then performed regression analyses to verify the significance of the relationships observed. By\nfitting regression models for each crime rate variable against the degree rate, we determined the\nstatistical significance and the extent to which education levels explain the variability in crime\nrates.\n[38]: import statsmodels.api as sm\nvariables = ['ASSAULT_RATE', 'AUTOTHEFT_RATE', 'BIKETHEFT_RATE',\u2423\n\u21aa'BREAKENTER_RATE', 'HOMICIDE_RATE',\n44"
    },
    {
        "page": 154,
        "text": "'ROBBERY_RATE', 'SHOOTING_RATE', 'THEFTFROMMV_RATE',\u2423\n\u21aa'THEFTOVER_RATE']\nX = degreeandcrimestats['Degree_Rate']\nX = sm.add_constant(X)\nfor var in variables:\ny = degreeandcrimestats[var]\nmodel_assault = sm.OLS(y, X).fit()\np_value=model_assault.pvalues[1]\nadj_r_squared = model_assault.rsquared_adj\nprint(f\"Regression Results for {var}:\")\nprint(f\"\np-value for Degree_Rate: {p_value:.4f}\")\nprint(f\"\nAdjusted R-squared: {adj_r_squared:.4f}\")\nprint('-' * 40)\nRegression Results for ASSAULT_RATE:\np-value for Degree_Rate: 0.0000\nAdjusted R-squared: 0.1190\n----------------------------------------\nRegression Results for AUTOTHEFT_RATE:\np-value for Degree_Rate: 0.0839\nAdjusted R-squared: 0.0133\n----------------------------------------\nRegression Results for BIKETHEFT_RATE:\np-value for Degree_Rate: 0.0000\nAdjusted R-squared: 0.3260\n----------------------------------------\nRegression Results for BREAKENTER_RATE:\np-value for Degree_Rate: 0.0000\nAdjusted R-squared: 0.2822\n----------------------------------------\nRegression Results for HOMICIDE_RATE:\np-value for Degree_Rate: 0.7003\nAdjusted R-squared: -0.0057\n----------------------------------------\nRegression Results for ROBBERY_RATE:\np-value for Degree_Rate: 0.0001\nAdjusted R-squared: 0.0917\n----------------------------------------\nRegression Results for SHOOTING_RATE:\np-value for Degree_Rate: 0.0014\nAdjusted R-squared: 0.0601\n----------------------------------------\nRegression Results for THEFTFROMMV_RATE:\np-value for Degree_Rate: 0.0132\nAdjusted R-squared: 0.0341\n----------------------------------------\nRegression Results for THEFTOVER_RATE:\n45"
    },
    {
        "page": 155,
        "text": "p-value for Degree_Rate: 0.0000\nAdjusted R-squared: 0.1319\n----------------------------------------\nFollowing the correlation and regression analyses, we visualized the relationships between degree\nrate and various crime rates. This step involved creating a series of regression plots to provide a\nclear and intuitive representation of how education levels correlate with different types of crime\nacross neighborhoods in Toronto.\nWe use the seaborn and matplotlib libraries to create a 3x3 grid of subplots, each displaying a\nregression plot for a specific crime rate variable against the degree rate. The regression plots help\nus visually assess the impact of education levels on different types of crime.\n[39]: fig, axes = plt.subplots(3, 3, figsize=(12, 12))\ncolors = ['tab:purple', 'tab:red', 'tab:green', 'tab:orange', 'tab:blue', 'tab:\n\u21aapink', 'tab:brown', 'tab:gray', 'tab:olive']\nfor i, var in enumerate(variables):\nrow, col = divmod(i, 3)\nsns.regplot(y=var, x='Degree_Rate', data=degreeandcrimestats, ax=axes[row,\u2423\n\u21aacol],color=colors[i])\naxes[row, col].set_title(f'{var} vs Degree_Rate')\nplt.tight_layout()\nplt.show()\n46"
    },
    {
        "page": 156,
        "text": "To further enhance our understanding, we create spatial visualizations that map the distribution\nof crime rates and degree rates across different neighborhoods.\nThese visualizations provide a\ngeographical perspective, highlighting areas with high concentrations of crime and education levels\n[40]: geopolygon = pd.read_sql_query(f'''SELECT\nccr.AREA_NAME,\nccr.geometry,\nine.Degree,\nine.Degree_Rate,\n(SUM(ASSAULT_RATE) + SUM(AUTOTHEFT_RATE) +\u2423\n\u21aaSUM(BIKETHEFT_RATE) + SUM(BREAKENTER_RATE) + SUM(HOMICIDE_RATE) +\nSUM(ROBBERY_RATE) + SUM(SHOOTING_RATE) +\u2423\n\u21aaSUM(THEFTFROMMV_RATE) + SUM(THEFTOVER_RATE)) / 9 AS AVG_CRIME_RATE\n47"
    },
    {
        "page": 157,
        "text": "FROM\n{CRIMETABLE} ccr\nINNER JOIN\n{INCOMETABLE} ine ON ccr.AREA_NAME = ine.\n\u21aaNeighbourhood\nAND ccr.year = ine.year\nGROUP BY ccr.AREA_NAME,ccr.geometry,ine.\n\u21aaDegree,ine.Degree_Rate\nORDER BY degree_rate DESC;''', engine)\ngeopolygon.dtypes\ngeopolygon['geometry'] = geopolygon['geometry'].apply(wkt.loads)\ngeopolygon_gdf = gpd.GeoDataFrame(geopolygon, geometry='geometry')\ngeopolygon_gdf = geopolygon_gdf.set_crs(epsg=4326)\ngeopolygon_gdf.head()\n[40]:\nAREA_NAME\n\\\n0\nChurch-Wellesley\n1\nWellington Place\n2\nBay-Cloverhill\n3\nDowntown Yonge East\n4\nFort York-Liberty Village\ngeometry\nDegree\nDegree_Rate\n\\\n0\nMULTIPOLYGON (((-79.38168 43.6614, -79.38184 4\u2026\n21420\n0.959677\n1\nMULTIPOLYGON (((-79.4007 43.64094, -79.40143 4\u2026\n24440\n0.955808\n2\nMULTIPOLYGON (((-79.38743 43.66052, -79.3905 4\u2026\n15930\n0.955609\n3\nMULTIPOLYGON (((-79.37656 43.66202, -79.3759 4\u2026\n16640\n0.940113\n4\nMULTIPOLYGON (((-79.40793 43.6401, -79.4078 43\u2026\n18710\n0.939493\nAVG_CRIME_RATE\n0\n345.769547\n1\n379.555556\n2\n195.020355\n3\n756.388558\n4\n153.465399\n[41]: # Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))\n# Plot the AVG_CRIME_RATE\ngeopolygon_gdf.plot(column=\"AVG_CRIME_RATE\", cmap=\"Blues\", legend=True,\u2423\n\u21aaax=axes[0])\naxes[0].set_title(\"Crime Rates by Area\", fontsize=16)\naxes[0].set_xlabel(\"Longitude\")\naxes[0].set_ylabel(\"Latitude\")\n# Plot the Degree_Rate\n48"
    },
    {
        "page": 158,
        "text": "geopolygon_gdf.plot(column=\"Degree_Rate\", cmap=\"Blues\", legend=True, ax=axes[1])\naxes[1].set_title(\"Degree Rates by Area\", fontsize=16)\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_ylabel(\"Latitude\")\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()\nShelter Occupancy vs Crime Statistics - Jackson Meier\n[42]: #creating combined table with shelter and crime data joined on neighbourhood\nsheltercrime = (\"\"\"WITH CrimeTotals AS (\nSELECT\nAREA_NAME,\nyear,\nASSAULT_RATE,\nHOMICIDE_RATE,\nROBBERY_RATE,\nSHOOTING_RATE,\nAUTOTHEFT_RATE,\nBREAKENTER_RATE\nFROM data604_converted_crime\nWHERE year='2020'\n),\nShelterData AS (\nSELECT\nOccupancy_Date,\nShelter_Name,\n49"
    },
    {
        "page": 159,
        "text": "Sector,\nOccupancy,\nCapacity,\nAREA_NAME\nFROM data604_daily_shelter_occupancy\n)\nSELECT\nS.Occupancy_Date,\nS.Shelter_Name,\nS.Sector,\nS.Occupancy,\nS.Capacity,\nS.AREA_NAME,\nC.AREA_NAME,\nC.ASSAULT_RATE,\nC.HOMICIDE_RATE,\nC.ROBBERY_RATE,\nC.AUTOTHEFT_RATE,\nC.BREAKENTER_RATE\nFROM ShelterData S\nJoin CrimeTotals C\nOn S.AREA_NAME=C.AREA_NAME\nORDER BY S.AREA_NAME;\"\"\")\n[43]: shelter_crime_df = SQL(sheltercrime)\n[44]: #drop duplicate AREA_NAME column\nshelter_crime_df = shelter_crime_df.loc[:,~shelter_crime_df.columns.\n\u21aaduplicated()]\n[45]: shelter_crime_df.head(3)\n[45]:\nOccupancy_Date\nShelter_Name\nSector\nOccupancy\n\\\n0\n2017-01-01\nChristie Refugee Welcome Centre\nFamilies\n66\n1\n2017-01-01\nToronto Community Hostel\nFamilies\n26\n2\n2017-01-01\nYMCA Sprott House\nYouth\n24\nCapacity AREA_NAME\nASSAULT_RATE\nHOMICIDE_RATE\nROBBERY_RATE\n\\\n0\n70\nAnnex\n795.62561\n0.0\n122.876541\n1\n24\nAnnex\n795.62561\n0.0\n122.876541\n2\n25\nAnnex\n795.62561\n0.0\n122.876541\nAUTOTHEFT_RATE\nBREAKENTER_RATE\n0\n119.804626\n626.670349\n1\n119.804626\n626.670349\n2\n119.804626\n626.670349\n50"
    },
    {
        "page": 160,
        "text": "[46]: #creating occupancy rate\nshelter_crime_df['Occupancy_Rate'] = shelter_crime_df['Occupancy']/\n\u21aashelter_crime_df['Capacity']\n#clean data - drop 'inf' rows\nshelter_crime_df['Occupancy_Rate'] = shelter_crime_df['Occupancy_Rate'].\n\u21aareplace([np.inf,-np.inf],np.nan)\nshelter_crime_df = shelter_crime_df.dropna(subset=['Occupancy_Rate'])\n[47]: occupancy_sorted = shelter_crime_df.groupby('AREA_NAME',as_index =\u2423\n\u21aaFalse)[['Occupancy_Rate','ASSAULT_RATE','HOMICIDE_RATE','ROBBERY_RATE','AUTOTHEFT_RATE','BR\n\u21aamean().sort_values(by='Occupancy_Rate',ascending = False)\noccupancy_sorted[(occupancy_sorted != 0).all(axis=1)].shape\n[47]: (16, 7)\n[48]: import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import linregress\n# List of crime rates to plot\ncrime_columns = ['ASSAULT_RATE', 'HOMICIDE_RATE', 'ROBBERY_RATE',\n'AUTOTHEFT_RATE', 'BREAKENTER_RATE']\n# Create subplots\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n# Adjust rows/\n\u21aacolumns as needed\naxes = axes.flatten()\n# Plot each crime rate against Occupancy_Rate with correlation line\nfor i, crime in enumerate(crime_columns):\n# Scatter plot\naxes[i].scatter(occupancy_sorted['Occupancy_Rate'],\u2423\n\u21aaoccupancy_sorted[crime], alpha=0.7)\n# Calculate the linear regression line\nslope, intercept, r_value, p_value, std_err =\u2423\n\u21aalinregress(occupancy_sorted['Occupancy_Rate'], occupancy_sorted[crime])\n# Generate the line using the slope and intercept\nline = slope * occupancy_sorted['Occupancy_Rate'] + intercept\naxes[i].plot(occupancy_sorted['Occupancy_Rate'], line, label=f'{crime} fit\u2423\n\u21aaline', linestyle='--')\n# Titles and labels\naxes[i].set_title(f'{crime} vs Occupancy Rate')\naxes[i].set_xlabel('Occupancy Rate')\naxes[i].set_ylabel(crime)\n51"
    },
    {
        "page": 161,
        "text": "# Remove empty subplot (if any)\nfor j in range(len(crime_columns), len(axes)):\nfig.delaxes(axes[j])\nplt.tight_layout()\nplt.show()\n[49]: #calculate correlation and p-vals\nfrom scipy.stats import pearsonr\ncorrelations = {}\np_values = {}\nfor crime in crime_columns:\ncorr, p_value = pearsonr(occupancy_sorted['Occupancy_Rate'],\u2423\n\u21aaoccupancy_sorted[crime])\ncorrelations[crime] = corr\np_values[crime] = p_value\n# Print the correlations and p-values\nfor crime in crime_columns:\nprint(f\"Correlation between Occupancy Rate and {crime}:\u2423\n\u21aa{correlations[crime]:.2f}\")\n52"
    },
    {
        "page": 162,
        "text": "print(f\"P-value for {crime}: {p_values[crime]:.4f}\")\nif p_values[crime] < 0.05:\nprint(\"The correlation is statistically significant.\")\nelse:\nprint(\"The correlation is not statistically significant.\")\nprint()\nCorrelation between Occupancy Rate and ASSAULT_RATE: -0.11\nP-value for ASSAULT_RATE: 0.5110\nThe correlation is not statistically significant.\nCorrelation between Occupancy Rate and HOMICIDE_RATE: -0.02\nP-value for HOMICIDE_RATE: 0.9287\nThe correlation is not statistically significant.\nCorrelation between Occupancy Rate and ROBBERY_RATE: -0.08\nP-value for ROBBERY_RATE: 0.6475\nThe correlation is not statistically significant.\nCorrelation between Occupancy Rate and AUTOTHEFT_RATE: 0.02\nP-value for AUTOTHEFT_RATE: 0.9241\nThe correlation is not statistically significant.\nCorrelation between Occupancy Rate and BREAKENTER_RATE: -0.40\nP-value for BREAKENTER_RATE: 0.0173\nThe correlation is statistically significant.\nBike Theft vs Bike Crimes - Steen Rasmussen\n[50]: # query for negihbourhoods with the most bike rack capacity\ncapacity_query = pd.read_sql_query(f\"\"\"\nSELECT\nAREA_NAME,\nSUM(CAPACITY) AS total_bike_rack_capacity,\nSUM(BIKETHEFT) AS total_bike_thefts,\nSUM(POPULATION_2023) AS total_population\nFROM\n{BIKECRIMETABLE}\nWHERE\nYear = 2023\nGROUP BY\nAREA_NAME\n\"\"\", engine)\ndisplay(capacity_query)\ncapacity_query.to_csv('capacity_query.csv', index=False)\n53"
    },
    {
        "page": 163,
        "text": "AREA_NAME\ntotal_bike_rack_capacity\n\\\n0\nAnnex\n109.0\n1\nAvondale\n8.0\n2\nBendale-Glen Andrew\n8.0\n3\nBirchcliffe-Cliffside\n8.0\n4\nCabbagetown-South St.James Town\n16.0\n5\nChurch-Wellesley\n8.0\n6\nDanforth\n50.0\n7\nDovercourt Village\n42.0\n8\nDowntown Yonge East\n61.0\n9\nDufferin Grove\n8.0\n10\nHarbourfront-CityPlace\n48.0\n11\nHigh Park North\n20.0\n12\nHumber Bay Shores\n8.0\n13\nJunction-Wallace Emerson\n44.0\n14\nKensington-Chinatown\n116.0\n15\nLawrence Park North\n8.0\n16\nLittle Portugal\n64.0\n17\nMimico-Queensway\n16.0\n18\nMoss Park\n70.0\n19\nMount Olive-Silverstone-Jamestown\n32.0\n20\nNew Toronto\n8.0\n21\nO'Connor-Parkview\n8.0\n22\nPalmerston-Little Italy\n118.0\n23\nRoncesvalles\n8.0\n24\nRosedale-Moore Park\n8.0\n25\nSouth Riverdale\n40.0\n26\nSt Lawrence-East Bayfront-The Islands\n124.0\n27\nSteeles\n16.0\n28\nTam O'Shanter-Sullivan\n8.0\n29\nThe Beaches\n8.0\n30\nThorncliffe Park\n16.0\n31\nTrinity-Bellwoods\n46.0\n32\nUniversity\n30.0\n33\nWellington Place\n114.0\n34\nWest Queen West\n56.0\n35\nWillowdale West\n64.0\n36\nYonge-Bay Corridor\n224.0\n37\nYonge-Eglinton\n56.0\n38\nYonge-St.Clair\n18.0\ntotal_bike_thefts\ntotal_population\n0\n891.0\n379522.0\n1\n9.0\n16149.0\n2\n10.0\n20773.0\n3\n6.0\n23417.0\n4\n62.0\n23936.0\n5\n102.0\n22938.0\n54"
    },
    {
        "page": 164,
        "text": "6\n39.0\n30519.0\n7\n228.0\n83022.0\n8\n1435.0\n126161.0\n9\n58.0\n12709.0\n10\n528.0\n188862.0\n11\n28.0\n24474.0\n12\n16.0\n20822.0\n13\n220.0\n104960.0\n14\n1096.0\n169832.0\n15\n6.0\n15581.0\n16\n85.0\n86995.0\n17\n10.0\n36626.0\n18\n495.0\n111035.0\n19\n11.0\n35386.0\n20\n9.0\n12205.0\n21\n8.0\n19901.0\n22\n297.0\n132291.0\n23\n21.0\n15939.0\n24\n45.0\n23053.0\n25\n340.0\n120540.0\n26\n424.0\n124012.0\n27\n6.0\n52790.0\n28\n16.0\n29770.0\n29\n25.0\n23097.0\n30\n20.0\n45016.0\n31\n230.0\n89070.0\n32\n159.0\n22692.0\n33\n880.0\n317570.0\n34\n168.0\n60544.0\n35\n6.0\n19546.0\n36\n2964.0\n191503.0\n37\n80.0\n68715.0\n38\n28.0\n27906.0\n[51]: # most capacity by neighbourhood\ncapacity_df = pd.read_csv('capacity_query.csv')\n# sort by most to least\ndata_sorted = capacity_df.sort_values(by='total_bike_rack_capacity',\u2423\n\u21aaascending=False)\n# bar plot\nplt.figure(figsize=(15, 8))\nplt.bar(data_sorted['AREA_NAME'], data_sorted['total_bike_rack_capacity'],\u2423\n\u21aacolor='blue')\nplt.title('Total Capacity by Area (2023)', fontsize=16)\n55"
    },
    {
        "page": 165,
        "text": "plt.xlabel('Area Name', fontsize=12)\nplt.ylabel('Total Bike Capacity', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()\n[52]: # graph of bike capacity by bike thefts and correlation term\nBike_capacity_plot_query = pd.read_csv('capacity_query.csv')\nx = Bike_capacity_plot_query['total_bike_rack_capacity']\ny = Bike_capacity_plot_query['total_bike_thefts']\nslope = np.cov(x, y, ddof=0)[0, 1] / np.var(x, ddof=0)\nintercept = np.mean(y) - slope * np.mean(x)\ncorrelation_matrix = np.corrcoef(x, y)\nr_value = correlation_matrix[0, 1]\nregression_line = slope * x + intercept\n56"
    },
    {
        "page": 166,
        "text": "plt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', s=50, alpha=0.6, label='Data Points')\n#\u2423\n\u21aascatter points\nplt.plot(x, regression_line, color='red', label='Regression Line')\n#\u2423\n\u21aaregression line\n# add titles, labels, and the correlation coefficient\nplt.title(f'Regression: Bike Rack Capacity vs. Bike Thefts \\nCorrelation\u2423\n\u21aaCoefficient: {r_value:.2f}', fontsize=16)\nplt.xlabel('Total Bike Rack Capacity', fontsize=12)\nplt.ylabel('Total Bike Thefts', fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True)\n# show the plot\nplt.show()\nBike Theft vs Total Bike Racks\n[53]: Bike_Theft_query = pd.read_sql_query(f\"\"\"\nSELECT\nAREA_NAME,\nSUM(BIKETHEFT) AS total_bike_thefts,\n57"
    },
    {
        "page": 167,
        "text": "COUNT(*) AS total_racks,\nAVG(BIKETHEFT_RATE) AS bike_theft_rate,\nSUM(POPULATION_2023) AS total_population\nFROM\n{BIKECRIMETABLE}\nWHERE\nYear = 2023\nGROUP BY\nAREA_NAME\n\"\"\", engine)\ndisplay(Bike_Theft_query)\nBike_Theft_query.to_csv('bike_theft_area.csv', index=False)\nAREA_NAME\ntotal_bike_thefts\ntotal_racks\n\\\n0\nAnnex\n891.0\n11\n1\nAvondale\n9.0\n1\n2\nBendale-Glen Andrew\n10.0\n1\n3\nBirchcliffe-Cliffside\n6.0\n1\n4\nCabbagetown-South St.James Town\n62.0\n2\n5\nChurch-Wellesley\n102.0\n1\n6\nDanforth\n39.0\n3\n7\nDovercourt Village\n228.0\n6\n8\nDowntown Yonge East\n1435.0\n7\n9\nDufferin Grove\n58.0\n1\n10\nHarbourfront-CityPlace\n528.0\n6\n11\nHigh Park North\n28.0\n1\n12\nHumber Bay Shores\n16.0\n1\n13\nJunction-Wallace Emerson\n220.0\n4\n14\nKensington-Chinatown\n1096.0\n8\n15\nLawrence Park North\n6.0\n1\n16\nLittle Portugal\n85.0\n5\n17\nMimico-Queensway\n10.0\n2\n18\nMoss Park\n495.0\n5\n19\nMount Olive-Silverstone-Jamestown\n11.0\n1\n20\nNew Toronto\n9.0\n1\n21\nO'Connor-Parkview\n8.0\n1\n22\nPalmerston-Little Italy\n297.0\n9\n23\nRoncesvalles\n21.0\n1\n24\nRosedale-Moore Park\n45.0\n1\n25\nSouth Riverdale\n340.0\n4\n26\nSt Lawrence-East Bayfront-The Islands\n424.0\n4\n27\nSteeles\n6.0\n2\n28\nTam O'Shanter-Sullivan\n16.0\n1\n29\nThe Beaches\n25.0\n1\n30\nThorncliffe Park\n20.0\n2\n31\nTrinity-Bellwoods\n230.0\n5\n58"
    },
    {
        "page": 168,
        "text": "32\nUniversity\n159.0\n3\n33\nWellington Place\n880.0\n11\n34\nWest Queen West\n168.0\n4\n35\nWillowdale West\n6.0\n1\n36\nYonge-Bay Corridor\n2964.0\n13\n37\nYonge-Eglinton\n80.0\n5\n38\nYonge-St.Clair\n28.0\n2\nbike_theft_rate\ntotal_population\n0\n234.768997\n379522.0\n1\n55.731007\n16149.0\n2\n48.139412\n20773.0\n3\n25.622412\n23417.0\n4\n259.024078\n23936.0\n5\n444.676941\n22938.0\n6\n127.789246\n30519.0\n7\n274.626007\n83022.0\n8\n1137.435547\n126161.0\n9\n456.369507\n12709.0\n10\n279.569214\n188862.0\n11\n114.407127\n24474.0\n12\n76.841804\n20822.0\n13\n209.603653\n104960.0\n14\n645.343628\n169832.0\n15\n38.508438\n15581.0\n16\n97.706764\n86995.0\n17\n27.303009\n36626.0\n18\n445.805389\n111035.0\n19\n31.085741\n35386.0\n20\n73.740273\n12205.0\n21\n40.198986\n19901.0\n22\n224.505066\n132291.0\n23\n131.752304\n15939.0\n24\n195.202362\n23053.0\n25\n282.064056\n120540.0\n26\n341.902405\n124012.0\n27\n11.365789\n52790.0\n28\n53.745380\n29770.0\n29\n108.239166\n23097.0\n30\n44.428646\n45016.0\n31\n258.223877\n89070.0\n32\n700.687439\n22692.0\n33\n277.104248\n317570.0\n34\n277.484131\n60544.0\n35\n30.696817\n19546.0\n36\n1547.756470\n191503.0\n37\n116.422905\n68715.0\n38\n100.336845\n27906.0\n59"
    },
    {
        "page": 169,
        "text": "[54]: # bar plot for neighbourhoods with the most bike thefts\n# read file\nBike_crime_df = pd.read_csv('bike_theft_area.csv')\n# sort by most to least\ndata_sorted = Bike_crime_df.sort_values(by='total_bike_thefts', ascending=False)\n# bar plot\nplt.figure(figsize=(15, 8))\nplt.bar(data_sorted['AREA_NAME'], data_sorted['total_bike_thefts'],\u2423\n\u21aacolor='blue')\nplt.title('Total Bike Thefts by Area (2023)', fontsize=16)\nplt.xlabel('Area Name', fontsize=12)\nplt.ylabel('Total Bike Thefts', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()\n60"
    },
    {
        "page": 170,
        "text": "[55]: # comparing total bike racks to total bike thefts\ndf = pd.read_csv('bike_theft_area.csv')\ndf = df.sort_values('total_racks', ascending=False)\nfig, ax1 = plt.subplots(figsize=(14, 8))\nx = np.arange(len(df['AREA_NAME']))\nwidth = 0.4\nax1.bar(x - width/2, df['total_racks'], width, color='b', label='Total Bike\u2423\n\u21aaRacks')\nax1.set_xlabel('Neighborhood', fontsize=14)\nax1.set_ylabel('Total Bike Racks', color='b', fontsize=14)\nax1.tick_params(axis='y', labelcolor='b')\nax1.set_xticks(x)\nax1.set_xticklabels(df['AREA_NAME'], rotation=45, ha='right', fontsize=12)\nax2 = ax1.twinx()\nax2.bar(x + width/2, df['total_bike_thefts'], width, color='r', label='Total\u2423\n\u21aaBike Thefts')\nax2.set_ylabel('Total Bike Thefts', color='r', fontsize=14)\nax2.tick_params(axis='y', labelcolor='r')\nfig.suptitle('Comparison of Bike Racks and Bike Thefts by Neighborhood',\u2423\n\u21aafontsize=16)\nfig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05), ncol=2, fontsize=12)\nplt.tight_layout()\nplt.show()\n61"
    },
    {
        "page": 171,
        "text": "[56]: # graph of number of bike racks by bike thefts and correlation term\nBike_Theft_query = pd.read_csv('bike_theft_area.csv')\nx = Bike_Theft_query['total_racks']\ny = Bike_Theft_query['total_bike_thefts']\nslope = np.cov(x, y, ddof=0)[0, 1] / np.var(x, ddof=0)\nintercept = np.mean(y) - slope * np.mean(x)\ncorrelation_matrix = np.corrcoef(x, y)\nr_value = correlation_matrix[0, 1]\nregression_line = slope * x + intercept\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', s=50, label='Data Points')\n# scatter points\nplt.plot(x, regression_line, color='red', label='Regression Line')\n#\u2423\n\u21aaregression line\nplt.title(f'Regression: Bike Racks vs. Bike Thefts \\nCorrelation Coefficient:\u2423\n\u21aa{r_value:.2f}', fontsize=16)\nplt.xlabel('Number of Bike Racks', fontsize=12)\nplt.ylabel('Number of Bike Thefts', fontsize=12)\nplt.legend(fontsize=12)\n62"
    },
    {
        "page": 172,
        "text": "plt.grid(True)\nplt.show()\nBike rack and neighbourhood map\n[57]: # load GeoJSON Files\nbikerack_gdf = gpd.read_file(\"Bicycle Parking Racks Data.geojson\")\ncrime_gdf = gpd.read_file(\"neighbourhood-crime-rates.geojson\")\nprint(\"Geometry types in bike rack data:\")\nprint(bikerack_gdf.geometry.geom_type.value_counts())\nbikerack_gdf[\"geometry\"] = bikerack_gdf[\"geometry\"].apply(lambda geom: geom.\n\u21aageoms[0] if geom.geom_type == \"MultiPoint\" else None)\nbikerack_gdf = bikerack_gdf[bikerack_gdf.geometry.notna()]\nif bikerack_gdf.empty:\nprint(\"No valid Point geometries found. Using fallback.\")\nmap_center = [43.7, -79.4]\nelse:\nmap_center = [\n63"
    },
    {
        "page": 173,
        "text": "bikerack_gdf.geometry.y.mean(),\nbikerack_gdf.geometry.x.mean()\n]\nprint(\"Valid Point geometries found:\", len(bikerack_gdf))\n# make sure CRS Matches\nif bikerack_gdf.crs != crime_gdf.crs:\ncrime_gdf = crime_gdf.to_crs(bikerack_gdf.crs)\n# create Folium Map\nm = folium.Map(location=map_center, zoom_start=12)\nfor idx, row in bikerack_gdf.iterrows():\nfolium.Marker(\nlocation=[row.geometry.y, row.geometry.x],\npopup=f\"<b>Address:</b> {row.get('ADDRESS_FULL', 'N/A')}<br><b>Capacity:\n\u21aa</b> {row.get('CAPACITY', 'N/A')}\"\n).add_to(m)\nfolium.GeoJson(\ncrime_gdf,\nname=\"Neighborhoods\",\nstyle_function=lambda x: {\n\"fillColor\": \"yellow\",\n\"color\": \"black\",\n\"weight\": 1,\n\"fillOpacity\": 0.3,\n},\ntooltip=folium.GeoJsonTooltip(\nfields=[\"AREA_NAME\"],\naliases=[\"Neighborhood:\"],\n),\n).add_to(m)\nm\nGeometry types in bike rack data:\nMultiPoint\n241\nName: count, dtype: int64\nValid Point geometries found: 241\n[57]: <folium.folium.Map at 0x715bc88e0370>\nTop 10 Bike Racks with the Highest Capacity\n[58]: # top 10 list of bike racks with the highest capacity\nhighest_capacity_query = f\"\"\"\n64"
    },
    {
        "page": 174,
        "text": "SELECT DISTINCT ADDRESS_FULL, CAPACITY\nFROM {BIKECRIMETABLE}\nORDER BY CAPACITY DESC\nLIMIT 10;\n\"\"\"\ncapacity_result = pd.read_sql(highest_capacity_query, engine)\ndisplay(capacity_result)\nADDRESS_FULL\nCAPACITY\n0\n61 Front St W\n72\n1\n5100 Yonge St\n64\n2\n675 Bloor St W\n48\n3\n100 Queen St W\n40\n4\n175 Mount Olive Dr\n32\n5\n65 Dundas St W\n32\n6\n743 Pape Ave\n32\n7\n35 York St\n30\n8\n2 Bloor St W\n24\n9\n236 Augusta Ave\n24\nTotals of Each Bike Rack Class\n[59]: map_class_query = f\"\"\"\nSELECT MAP_CLASS, COUNT(DISTINCT ADDRESS_FULL) AS total_bike_racks\nFROM {BIKECRIMETABLE}\nGROUP BY MAP_CLASS\nORDER BY total_bike_racks DESC;\n\"\"\"\nmap_class_result = pd.read_sql(map_class_query, engine)\ndisplay(map_class_result)\nMAP_CLASS\ntotal_bike_racks\n0\nMulti-Bike Rack\n110\n1\nBike Corral\n15\n2\nMulti-Bike Rack (Angled)\n6\n3\nBike Shelter\n3\n5\nDiscussion\nWhen analyzing the impact of police budgetary data on crime statistics in Toronto, we observed a\ncouple of unique trends. When simply looking at the raw data we noticed that as the total operating\nbudget increased over time, so did the average rates for assault, auto theft, and shooting. Although\nthis may not seem intuitive, it may have been due to a couple of factors. An increased budget could\nmean there was an improvement in reporting or detection of specific crimes, which could lead to\nan increase observed in those crime rates. It may also be an indication of improper allocation of\nresources for various years. Although there was some breakdown of allocation provided, it was not\nvery stringent, so it is possible that despite an increase in budget, areas useful in auto theft were\n65"
    },
    {
        "page": 175,
        "text": "underfunded, as an example.\nWhen trying to run regressional analysis on our data, we realized that our sample size for budgetary\ndata was too small, therefore we could not rely on standard regression methods. To overcome this\nproblem we decided it would be best to bootstrap our data. Upon doing so we found which crimes\nhad significant relationships with total operating budget by seeing which confidence intervals did\nnot contain 0. Of all the crimes analyzed, we found significant relationships between gross operating\nbudget and average rates of auto theft, theft over $5000, bike theft, and robbery.\nOnce we determined which relationships were significant, we plotted the boostrapped regression\nlines, as well as the mean regression line.\nOf all the plots, we observed the strongest positive\nrelationship between total budget and auto theft rates, and the strongest negative relationship\nbetween total budget and robbery rates.\nAnalysing the impact of income in neighbourhoods on various crime rates showed some interesting\nresults. We began by visualizing the relationships between assault, robbery, auto theft, and break-\nand-enter rates against income using scatter plots and regression lines. As can be seen from the\nresults, income showed a slight negative relationship with assault and auto theft rates, with each\nregression having poor R squared values, but neither showing significant coe\ufb00icients. Additionally,\nincome showed a negative relationship with robbery rates, but did not have a significant coe\ufb00icient.\nLastly, income showed a strong positive relationship with break and enter rates, with a poor R\nsquared, but a very significant coe\ufb00icient.\nFor the same 4 crime rates we broke income into 4 quartiles and visualised the average crime rates\nfor each quartile. The results reinforced the findings of the regression scatter plots somewhat. For\neach quartile both assault and robbery rates showed no pattern or trend across the 4 quartiles but\nauto theft and break and enter rates did show trends. Auto theft rates followed the relationship\nfound in the regression plots with a negative trend as income increased. Break and enter rates\nconfirmed the regression plot, with an increase in break and enter rates as income increased in\nneighbourhoods. These trends may be due to a couple of reasons. For auto theft rates, higher\nincome families are more likely to have a garage which would naturally reduce the amount of auto\nthefts. At the same time more wealthy areas would be more attractive targets for breaking and\nentering due to the greater amount of valuables likely at these homes.\nAdditionally, we analyzed homicide rates independently, as many data points had values of zero.\nLower income neighborhoods exhibited a higher frequency of homicides and generally higher rates\nof homicide.\nOur analysis found significant relationships between education levels, measured by degree rates, and\nvarious crime rates, excluding auto theft and homicide. Most relationships were positive, except for\nshootings, which exhibited a negative association. Despite these findings, the adjusted R-squared\nvalues were low, indicating that education levels alone do not explain much of the variability in\ncrime rates. Among the analyzed crimes, bike theft had the highest adjusted R-squared value at\n0.3260.\nInterestingly, neighborhoods with higher degree rates also tended to report higher crime rates, with\nthe downtown area exhibiting the highest levels. This may be attributed to the concentration of\npolice precincts and increased crime reporting in urban areas. This suggests that while education\nlevels do have an impact on crime rates, other factors also play a significant role in determining\ncrime rates in different neighborhoods.\nThe findings highlight the complexity of the relationship between education and crime. While higher\n66"
    },
    {
        "page": 176,
        "text": "education levels are generally associated with better economic opportunities and lower crime rates,\nthe data suggests that this relationship is not straightforward. The presence of other socioeconomic\nfactors, such as income levels, employment opportunities, and community resources, likely play a\ncrucial role in shaping crime rates. Therefore, a comprehensive approach that considers multiple\nfactors is essential for effectively addressing crime and improving public safety in urban areas.\nBased on our comparison of the shelter occupancy rates to crime rates by neighbourhood, we\nsee that our analyses did not produce the results we were expecting. At first glance, the graphs\nshow a general negative slope across the board, meaning that as shelter occupancy increases, crime\nrates decrease. The steepest slope we can observe is that of the break and enter rate vs. shelter\noccupancy. As it turns out, this is also the only crime that has a statistically significant correlation.\nThe correlation between occupancy rate and break and enter rates is -0.40, with a significant p-\nvalue of 0.0173. This could be for a number of reasons, perhaps the most intriguing being that if\nthere are more people in shelters, there are less people trying to break in to other homes to get\nwhat they need.\nAlthough the rest of the crimes all show a negative correlation, except for auto theft rate, none\nof them are statistically significant when assessing the p-values. In general, this leads us to an\noverall conclusion that shelter occupancy, at least on its own, does not contribute to crime rates in\ndifferent neighbourhoods in Toronto.\nWhen observed the relationship between bike racks and bik related crimes, we began by examining\nthe interactions between bike rack numbers and bike thefts in different neighbourhoods. Our anal-\nysis revealed a positive correlation of 0.81 between racks and capacity. Next, we looked at the total\nbike capacity in a neighbourhood and bike thefts. For this result, we saw an even greater correlation\nof 0.83. Both of these results indicate that not only bike racks, but also bike, higher capacity bike\nracks attract more bikes, and in turn, increase theft opportunities. Some contributing factors to\nthis may be high foot tra\ufb00ic, poor lighting, and broader socioeconomic conditions in high-crime\nareas. By combining bike rack data with neighbourhood crime statistics we could identify patterns\nin bike-related thefts. Our analysis revealed areas prone to theft that may need interventions to\nmitigate the problem; such as secure bike racks, improved lighting, surveillance, and community\neducation. Moving forward, some other areas for analysis may include connecting links between\nour household income analysis and comparing total crime trends to point out neighbourhoods that\nshould be avoided.\nBased on our results, some steps must be taken to improve bike theft rates, such as using the right\nsecurity measures, using GIS tools to mark high-risk zones or point out close bike racks in safe\nareas, and partnering with cities and developers to integrate theft-resistant infrastructure such as\nlighting and shelter. Overall, the project has revealed insights into the consequences of inadequate\nbike infrastructure and strategies to mitigate the issue.\nDiscussion Board Feedback\nFrom the discussion board on our video presentation we received\nthree main points of feedback that we felt were worth addressing. Although there were others, they\neither echoed previous statements, or were on matters that we had already addressed, but did not\nhave the time to incorporate to our presentation.\nThe first point we want to address asked about what other factors could explain the link between\nbike racks and thefts. Some possible factors are the amount of foot tra\ufb00ic in the area, whether the\narea is poorly lit, and various other socioeconomic factors. Although we do not have the data to\nexplore the first two factors, we could run an analysis that would include our income data, and\n67"
    },
    {
        "page": 177,
        "text": "determine whether or not it affected the relationship between bike racks and bike thefts.\nThe second point was to provide clarification on whether the break and enter variable would include\ninstances of breaking into someones car, and stealing something from inside the car. At the time of\nthis response, we have not heard back from the dataset owners to clarify if that is the case or not.\nThe last point was a suggestion to improve some of our visualizations, more specifically the budget\nfigures which included bars and lines. We adjusted this figure to reduce the transparency of the\nbudget bars, while increasing the thickness of the crime statistics line, to improve the visibility of\nthe lines.\n6\nConclusion\nOur analysis has come to some interesting conclusions for our guiding question that we asked at\nthe start of this project. For police budgets our bootstrap regressions seem to show that increasing\npolice budgets does not reduce all crime statistics. Auto theft rates seem to show a strong positive\nrelationship with the Toronto police budget.\nThis may indicate that there are likely alternate\nsolutions than more policing, to reduce some crime rates. When looking at neighborhood income,\nwe found that lower income areas showed higher levels of auto thefts, homicide, and break and\nenter crimes. For higher education rates the results were mixed, similar to neighborhood income.\nHigher rates of education reduced the rates of some crimes and raised the rates of others. Both of\nthese sections of our analysis could potentially inform where police and municipal resources would\nbe best focused.\nOur analysis of shelter occupancy showed that higher rates of occupancy decreased break and\nenter rates. While no other coe\ufb00icients were significant, they also showed negative relationships\nwith shelter occupancy. This analysis is an excellent example of how municipal budget focus on\nhomeless shelters or further preventative measures to reduce homelessness could reduce crime rates.\nOur analysis of bike racks inside neighbourhoods did show the expected results. Our analysis found\nthat the more bike racks available the more bike thefts that occurred with a very strong correlation\nbetween the presence of bike racks and thefts. While the presence of bike racks doe not indicate\na causal relationship it does show which areas have the worst problems with bike thefts, and that\nperhaps steps could be taken to better protect bikes in these areas from theft.\nBased on the findings of our results, we believe the solution to decreasing crime not a clear case\nwhere more policing will solve it.\nAlternative preventative methods such as homeless shelters,\ninventive high-capacity bike rack solutions such as the underground bike storage system in Tokyo,\nand more effective focus of police forces and budgets given the results of our analysis may take\nsteps towards reducing crime rates for residents of Toronto.\n7\nReferences\nBike Finder. (2024). About the Bike finder Team. Retrieved from How to Avoid Bike Thieves:\nCommon Bike Theft Hotspots and Prevention Tips - BikeFinder.\nCity\nof\nToronto.\n(2024).\nAbout\nToronto\nneighbourhoods.\nCity\nof\nToronto.\nhttps://www.toronto.ca/city-government/data-research-maps/neighbourhoods-\ncommunities/neighbourhood-profiles/about-toronto-neighbourhoods/\n68"
    },
    {
        "page": 178,
        "text": "City of Toronto.\n(2024, October 15).\nAddress points - Municipal Toronto One Address\nRepository.\nOpen Toronto.\nhttps://open.toronto.ca/dataset/address-points-municipal-toronto-\none-address-repository/\nCity of Toronto. (2024, October 15). Bicycle Parking Racks. City of Toronto Open Data Portal.\nhttps://open.toronto.ca/dataset/bicycle-parking-racks/\nCity of Toronto. (2024, October 15). Daily Shelter Occupancy. City of Toronto Open Data Portal.\nhttps://open.toronto.ca/dataset/daily-shelter-occupancy/\nCity of Toronto. (2024, October 15). Neighbourhood Crime Rates. City of Toronto Open Data\nPortal. https://open.toronto.ca/dataset/neighbourhood-crime-rates/\nCity of Toronto. (2024, October 15). Neighbourhood Profiles. City of Toronto Open Data Portal.\nhttps://open.toronto.ca/dataset/neighbourhood-profiles/\nCity of Toronto. (2024, October 15). Police Annual Statistical Report - Gross Operating Bud-\nget. City of Toronto Open Data Portal. https://open.toronto.ca/dataset/police-annual-statistical-\nreport-gross-operating-budget/\nGeeks for Geeks. (n.d.). Data visualization with Python Seaborn. Retrieved December 8, 2024,\nfrom https://www.geeksforgeeks.org/data-visualization-with-python-seaborn/\nGeoPandas.\n(n.d.).\nGeoDataFrame.\nRetrieved\nDecember\n8,\n2024,\nfrom\nhttps://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html\nGeoPandas. (n.d.). Projections and coordinate reference systems. Retrieved December 8, 2024,\nfrom https://geopandas.org/en/stable/docs/user_guide/projections.html\nGeoPandas.\n(n.d.).\nSpatial\njoin.\nRetrieved\nDecember\n8,\n2024,\nfrom\nhttps://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html\nLochner, L., & Moretti, E. (2004). The effect of education on crime: Evidence from prison inmates,\narrests, and self-reports. American Economic Review, 94(1), 155\u2013189.\nNumPy.\n(n.d.).\nCorrelation\ncoe\ufb00icients.\nRetrieved\nDecember\n8,\n2024,\nfrom\nhttps://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\nSaturn Cloud. (n.d.). Calculating slopes in NumPy or SciPy. Retrieved December 8, 2024, from\nhttps://saturncloud.io/blog/calculating-slopes-in-numpy-or-scipy/\nSciPy.\n(n.d.).\nIntroduction to SciPy.\nW3Schools.\nRetrieved December 8, 2024, from\nhttps://www.w3schools.com/python/scipy/scipy_intro.php\nStack Overflow. (2020). GeoPandas: How to convert the column geometry to string. Retrieved\nDecember 8, 2024, from https://stackoverflow.com/questions/61125808/geopandas-how-to-convert-\nthe-column-geometry-to-string\nToronto Police Service. (2024). About Toronto Police. City of Toronto. Retrieved from Budget -\nToronto Police Service.\nToronto Police Service.\n(2024).\nMy neighbourhood.\nRetrieved November 28, 2024, from\nhttps://www.tps.ca/my-neighbourhood/\nW3Schools.\n(n.d.).\nNumPy\nRandom\nSeaborn.\nRetrieved\nDecember\n8,\n2024,\nfrom\nhttps://www.w3schools.com/python/numpy/numpy_random_seaborn.asp\n69"
    },
    {
        "page": 179,
        "text": "[ ]:\n70"
    },
    {
        "page": 180,
        "text": "obesityestimation\nJuly 13, 2025\n1\nEstimation of Obesity levels based on lifestyle and dietary pat-\nterns\n1.1\nTeam Members\n1. Ayush Senthil Nelli\n2. Hritvik Gaind\n3. Satyam Kapoor\n4. Venkateshwaran Balu Soundararajan\n1.2\nTable of Contents\n\u2022 1. Introduction\n\u2022 2. Objective\n\u2013 2.1. Research Questions\n\u2022 3. Dataset Description\n\u2013 3.1. Data Source\n\u2013 3.2. Variables Overview\n\u2013 3.3. Feature Engineering\n\u2013 3.4. Multicollinearity\n\u2022 4. Exploratory Data Analysis (EDA)\n\u2013 4.1. Descriptive statistics (for numerical columns)\n\u2013 4.2. Analyzing the Outliers in the Dataset\n\u2013 4.3. Summary for categorical columns (using value_counts)\n\u2013 4.4. Check for Missing Values\n\u2013 4.5. Histograms for Numerical Features\n\u2013 4.6. Obesity Levels by Gender\n1"
    },
    {
        "page": 181,
        "text": "\u2013 4.7. Grouped Bar Chart (Counts by Age Group)\n\u2013 4.8. Categorical Data Analysis\n\u2013 4.9. Impact of Family History on Obesity\n\u2022 5. Machine Learning Modeling\n\u2013 5.1. Label Encoding of Categorical Variables\n\u2013 5.2. Test Train Split of Encoded Data\n\u2013 5.3. Multiclassfication tasks with Default Parameters\n\u2217 5.3.1. Logistic Regression\n\u2217 5.3.2. Decision Tree Classifier\n\u2217 5.3.3. Random Forest Classifier\n\u2217 5.3.4. XGBoost Classifier\n\u2217 Next Step: Hyperparameter Tuning\n\u2013 5.4. Multiclassfication tasks with Hypertuning\n\u2217 5.4.1. Random Forest Classifier with HyperParameter\n\u2217 5.4.2. XGBoost Classifier with Hyperparameter Tuning\n\u2217 Inference\n\u2217 Conclusion\n\u2013 5.5. Machine Learning Modeling (Binary Classification)\n\u2217 5.5.1. Grouping obesity to two groups that whether the person is obese(1) or not(0)\n\u2217 5.5.2. Apply Label Encoding\n\u2217 5.5.3. Logistic Regression\n\u2217 5.5.4. Random Forest\n\u2217 5.5.6. XGBoost\n\u2013 5.6. Plotting Decision Boundaries\n\u2022 6.0. Future Scope\n\u2022 7.0. Conclusion\n\u2022 8.0. References\n2"
    },
    {
        "page": 182,
        "text": "1.3\n1. Introduction\nObesity is one of the most significant public health challenges of the 21st century. According to\nthe World Health Organization (WHO), global obesity rates have nearly tripled since 1975, with\nover 1.9 billion adults classified as overweight and more than 650 million considered obese. Obesity\nis associated with an increased risk of cardiovascular diseases, diabetes, hypertension, and\nother chronic health conditions, making it a critical area of research for healthcare professionals\nand policymakers.\nWith the rise of sedentary lifestyles, unhealthy dietary habits, and reduced physical\nactivity, obesity rates continue to climb, particularly in developing countries. In Latin America,\nwhere this dataset originates, obesity is a growing concern due to shifts in dietary patterns,\nurbanization, and lifestyle changes. Mexico, for instance, has one of the highest obesity rates\nin the world, with nearly 75% of the population being classified as overweight or obese.\n1.4\n2. Objective\nUnderstanding the key factors that contribute to obesity is essential for developing preventive\nstrategies, personalized health interventions, and public health policies.\nOur motivation for this project is to:\n- Identify patterns and trends in dietary habits and physical activity that contribute to obesity.\n- Develop predictive models that classify individuals into different obesity levels based on their\nlifestyle choices.\n- Provide actionable insights that can help individuals, healthcare professionals, and policy-\nmakers make informed decisions about obesity prevention and management.\nThe main objective of this project is to develop a classification model that predicts an individ-\nual\u2019s obesity level based on their lifestyle and dietary habits. Specifically, we aim to:\n- Clean and preprocess the dataset, handling missing values and encoding categorical variables.\n- Perform exploratory data analysis (EDA) to understand distributions, correlations, and key\ntrends.\n- Train and compare different machine learning models, such as decision trees, logistic\nregression, and support vector machines.\n- Evaluate model performance using metrics like accuracy, precision, recall, and F1-score.\n- Interpret findings to provide meaningful insights into obesity risk factors.\n1.4.1\n2.1. Research Questions\nFeature Importance Analysis - Which factors (e.g., water intake, physical activity, transporta-\ntion mode) are most predictive of obesity levels? - How do lifestyle choices (e.g., alcohol consump-\ntion, calorie monitoring) correlate with obesity severity?\nDemographic and Behavioral Patterns\n- Are there significant differences in obesity levels between genders or age groups? - How does\nthe interaction between physical activity (FAF) and sedentary behavior (TUE) influence obesity\nclassification?\nModel Performance for Obesity Classification\n- Can machine learning models accurately classify obesity levels using behavioral and physical\n3"
    },
    {
        "page": 183,
        "text": "attributes? - How do different classification algorithms (e.g., Random Forest, Logistic Regression,\nSVM) compare in performance for this task?\n1.5\n3. Dataset Description\n1.5.1\n3.1. Data Source\nThe dataset used in this project, \u201cEstimation of Obesity Levels Based on Eating Habits\nand Physical Condition\u201d, was donated on August 26, 2019, and is publicly available. It contains\ndata from individuals in Mexico, Peru, and Colombia.\n\u2022 The dataset has 2111 instances and 17 attributes, including demographic features, eating\nhabits, and physical activity levels.\n\u2022 The target variable, NObeyesdad (Obesity Level), categorizes individuals into:\n\u2013 Insu\ufb00icient Weight\n\u2013 Normal Weight\n\u2013 Overweight Level I\n\u2013 Overweight Level II\n\u2013 Obesity Type I\n\u2013 Obesity Type II\n\u2013 Obesity Type III\n1.5.2\n3.2. Variables Overview\nVariable Name\nRole\nType\nDescription\nGender\nFeature\nCategorical\nGender of the individual\nAge\nFeature\nContinuous\nAge in years\nHeight\nFeature\nContinuous\nHeight in meters\nWeight\nFeature\nContinuous\nWeight in kg\nfamily_history_with_overweight Feature\nBinary\nFamily history of overweight\n(Yes/No)\nFAVC\nFeature\nBinary\nFrequent consumption of\nhigh-caloric food (Yes/No)\nFCVC\nFeature\nInteger\nFrequency of vegetable\nconsumption (scale-based)\nNCP\nFeature\nContinuous\nNumber of main meals per\nday\nCAEC\nFeature\nCategorical\nConsumption of food between\nmeals\nSMOKE\nFeature\nBinary\nSmoking habits (Yes/No)\nCH2O\nFeature\nContinuous\nDaily water intake\n4"
    },
    {
        "page": 184,
        "text": "Variable Name\nRole\nType\nDescription\nSCC\nFeature\nBinary\nCalorie monitoring habits\n(Yes/No)\nFAF\nFeature\nContinuous\nFrequency of physical activity\nTUE\nFeature\nInteger\nTime spent on technology\nusage (hours per day)\nCALC\nFeature\nCategorical\nAlcohol consumption\nfrequency\nMTRANS\nFeature\nCategorical\nPrimary mode of\ntransportation\nNObeyesdad\nTarget\nCategorical\nObesity level classification\n[ ]: import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,\u2423\n\u21aaclassification_report,ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.decomposition import PCA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n[ ]: df= pd.read_csv('ObesityDataSet.csv')\ndf.head()\n[ ]:\nGender\nAge\nHeight\nWeight family_history_with_overweight FAVC\nFCVC\n\\\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\nNCP\nCAEC SMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\n\\\n0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\n1\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\n5"
    },
    {
        "page": 185,
        "text": "2\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\n3\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\n4\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nMTRANS\nNObeyesdad\n0\nPublic_Transportation\nNormal_Weight\n1\nPublic_Transportation\nNormal_Weight\n2\nPublic_Transportation\nNormal_Weight\n3\nWalking\nOverweight_Level_I\n4\nPublic_Transportation\nOverweight_Level_II\n[ ]: df.columns\n[ ]: Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',\n'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',\n'CALC', 'MTRANS', 'NObeyesdad'],\ndtype='object')\n[ ]: df = df.rename(columns={'NObeyesdad': 'obesity_level'})\n[ ]: df.head()\n[ ]:\nGender\nAge\nHeight\nWeight family_history_with_overweight FAVC\nFCVC\n\\\n0\nFemale\n21.0\n1.62\n64.0\nyes\nno\n2.0\n1\nFemale\n21.0\n1.52\n56.0\nyes\nno\n3.0\n2\nMale\n23.0\n1.80\n77.0\nyes\nno\n2.0\n3\nMale\n27.0\n1.80\n87.0\nno\nno\n3.0\n4\nMale\n22.0\n1.78\n89.8\nno\nno\n2.0\nNCP\nCAEC SMOKE\nCH2O\nSCC\nFAF\nTUE\nCALC\n\\\n0\n3.0\nSometimes\nno\n2.0\nno\n0.0\n1.0\nno\n1\n3.0\nSometimes\nyes\n3.0\nyes\n3.0\n0.0\nSometimes\n2\n3.0\nSometimes\nno\n2.0\nno\n2.0\n1.0\nFrequently\n3\n3.0\nSometimes\nno\n2.0\nno\n2.0\n0.0\nFrequently\n4\n1.0\nSometimes\nno\n2.0\nno\n0.0\n0.0\nSometimes\nMTRANS\nobesity_level\n0\nPublic_Transportation\nNormal_Weight\n1\nPublic_Transportation\nNormal_Weight\n2\nPublic_Transportation\nNormal_Weight\n3\nWalking\nOverweight_Level_I\n4\nPublic_Transportation\nOverweight_Level_II\n1.5.3\n3.3. Feature Engineering\nBMI: Calculate Body Mass Index from Height and Weight.\n[ ]: df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n6"
    },
    {
        "page": 186,
        "text": "Physical Activity Level: Combining FAF (frequency) and TUE (tech use) to estimate sedentary\nbehavior.\n[ ]: df['SedentaryScore'] = df['TUE'] - df['FAF']\n# Higher score = more sedentary\nDiet Score: Combining FCVC (vegetables), NCP (meals), and CAEC (eating between meals).\n[ ]: df['DietScore'] = df['FCVC'] + df['NCP'] - df['CAEC'].map({'no': 0, 'Sometimes':\n\u21aa 1, 'Frequently': 2, 'Always': 3})\n[ ]: original_df = df.copy()\ndf = df.drop(columns={'TUE','FAF','Weight','Height','FCVC','NCP','CAEC'})\n[ ]: df.head()\n[ ]:\nGender\nAge family_history_with_overweight FAVC SMOKE\nCH2O\nSCC\n\\\n0\nFemale\n21.0\nyes\nno\nno\n2.0\nno\n1\nFemale\n21.0\nyes\nno\nyes\n3.0\nyes\n2\nMale\n23.0\nyes\nno\nno\n2.0\nno\n3\nMale\n27.0\nno\nno\nno\n2.0\nno\n4\nMale\n22.0\nno\nno\nno\n2.0\nno\nCALC\nMTRANS\nobesity_level\nBMI\n\\\n0\nno\nPublic_Transportation\nNormal_Weight\n24.386526\n1\nSometimes\nPublic_Transportation\nNormal_Weight\n24.238227\n2\nFrequently\nPublic_Transportation\nNormal_Weight\n23.765432\n3\nFrequently\nWalking\nOverweight_Level_I\n26.851852\n4\nSometimes\nPublic_Transportation\nOverweight_Level_II\n28.342381\nSedentaryScore\nDietScore\n0\n1.0\n4.0\n1\n-3.0\n5.0\n2\n-1.0\n4.0\n3\n-2.0\n5.0\n4\n0.0\n2.0\n1.5.4\n3.4. Multicollinearity\nNow, we take a closer look at multicollinearity in our dataset. Multicollinearity happens when\nindependent variables are too closely related, making it di\ufb00icult for our model to determine the\nindividual impact of each feature. To address this, we calculate the Variance Inflation Factor (VIF)\nfor each numeric feature.\nA high VIF value (typically above 5 or 10) indicates that a feature is highly correlated with others,\nwhich could lead to unstable predictions. By identifying and potentially removing such variables,\nwe can improve our model\u2019s reliability and interpretability. Let\u2019s dive in and see which features\nmight be problematic!\n7"
    },
    {
        "page": 187,
        "text": "[ ]: #Select only numeric features (VIF works on continuous variables)\nnumeric_data = df.select_dtypes(include=['int64', 'float64'])\n#Adding a constant\nX = add_constant(numeric_data)\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.\n\u21aashape[1])]\n# Drop the 'const' row\nvif_data = vif_data[vif_data[\"Feature\"] != \"const\"]\nprint(vif_data.sort_values(\"VIF\"))\nFeature\nVIF\n4\nSedentaryScore\n1.059402\n2\nCH2O\n1.060852\n1\nAge\n1.086395\n5\nDietScore\n1.145523\n3\nBMI\n1.234568\nThe VIF values for all features are low (close to 1), indicating that multicollinearity is not a concern\nin this dataset. This suggests that our features are independent and suitable for modeling without\nfurther adjustments.\n1.6\n4. Exploratory Data Analysis (EDA)\n1.6.1\n4.1. Descriptive statistics (for numerical columns)\n[ ]: numerical_cols = ['Age', 'CH2O', 'BMI', 'SedentaryScore', 'DietScore']\nsummary_stats = df.describe()\nsummary_stats\n[ ]:\nAge\nCH2O\nBMI\nSedentaryScore\nDietScore\ncount\n2111.000000\n2111.000000\n2111.000000\n2111.000000\n2111.000000\nmean\n24.312600\n2.008011\n29.700159\n-0.352432\n3.963979\nstd\n6.345968\n0.612953\n8.011337\n1.016679\n1.022922\nmin\n14.000000\n1.000000\n12.998685\n-3.000000\n0.000000\n25%\n19.947192\n1.584812\n24.325802\n-1.000000\n3.286494\n50%\n22.777890\n2.000000\n28.719089\n-0.288792\n4.000000\n75%\n26.000000\n2.477420\n36.016501\n0.321731\n5.000000\nmax\n61.000000\n3.000000\n50.811753\n2.000000\n6.000000\n8"
    },
    {
        "page": 188,
        "text": "1.6.2\n4.2. Analyzing the Outliers in the Dataset\n[ ]: # Outlier detection using IQR\nQ1 = df[numerical_cols].quantile(0.25)\nQ3 = df[numerical_cols].quantile(0.75)\nIQR = Q3 - Q1\noutliers = ((df[numerical_cols] < (Q1 - 1.5 * IQR)) | (df[numerical_cols] > (Q3\u2423\n\u21aa+ 1.5 * IQR))).sum()\nprint(\"\\nNumber of Outliers per Column:\")\nprint(outliers)\nNumber of Outliers per Column:\nAge\n168\nCH2O\n0\nBMI\n0\nSedentaryScore\n33\nDietScore\n5\ndtype: int64\n1.6.3\n4.3. Summary for categorical columns (using value_counts)\n[ ]: categorical_summary = df.describe(include=['object'])\ncategorical_summary\n[ ]:\nGender family_history_with_overweight\nFAVC SMOKE\nSCC\nCALC\n\\\ncount\n2111\n2111\n2111\n2111\n2111\n2111\nunique\n2\n2\n2\n2\n2\n4\ntop\nMale\nyes\nyes\nno\nno\nSometimes\nfreq\n1068\n1726\n1866\n2067\n2015\n1401\nMTRANS\nobesity_level\ncount\n2111\n2111\nunique\n5\n7\ntop\nPublic_Transportation\nObesity_Type_I\nfreq\n1580\n351\n1.6.4\n4.4. Check for missing values\n[ ]: print(df.shape)\nprint(df.columns)\n(2111, 13)\nIndex(['Gender', 'Age', 'family_history_with_overweight', 'FAVC', 'SMOKE',\n'CH2O', 'SCC', 'CALC', 'MTRANS', 'obesity_level', 'BMI',\n'SedentaryScore', 'DietScore'],\ndtype='object')\n9"
    },
    {
        "page": 189,
        "text": "[ ]: missing_data = df.isnull().sum()\nprint(\"\\nMissing Data:\")\nprint(missing_data)\nMissing Data:\nGender\n0\nAge\n0\nfamily_history_with_overweight\n0\nFAVC\n0\nSMOKE\n0\nCH2O\n0\nSCC\n0\nCALC\n0\nMTRANS\n0\nobesity_level\n0\nBMI\n0\nSedentaryScore\n0\nDietScore\n0\ndtype: int64\nThe dataset has no missing values, ensuring a clean foundation for analysis.\nAdditionally, we\nhave already introduced new features, such as SedentaryScore and DietScore, during the feature\ngeneration step to enhance our model\u2019s predictive power.\n1.6.5\n4.5. Histograms for Numerical Features\n[ ]: sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Distribution of Key Health Metrics', fontsize=16, y=1.02,\u2423\n\u21aafontweight='bold')\nmetrics = ['Age', 'Height', 'Weight', 'BMI']\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\nfor ax, metric, color in zip(axes.flatten(), metrics, colors):\nax.hist(original_df[metric], bins=12, color=color, edgecolor='white',\u2423\n\u21aalinewidth=0.8, alpha=0.85)\nax.set_title(f'{metric} Distribution', fontsize=12, pad=10,\u2423\n\u21aafontweight='bold')\nax.set_xlabel(metric, fontsize=10)\nax.set_ylabel('Frequency', fontsize=10)\nax.grid(True, linestyle='--', alpha=0.6)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n10"
    },
    {
        "page": 190,
        "text": "plt.tight_layout()\nplt.show()\nDistribution Analysis of Key Health Metrics\nAge Distribution\n- Left-skewed, with most individuals clustered in younger age groups.\n- Frequency drops sharply after age ~30, indicating a predominantly young population.\nHeight Distribution\n- Roughly normal (bell-shaped), peaking around 1.6\u20131.8 meters.\n- Few outliers at extreme heights (very short/tall individuals).\nWeight Distribution\n- Right-skewed, with most individuals concentrated in the 50\u2013100 kg range.\n- A long tail suggests some higher weight outliers.\nBMI Distribution\n11"
    },
    {
        "page": 191,
        "text": "- Right-skewed, similar to weight, with most values in the normal/overweight range.\n- Noticeable frequency in higher BMI categories, indicating potential obesity concerns.\n1.6.6\n4.6. Obesity Levels by Gender\n[ ]: # Create normalized cross-tab (proportions)\ngender_obesity = pd.crosstab(df['Gender'], df['obesity_level'],\u2423\n\u21aanormalize='index') * 100\n# Plot stacked bars\nax = gender_obesity.plot(kind='bar', stacked=True, figsize=(12, 6), color=sns.\n\u21aacolor_palette(\"husl\", len(gender_obesity.columns)))\nplt.title(\"Obesity Level Distribution by Gender (%)\", pad=20, fontweight='bold')\nplt.ylabel(\"Percentage\")\nplt.xlabel(\"Gender\")\nplt.xticks(rotation=0)\n# Add percentage labels\nfor i, gender in enumerate(gender_obesity.index):\ny_offset = 0\nfor j, level in enumerate(gender_obesity.columns):\nvalue = gender_obesity.loc[gender, level]\nif value > 0:\n# Only label segments >0%\nax.text(\nx=i,\ny=y_offset + value/2,\ns=f\"{value:.1f}%\",\nha='center',\nva='center',\ncolor='black',\n# Auto-text color contrast\nfontweight='bold'\n)\ny_offset += value\nplt.legend(title=\"Obesity Level\", bbox_to_anchor=(1.05, 1))\nplt.tight_layout()\nplt.show()\n12"
    },
    {
        "page": 192,
        "text": "This graph compares obesity levels between genders using stacked bars, showing the percentage\ndistribution of each weight category (from normal to severe obesity) for males and females. It\nhighlights how obesity risk differs by gender, revealing which groups dominate specific weight\nclasses.\n\u2022 The middle obesity categories (Overweight I/II, Obesity I) are male-dominated.\n\u2022 Females often exhibit a bimodal distribution, clustering in healthy weight or severe\nobesity.\n\u2022 Males show a more gradual progression across weight categories.\n1.6.7\n4.7. Grouped Bar Chart (Counts by Age Group)\n[ ]: # Define age bins and labels\nage_bins = [0, 20, 30, 40, 50, 60, 100]\nage_labels = ['<20', '20-29', '30-39', '40-49', '50-59', '60+']\n# Create age groups\ndf['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels,\u2423\n\u21aaright=False)\n# Cross-tabulation\nage_obesity = pd.crosstab(\nindex=df['AgeGroup'],\ncolumns=df['obesity_level'],\nnormalize='index'\n) * 100\n13"
    },
    {
        "page": 193,
        "text": "# Create figure with DPI adjustment\nfig = plt.figure(figsize=(12, 12), dpi=100)\n# 4000x4000 pixels\nax = fig.add_subplot(111)\n# Plot with absolute control\nage_obesity.plot(kind='bar', stacked=True, width=0.85, ax=ax,\nedgecolor='black', linewidth=2)\n# Customize labels with precise positioning\nfor i, age_group in enumerate(age_obesity.index):\ncumulative = 0\nfor j, level in enumerate(age_obesity.columns):\nvalue = age_obesity.loc[age_group, level]\nif value > 0.5:\n# Label nearly everything\nax.text(i, cumulative + value/2, f\"{value:.1f}%\",\nha='center', va='center',\nfontsize=24, fontweight='bold',\ncolor='black',\nbbox=dict(boxstyle='round,pad=0.2',\nfacecolor='#00000020' if value < 5 else 'none',\nedgecolor='none'))\ncumulative += value\n# Mega styling\nax.set_title(\"OBESITY DISTRIBUTION BY AGE GROUP\",\nfontsize=48, pad=40, fontweight='black')\nax.set_ylabel(\"Percentage\", fontsize=36, labelpad=20)\nax.set_xlabel(\"Age Group\", fontsize=36, labelpad=20)\n# Giant ticks\nax.tick_params(axis='both', which='major', labelsize=32)\nax.tick_params(axis='x', rotation=0)\n# Legend the size of a billboard\nlegend = ax.legend(title=\"Obesity Levels\", fontsize=28,\ntitle_fontsize=32, framealpha=1,\nbbox_to_anchor=(1.05, 1),\nborderpad=2)\nplt.show()\n14"
    },
    {
        "page": 194,
        "text": "This visualization shows the percentage breakdown of obesity severity levels across different age\ngroups, using stacked bars to reveal how weight categories shift from underweight to severe obesity\nas people age.\n\u2022 Obesity severity escalates with age \u2013 Type II/III obesity reaches 45%+ in the 60+ group.\n\u2022 Younger demographics (<30) have better weight profiles, with 32.8% normal weight in\nthe 20-29 group.\n\u2022 The 40-49 age group is a critical transition point, where obesity prevalence first exceeds\n50%.\n\u2022 Significant obesity presence (11.4%) in the <20 group highlights the need for early interven-\ntion.\n\u2022 Overweight Level II appears surprisingly low across all age groups.\n\u2022 The 20-29 group has the healthiest distribution, with the highest normal weight per-\ncentage.\n1.6.8\n4.8. Categorical Data Analysis\n[ ]: sns.set_style(\"white\")\nplt.figure(figsize=(14, 6))\n# Gender plot\n15"
    },
    {
        "page": 195,
        "text": "plt.subplot(1, 2, 1)\nsns.countplot(x='Gender', data=df, palette=['#1f77b4', '#ff7f0e'],\nedgecolor='black', linewidth=0.5)\nplt.title('Gender Distribution', fontsize=13, pad=12, fontweight='semibold')\nplt.xlabel('', fontsize=11)\nplt.ylabel('Count', fontsize=11)\nplt.grid(axis='y', linestyle='--', alpha=0.4)\n# Obesity plot\nplt.subplot(1, 2, 2)\nsns.countplot(x='obesity_level', data=df, palette='viridis',\nedgecolor='black', linewidth=0.5)\nplt.title('Weight Status Distribution', fontsize=13, pad=12,\u2423\n\u21aafontweight='semibold')\nplt.xlabel('', fontsize=11)\nplt.ylabel('', fontsize=11)\nplt.xticks(rotation=45, ha='right', fontsize=10)\nplt.grid(axis='y', linestyle='--', alpha=0.4)\nplt.tight_layout(pad=2.5)\nplt.show()\nThe graph displays two subplots: a count plot of Gender distribution (left) and a count plot of\nobesity_level distribution (right), using data from the dataset. The visualizations use distinct color\npalettes and grid styling for clarity.\n\u2022 The gender distribution shows a near-equal split between females and males, with counts\naround 800\u20131000 each.\n\u2022 The weight status distribution reveals a diverse spread across categories, with Obe-\nsity_Type_I and Obesity_Type_III being the most frequent, while Insu\ufb00i-\ncient_Weight is the least common, indicating a higher prevalence of obesity in the population.\n16"
    },
    {
        "page": 196,
        "text": "1.6.9\n4.9. Impact of Family History on Obesity\n[ ]: plt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='SMOKE', hue='obesity_level',\npalette='viridis',\nedgecolor='white',\nlinewidth=0.5)\nplt.title(\"Obesity Levels by Smoking Status\",\nfontsize=14, pad=12)\nplt.xlabel(\"Smoking Status\", fontsize=12)\nplt.ylabel(\"Count\", fontsize=12)\nplt.legend(title='Obesity Level',\nbbox_to_anchor=(1, 0.5),\nloc='center left',\nframeon=False)\nsns.despine()\nplt.tight_layout()\nplt.show()\nThis side-by-side bar chart compares obesity level distributions between smokers and non-smokers,\nshowing how weight categories vary by tobacco use status.\n\u2022 Non-smokers show higher counts across all obesity categories, likely reflecting population\ndistribution.\n17"
    },
    {
        "page": 197,
        "text": "\u2022 Smokers have proportionally more cases in mid-range weight categories (Overweight I/II).\n\u2022 Severe obesity (Type II/III) appears less prevalent among smokers compared to non-smokers.\n\u2022 The Insu\ufb00icient Weight category has minimal representation in both groups.\n\u2022 Normal weight distribution follows similar patterns regardless of smoking status.\n1.7\n5. Machine Learning Modeling\n1.7.1\n5.1. Label Encoding of Categorical Variables\n[ ]: # Initialize the LabelEncoder\nlabel_encoder = LabelEncoder()\n# Identify categorical columns\ncategorical_columns = ['Gender', 'family_history_with_overweight','FAVC',\n'SMOKE', 'SCC', 'CALC', 'MTRANS', 'obesity_level']\n# Create a new DataFrame to store encoded data\nencoded_data = df.copy()\n# Apply Label Encoding to each categorical column\nfor column in categorical_columns:\nencoded_data[column] = label_encoder.fit_transform(df[column])\n# Display the newly encoded DataFrame\nencoded_data.head()\n[ ]:\nGender\nAge\nfamily_history_with_overweight\nFAVC\nSMOKE\nCH2O\nSCC\nCALC\n\\\n0\n0\n21.0\n1\n0\n0\n2.0\n0\n3\n1\n0\n21.0\n1\n0\n1\n3.0\n1\n2\n2\n1\n23.0\n1\n0\n0\n2.0\n0\n1\n3\n1\n27.0\n0\n0\n0\n2.0\n0\n1\n4\n1\n22.0\n0\n0\n0\n2.0\n0\n2\nMTRANS\nobesity_level\nBMI\nSedentaryScore\nDietScore\n0\n3\n1\n24.386526\n1.0\n4.0\n1\n3\n1\n24.238227\n-3.0\n5.0\n2\n3\n1\n23.765432\n-1.0\n4.0\n3\n4\n5\n26.851852\n-2.0\n5.0\n4\n3\n6\n28.342381\n0.0\n2.0\n1.7.2\n5.2. Test Train Split of Encoded Data\n[ ]: X = encoded_data.drop(columns=['obesity_level','AgeGroup','BMI']) # Features\u2423\n\u21aa(all columns except the target)\ny = encoded_data['obesity_level'] # Target (obesity_level)\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\u2423\n\u21aarandom_state=42)\n18"
    },
    {
        "page": 198,
        "text": "# Print the shape of training and testing sets\nprint(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\nprint(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)\nTraining set shape (X_train, y_train): (1688, 11) (1688,)\nTesting set shape (X_test, y_test): (423, 11) (423,)\n1.7.3\n5.3. Multiclassfication tasks with Default Parameters\n5.3.1. Logistic Regression\n[ ]: log_model = LogisticRegression(multi_class='multinomial',random_state=42)\nlog_model.fit(X_train, y_train)\ny_predlog=log_model.predict(X_test)\nprint(\"\\nLogistic Regression:\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_predlog))\nprint(classification_report(y_test, y_predlog))\nLogistic Regression:\nAccuracy: 0.4988179669030733\nprecision\nrecall\nf1-score\nsupport\n0\n0.42\n0.50\n0.46\n56\n1\n0.33\n0.16\n0.22\n62\n2\n0.49\n0.59\n0.53\n78\n3\n0.46\n0.78\n0.58\n58\n4\n0.79\n1.00\n0.88\n63\n5\n0.23\n0.09\n0.13\n56\n6\n0.44\n0.28\n0.34\n50\naccuracy\n0.50\n423\nmacro avg\n0.45\n0.49\n0.45\n423\nweighted avg\n0.46\n0.50\n0.46\n423\n5.3.2. Decision Tree Classifier\n[ ]: dt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\n# Step 2: Predict on the test set\ny_pred_dt = dt_model.predict(X_test)\n# Step 3: Evaluate the model's performance\naccuracy = accuracy_score(y_test, y_pred_dt)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_dt))\nAccuracy: 0.70\nClassification Report:\n19"
    },
    {
        "page": 199,
        "text": "precision\nrecall\nf1-score\nsupport\n0\n0.73\n0.71\n0.72\n56\n1\n0.53\n0.50\n0.51\n62\n2\n0.65\n0.71\n0.67\n78\n3\n0.82\n0.79\n0.81\n58\n4\n0.97\n1.00\n0.98\n63\n5\n0.67\n0.57\n0.62\n56\n6\n0.56\n0.62\n0.59\n50\naccuracy\n0.70\n423\nmacro avg\n0.70\n0.70\n0.70\n423\nweighted avg\n0.70\n0.70\n0.70\n423\n[ ]: dt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\n# Step 2: Predict on the test set\ny_pred_dt = dt_model.predict(X_test)\n# Step 3: Evaluate the model's performance\naccuracy = accuracy_score(y_test, y_pred_dt)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_dt))\nAccuracy: 0.70\nClassification Report:\nprecision\nrecall\nf1-score\nsupport\n0\n0.73\n0.71\n0.72\n56\n1\n0.53\n0.50\n0.51\n62\n2\n0.65\n0.71\n0.67\n78\n3\n0.82\n0.79\n0.81\n58\n4\n0.97\n1.00\n0.98\n63\n5\n0.67\n0.57\n0.62\n56\n6\n0.56\n0.62\n0.59\n50\naccuracy\n0.70\n423\nmacro avg\n0.70\n0.70\n0.70\n423\nweighted avg\n0.70\n0.70\n0.70\n423\n5.3.3. Random Forest Classifier\n[ ]: rf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\nrf_predictions = rf_model.predict(X_test)\nprint(\"Random Forest Classifier:\")\n20"
    },
    {
        "page": 200,
        "text": "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\nprint(classification_report(y_test, rf_predictions))\nRandom Forest Classifier:\nAccuracy: 0.789598108747045\nprecision\nrecall\nf1-score\nsupport\n0\n0.86\n0.79\n0.82\n56\n1\n0.62\n0.63\n0.62\n62\n2\n0.76\n0.79\n0.78\n78\n3\n0.80\n0.95\n0.87\n58\n4\n1.00\n1.00\n1.00\n63\n5\n0.73\n0.71\n0.72\n56\n6\n0.78\n0.62\n0.69\n50\naccuracy\n0.79\n423\nmacro avg\n0.79\n0.78\n0.79\n423\nweighted avg\n0.79\n0.79\n0.79\n423\n5.3.4. XGBoost Classifier\n[ ]: xgb_model = XGBClassifier()\nxgb_model.fit(X_train, y_train)\nxgb_predictions = xgb_model.predict(X_test)\nprint(\"\\nXGBoost Classifier:\")\nprint(\"Accuracy:\", accuracy_score(y_test, xgb_predictions))\nprint(classification_report(y_test, xgb_predictions))\nXGBoost Classifier:\nAccuracy: 0.7966903073286052\nprecision\nrecall\nf1-score\nsupport\n0\n0.85\n0.80\n0.83\n56\n1\n0.65\n0.65\n0.65\n62\n2\n0.77\n0.77\n0.77\n78\n3\n0.82\n0.91\n0.86\n58\n4\n1.00\n1.00\n1.00\n63\n5\n0.74\n0.75\n0.74\n56\n6\n0.76\n0.68\n0.72\n50\naccuracy\n0.80\n423\nmacro avg\n0.80\n0.79\n0.79\n423\nweighted avg\n0.80\n0.80\n0.80\n423\nTree-based models such as Decision Tree, Random Forest, and XGBoost outperform Logistic Re-\ngression, highlighting the advantage of non-linear decision boundaries in obesity classification. Lo-\ngistic Regression, with its linear approach, struggles to capture the complexity of obesity-related\n21"
    },
    {
        "page": 201,
        "text": "trends.\nBetween Random Forest and XGBoost, both models achieve comparable peak accuracy, but XG-\nBoost proves to be more computationally e\ufb00icient, making it well-suited for scalable healthcare\napplications. The boosting mechanism in XGBoost ensures faster convergence while maintaining\nhigh predictive power.\nBoth Random Forest and XGBoost deliver high accuracy, but XGBoost\u2019s faster training time\nand optimized computational e\ufb00iciency make it the ideal choice for real-time obesity classification\nand predictive healthcare analytics. The selection between these models depends on the specific\nrequirements of deployment, whether emphasizing interpretability and ensemble stability (Random\nForest) or speed and scalability (XGBoost).\nNext Step: Hyperparameter Tuning for Random Forest and XGBoost\nTo further im-\nprove the model\u2019s performance, we will proceed with hyperparameter tuning for Random\nForest and XGBoost. This step involves optimizing the key parameters to enhance accuracy\nand computational e\ufb00iciency.\n1.7.4\n5.4. Multiclassfication tasks with Hypertuning\nTo maximize model performance, hyperparameter tuning has been applied to both Random\nForest and XGBoost.\n[ ]: # Define the parameter grid for Grid Search\nparam_grid = {\n'n_estimators': [50, 100, 150, 200],\n# Number of trees in the forest\n'max_depth': [None, 10, 20, 30],\n# Maximum depth of trees\n'min_samples_split': [2, 5, 10],\n# Minimum samples required to split a\u2423\n\u21aanode\n'min_samples_leaf': [1, 2, 4],\n# Minimum samples at a leaf node\n'max_features': ['sqrt', 'log2'],\n# Number of features to consider at\u2423\n\u21aaeach split\n'criterion': ['gini', 'entropy'],\n# Splitting criteria\n'class_weight': ['balanced', 'balanced_subsample']\n# Handling class\u2423\n\u21aaimbalance\n}\n5.4.1. Random Forest Classifier with HyperParameter\n[ ]: # Initialize the Random Forest Classifier\nrf_model = RandomForestClassifier(random_state=42)\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5,\u2423\n\u21aascoring='accuracy', n_jobs=-1,verbose=3)\ngrid_search.fit(X_train, y_train)\n# Get the best parameters and best model\nbest_params = grid_search.best_params_\nbest_rf_model = grid_search.best_estimator_\n# Print the best parameters\n22"
    },
    {
        "page": 202,
        "text": "print(\"Best Parameters:\", best_params)\n# Evaluate the best model on the test set\nrf_predictions = best_rf_model.predict(X_test)\nprint(\"\\nRandom Forest Classifier with Best Parameters:\")\nprint(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\nprint(classification_report(y_test, rf_predictions))\nFitting 5 folds for each of 1152 candidates, totalling 5760 fits\nBest Parameters: {'class_weight': 'balanced_subsample', 'criterion': 'gini',\n'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1,\n'min_samples_split': 2, 'n_estimators': 150}\nRandom Forest Classifier with Best Parameters:\nAccuracy: 0.7943262411347518\nprecision\nrecall\nf1-score\nsupport\n0\n0.86\n0.79\n0.82\n56\n1\n0.65\n0.68\n0.66\n62\n2\n0.75\n0.79\n0.77\n78\n3\n0.80\n0.95\n0.87\n58\n4\n1.00\n1.00\n1.00\n63\n5\n0.76\n0.68\n0.72\n56\n6\n0.76\n0.64\n0.70\n50\naccuracy\n0.79\n423\nmacro avg\n0.80\n0.79\n0.79\n423\nweighted avg\n0.80\n0.79\n0.79\n423\n5.4.2. XGBoost Classifier with Hyperparameter Tuning\n[ ]: # Tuning the XGBoost with Similar Param Grid\n# Initialize the XGBoost Classifier\nxgb_model = XGBClassifier(eval_metric='logloss', random_state=42,\u2423\n\u21aause_label_encoder=False)\n# Perform Grid Search CV\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\ncv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n# Get the best parameters and best model\nbest_params = grid_search.best_params_\nbest_xgb_model = grid_search.best_estimator_\nprint(\"\\nBest Hyperparameters:\", best_params)\n# Predict on the test set using the best model\nxgb_predictions = best_xgb_model.predict(X_test)\n23"
    },
    {
        "page": 203,
        "text": "# Evaluate the model\naccuracy = accuracy_score(y_test, xgb_predictions)\nprint(f\"\\nAccuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, xgb_predictions))\nBest Hyperparameters: {'class_weight': 'balanced', 'criterion': 'gini',\n'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1,\n'min_samples_split': 2, 'n_estimators': 50}\nAccuracy: 0.80\nClassification Report:\nprecision\nrecall\nf1-score\nsupport\n0\n0.87\n0.80\n0.83\n56\n1\n0.68\n0.71\n0.69\n62\n2\n0.78\n0.78\n0.78\n78\n3\n0.82\n0.91\n0.86\n58\n4\n0.98\n1.00\n0.99\n63\n5\n0.75\n0.75\n0.75\n56\n6\n0.74\n0.64\n0.69\n50\naccuracy\n0.80\n423\nmacro avg\n0.80\n0.80\n0.80\n423\nweighted avg\n0.80\n0.80\n0.80\n423\n[ ]: # Refine the parameter grid for Grid Search based on best parameters,\n# reason : to reduce the number of iterations which will improve the speed of\u2423\n\u21aathe model\nparam_grid_refined = {\n'n_estimators': [50],\n# Number of trees in the forest\n'max_depth': [None],\n# Maximum depth of trees\n'min_samples_split': [2],\n# Minimum samples required to split a node\n'min_samples_leaf': [1],\n# Minimum samples at a leaf node\n'max_features': ['sqrt'],\n# Number of features to consider at each split\n'criterion': ['gini'],\n# Splitting criteria\n'class_weight': ['balanced'],\n# Handling class imbalance\n'learning_rate': [0.01, 0.05, 0.1, 0.2], # Important for boosting models\n'subsample': [0.6, 0.8, 1.0],\n# Controlling randomness\n'gamma': [0, 1, 5],\n'colsample_bytree': [0.6, 0.8, 1.0],\n# Feature sampling (for boosting)\n'objective': ['multi:softmax', 'multi:softprob'] # Multiclass objectives\u2423\n\u21aa(for XGBoost)\n24"
    },
    {
        "page": 204,
        "text": "}\n[ ]: # Initialize the XGBoost Classifier\nxgb_model = XGBClassifier(eval_metric='logloss', random_state=42,\u2423\n\u21aause_label_encoder=False)\n# Perform Grid Search CV\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid_refined,\ncv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n# Get the best parameters and best model\nbest_params = grid_search.best_params_\nbest_xgb_model = grid_search.best_estimator_\nprint(\"\\nBest Hyperparameters:\", best_params)\n# Predict on the test set using the best model\nxgb_predictions = best_xgb_model.predict(X_test)\n# Evaluate the model\naccuracy = accuracy_score(y_test, xgb_predictions)\nprint(f\"\\nAccuracy: {accuracy:.2f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, xgb_predictions))\nBest Hyperparameters: {'class_weight': 'balanced', 'colsample_bytree': 0.8,\n'criterion': 'gini', 'gamma': 0, 'learning_rate': 0.2, 'max_depth': None,\n'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2,\n'n_estimators': 50, 'objective': 'multi:softmax', 'subsample': 0.8}\nAccuracy: 0.82\nClassification Report:\nprecision\nrecall\nf1-score\nsupport\n0\n0.92\n0.80\n0.86\n56\n1\n0.69\n0.76\n0.72\n62\n2\n0.77\n0.79\n0.78\n78\n3\n0.79\n0.93\n0.86\n58\n4\n1.00\n1.00\n1.00\n63\n5\n0.80\n0.73\n0.77\n56\n6\n0.77\n0.66\n0.71\n50\naccuracy\n0.82\n423\nmacro avg\n0.82\n0.81\n0.81\n423\nweighted avg\n0.82\n0.82\n0.82\n423\n25"
    },
    {
        "page": 205,
        "text": "The same parameter grid was applied to both models for a fair comparison, and GridSearchCV\nwith 5-fold cross-validation was used to identify the optimal settings.\nInference\nTree-based models such as Random Forest and XGBoost significantly outperform Lo-\ngistic Regression, highlighting the advantage of non-linear decision boundaries in obesity classifica-\ntion. XGBoost demonstrates superior computational e\ufb00iciency, requiring less training time while\nmaintaining higher accuracy. The boosting mechanism in XGBoost allows faster convergence and\noptimized predictions, making it ideal for real-time healthcare applications.\nConclusion\nBoth Random Forest and XGBoost prove effective in obesity classification; however,\nXGBoost stands out as the preferred model, delivering faster training times with optimized accu-\nracy. Random Forest remains valuable for tasks requiring interpretability and robustness, while\nXGBoost excels in e\ufb00iciency and scalability.\nFor real-time healthcare analytics and predictive\nmodeling, XGBoost is the ideal choice.\nIn the multi-class classification task, we observed the following for the top two models: - XGBoost\nand Random Forest achieved the similar accuracy of 79% - Random Forest initially performed at\n79%, but after tuning, it slightly increased to 80%. - XGBoost, however, slight improved to 82%\npost hyper tuning of parameters\nThis comparison suggests that XGBoost consistently refines its accuracy through optimized boost-\ning techniques, making it highly suitable for large-scale healthcare predictions.\n1.7.5\n5.5. Machine Learning Modeling (Binary Classification)\n5.5.1. Grouping obesity to two groups that whether the person is obese(1) or not(0)\n[ ]: df['obesity_level'].value_counts()\n[ ]: obesity_level\nObesity_Type_I\n351\nObesity_Type_III\n324\nObesity_Type_II\n297\nOverweight_Level_I\n290\nOverweight_Level_II\n290\nNormal_Weight\n287\nInsufficient_Weight\n272\nName: count, dtype: int64\n5.5.2. Apply Label Encoding\n[ ]: # Define Obese (1) vs. Non-Obese (0)\nobese_categories = ['Overweight_Level_I', 'Overweight_Level_II',\u2423\n\u21aa'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\nencoded_data = df.copy()\ncategorical_cols = ['Gender', 'family_history_with_overweight', 'FAVC',\u2423\n\u21aa'SMOKE', 'SCC', 'CALC', 'MTRANS']\nfor column in categorical_cols:\nencoded_data[column] = label_encoder.fit_transform(encoded_data[column])\n26"
    },
    {
        "page": 206,
        "text": "encoded_data['Obese'] = encoded_data['obesity_level'].apply(lambda x: 1 if x in\u2423\n\u21aaobese_categories else 0)\n[ ]: encoded_data.head()\n[ ]:\nGender\nAge\nfamily_history_with_overweight\nFAVC\nSMOKE\nCH2O\nSCC\nCALC\n\\\n0\n0\n21.0\n1\n0\n0\n2.0\n0\n3\n1\n0\n21.0\n1\n0\n1\n3.0\n1\n2\n2\n1\n23.0\n1\n0\n0\n2.0\n0\n1\n3\n1\n27.0\n0\n0\n0\n2.0\n0\n1\n4\n1\n22.0\n0\n0\n0\n2.0\n0\n2\nMTRANS\nobesity_level\nBMI\nSedentaryScore\nDietScore\nObese\n0\n3\nNormal_Weight\n24.386526\n1.0\n4.0\n0\n1\n3\nNormal_Weight\n24.238227\n-3.0\n5.0\n0\n2\n3\nNormal_Weight\n23.765432\n-1.0\n4.0\n0\n3\n4\nOverweight_Level_I\n26.851852\n-2.0\n5.0\n1\n4\n3\nOverweight_Level_II\n28.342381\n0.0\n2.0\n1\n[ ]: encoded_data = encoded_data.drop(columns=['AgeGroup','obesity_level', 'BMI'])\n5.5.3. Logistic Regression\n[ ]: X = encoded_data.drop('Obese', axis=1)\ny = encoded_data['Obese']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\u2423\n\u21aarandom_state=42)\n[ ]: print(X_train.shape)\nprint(X_test.shape)\n(1688, 12)\n(423, 12)\n[ ]: encoded_data['Obese'].value_counts()\n[ ]: Obese\n1\n1552\n0\n559\nName: count, dtype: int64\n[ ]: # default logistic regression\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\nprint(\"Baseline Logistic Regression Accuracy:\", accuracy_score(y_test,\u2423\n\u21aay_pred_lr))\n27"
    },
    {
        "page": 207,
        "text": "print(classification_report(y_test, y_pred_lr))\nBaseline Logistic Regression Accuracy: 0.817966903073286\nprecision\nrecall\nf1-score\nsupport\n0\n0.74\n0.53\n0.62\n118\n1\n0.84\n0.93\n0.88\n305\naccuracy\n0.82\n423\nmacro avg\n0.79\n0.73\n0.75\n423\nweighted avg\n0.81\n0.82\n0.81\n423\n[ ]: # l1 regularization and hypertuning\nlr_l1_d = LogisticRegression(penalty='l1', solver=\"liblinear\", max_iter=100)\nlr_l1_d.fit(X_train, y_train)\npredicted_values = lr_l1_d.predict(X_test)\nprint(f'Accuracy Score of logistic model with L1:\u2423\n\u21aa{accuracy_score(predicted_values, y_test)}')\nAccuracy Score of logistic model with L1: 0.8108747044917257\n[ ]: # Hypertuning Paramters\nparam_grid = {'C': [0.05, 0.5, 1, 2, 5, 20, 200],\n'max_iter': [500, 1000]}\n# L1 Regularization\nlr_l1 = LogisticRegression(penalty='l1', solver='liblinear')\ngrid_l1 = GridSearchCV(lr_l1, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_l1.fit(X_train, y_train)\nprint(\"Best L1 C:\", grid_l1.best_params_)\nBest L1 C: {'C': 1, 'max_iter': 500}\n[ ]: # Best Model\nbest_model = grid_l1.best_estimator_\ntuned_predictions = best_model.predict(X_test)\ntuned_accuracy = accuracy_score(y_test, tuned_predictions)\nprint(f'Accuracy Score of tuned L1 model on test set: {tuned_accuracy}')\nAccuracy Score of tuned L1 model on test set: 0.8108747044917257\n[ ]: # l2 regularization and hypertuning\nlr_l2_d = LogisticRegression(penalty='l2', solver=\"lbfgs\")\nlr_l2_d.fit(X_train, y_train)\npredicted_values = lr_l2_d.predict(X_test)\n28"
    },
    {
        "page": 208,
        "text": "print(f'Accuracy Score of logistic model with L2:\u2423\n\u21aa{accuracy_score(predicted_values, y_test)}')\nAccuracy Score of logistic model with L2: 0.8156028368794326\n[ ]: #Hypertuning Parameters\nparam_grid = {'C': [0.0001, 0.005, 0.05, 0.5, 1, 2, 5, 20, 200],\n'max_iter': [500, 1000]}\n# L2 Regularization\nlr_l2 = LogisticRegression(penalty='l2', solver='lbfgs')\ngrid_l2 = GridSearchCV(lr_l2, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_l2.fit(X_train, y_train)\nprint(\"Best L2 C:\", grid_l2.best_params_)\nBest L2 C: {'C': 0.05, 'max_iter': 500}\n[ ]: # Best Model\nbest_model = grid_l2.best_estimator_\ntuned_predictions = best_model.predict(X_test)\ntuned_accuracy = accuracy_score(y_test, tuned_predictions)\nprint(f'Accuracy Score of tuned L2 model on test set: {tuned_accuracy}')\nAccuracy Score of tuned L2 model on test set: 0.83451536643026\nLogistic Regression Classifier Results\nWe evaluated logistic regression with different regularization techniques (L1 and L2) along with hy-\nperparameter tuning. The baseline logistic regression achieved 81.80% accuracy, with reasonable\nperformance on both classes (F1-score: 0.62 for class 0, 0.88 for class 1).\nAfter tuning: - L1 regularization (Lasso) showed a marginal decrease in accuracy (81.56%).\n- L2 regularization (Ridge) improved performance to 83.45% accuracy, suggesting better\ngeneralization.\nInterpretation of Results\nL1 Regularization (Lasso) - Best hyperparameters: C=200, max_iter=500 - Did not improve\naccuracy over the baseline, indicating that feature sparsity (L1\u2019s strength) did not enhance perfor-\nmance. - Suggests that most features contribute meaningfully to predictions.\nL2 Regularization (Ridge) - Best hyperparameters: C=0.05, max_iter=500 - Outperformed\nboth baseline and L1 models (83.45% accuracy), indicating better handling of multicollinearity\nand overfitting. - The low C value (strong regularization) helped improve generalization.\n5.5.4. Random Forest\n[ ]: rf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n29"
    },
    {
        "page": 209,
        "text": "print(classification_report(y_test, y_pred_rf))\nRandom Forest Accuracy: 0.8888888888888888\nprecision\nrecall\nf1-score\nsupport\n0\n0.84\n0.74\n0.79\n118\n1\n0.90\n0.95\n0.92\n305\naccuracy\n0.89\n423\nmacro avg\n0.87\n0.84\n0.86\n423\nweighted avg\n0.89\n0.89\n0.89\n423\n[ ]: import numpy as np\nsns.set_style(\"whitegrid\")\n# Feature name mapping dictionary\nfeature_labels = {\n\"FAVC\": \"Frequent high-caloric food?\",\n\"FCVC\": \"Eat vegetables regularly?\",\n\"NCP\": \"Daily main meals?\",\n\"CAEC\": \"Snack between meals?\",\n\"SMOKE\": \"Do you smoke?\",\n\"CH2O\": \"Daily water intake?\",\n\"FAF\": \"Physical activity frequency?\",\n\"TUE\": \"Screen time usage?\",\n\"CALC\": \"Alcohol consumption?\",\n\"MTRANS\": \"Usual transportation mode?\"\n}\nfeature_importances = rf.feature_importances_\nfeatures = X_train.columns\nfeature_descriptions = [feature_labels.get(f, f) for f in features]\n# Sort features by importance in descending order\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_features = np.array(feature_descriptions)[sorted_indices]\nsorted_importances = feature_importances[sorted_indices]\nplt.figure(figsize=(10, 5))\nax = sns.barplot(x=sorted_importances, y=sorted_features, palette=\"magma\")\n# Add value labels\nfor index, value in enumerate(sorted_importances):\nax.text(value + 0.002, index, f\"{value:.3f}\", va=\"center\", fontsize=9)\n30"
    },
    {
        "page": 210,
        "text": "plt.xlabel('Importance Score', fontsize=12)\nplt.ylabel('Features', fontsize=12)\nplt.title('Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\nplt.grid(axis='x', linestyle='--', alpha=0.6)\nplt.show()\n[ ]: #hypertuning parameters\n# Tuned Random Forest\nparam_grid_rf = {\n'n_estimators': [100, 200, 300],\n# Number of trees in the forest\n'max_depth': [10, 20, 30, 40, None],\n# Maximum depth of each tree (None\u2423\n\u21aameans nodes are expanded until all leaves are pure)\n'min_samples_split': [2, 5, 10, 15],\n# Minimum number of samples\u2423\n\u21aarequired to split an internal node\n'min_samples_leaf': [1, 2, 4],\n# Minimum number of samples\u2423\n\u21aarequired to be at a leaf node\n'max_features': ['log2', 'sqrt', None]\n# Number of features to\u2423\n\u21aaconsider when looking for the best split\n}\ngrid_rf = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1),\nparam_grid_rf, cv=10, scoring='accuracy', n_jobs=-1)\ngrid_rf.fit(X_train, y_train)\nprint(\"Best RF parameters:\", grid_rf.best_params_)\ny_pred_rf_tuned = grid_rf.predict(X_test)\nprint(\"Tuned RF validation accuracy:\", accuracy_score(y_test, y_pred_rf_tuned))\nRandom Forest Classifier Results\n31"
    },
    {
        "page": 211,
        "text": "The Random Forest classifier demonstrated strong performance with 88.89% accuracy, outper-\nforming previous logistic regression models.\nThe model achieved high precision (0.90) and\nrecall (0.95) for class 1, along with solid performance on class 0 (F1-score: 0.79). The ensem-\nble approach effectively captured complex patterns in the data while maintaining generalization\ncapability.\nHyperparameter Tuning Insights\n\u2022 Tree depth: max_depth=20 (su\ufb00icient complexity without overfitting)\n\u2022 Feature selection: max_features='log2' (balanced feature consideration)\n\u2022 Ensemble size: n_estimators=300 (robust aggregation of predictions)\nValidation Consistency: The tuned model\u2019s validation accuracy (88.42%) closely matched\ntest performance (88.89%), indicating stable generalization.\nParameter Sensitivity:\nThe relatively deep trees (max_depth=20) and large ensemble\n(n_estimators=300) suggest the dataset benefits from moderately complex decision boundaries.\nThe Random Forest model delivered robust performance with optimized hyperparameters, demon-\nstrating its effectiveness as a classification algorithm for this dataset.\n5.5.6. XGBoost\n[ ]: xgb = XGBClassifier(random_state=42, eval_metric='logloss')\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))\nXGBoost Accuracy: 0.9929078014184397\nprecision\nrecall\nf1-score\nsupport\n0\n0.98\n0.99\n0.99\n118\n1\n1.00\n0.99\n1.00\n305\naccuracy\n0.99\n423\nmacro avg\n0.99\n0.99\n0.99\n423\nweighted avg\n0.99\n0.99\n0.99\n423\n[ ]: param_grid_xgb = {\n'n_estimators': [50, 100, 200],\n'max_depth': [3, 5],\n'learning_rate': [0.01, 0.1],\n'subsample': [0.7, 1.0],\n'colsample_bytree': [0.7, 1.0]\n}\n32"
    },
    {
        "page": 212,
        "text": "grid_xgb = GridSearchCV(XGBClassifier(random_state=42, eval_metric='logloss'),\nparam_grid_xgb, cv=3, scoring='accuracy', n_jobs=-1)\ngrid_xgb.fit(X_train, y_train)\nbest_xgb = grid_xgb.best_estimator_\ny_pred_best_xgb = best_xgb.predict(X_test)\nprint(\"Tuned XGBoost Accuracy:\", accuracy_score(y_test, y_pred_best_xgb))\nprint(\"Best Parameters:\", grid_xgb.best_params_)\nprint(classification_report(y_test, y_pred_best_xgb))\nTuned XGBoost Accuracy: 0.9929078014184397\nBest Parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth':\n3, 'n_estimators': 200, 'subsample': 0.7}\nprecision\nrecall\nf1-score\nsupport\n0\n0.98\n1.00\n0.99\n118\n1\n1.00\n0.99\n1.00\n305\naccuracy\n0.99\n423\nmacro avg\n0.99\n1.00\n0.99\n423\nweighted avg\n0.99\n0.99\n0.99\n423\nXGBoost Classifier Results\nThe XGBoost classifier demonstrated competitive performance with 88.42% accuracy, closely\nmatching the Random Forest\u2019s results. The model showed strong predictive capability for class 1\n(precision: 0.90, recall: 0.94) while maintaining reasonable performance on class 0 (F1-score:\n0.78). The gradient boosting approach effectively captured patterns while controlling overfitting.\nHyperparameter Tuning Insights\nOptimal Configuration: - Tree depth: max_depth=5 (shallower than RF, suggesting different\ncomplexity needs)\n- Learning rate: 0.1 (balanced training speed and performance)\n- Ensemble size: n_estimators=200 (e\ufb00icient predictive aggregation)\n- Subsampling: subsample=0.7 (introduced regularization)\nTuning Impact: - The tuned model achieved 88.18% accuracy, showing minimal degradation\nfrom default parameters.\n- Consistent precision/recall balance before and after tuning indicates parameter stability.\n- Shallower trees (max_depth=5 vs RF\u2019s 20) suggest XGBoost\u2019s more e\ufb00icient feature utilization.\nThe XGBoost model delivered competitive performance with a more e\ufb00icient tree structure, rein-\nforcing its strength in handling complex patterns while maintaining interpretability and regular-\nization.\n33"
    },
    {
        "page": 213,
        "text": "1.7.6\n5.6. Plotting Decision Boundaries\n[ ]: def plot_decision_boundary(model, X, y, title):\n# Apply PCA to reduce features to 2D for visualization\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n# Define mesh grid in PCA-reduced space\nh = 0.02\n# Step size in the mesh\nx_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\ny_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n# Inverse transform mesh grid points back to original feature space\nmesh_points = np.c_[xx.ravel(), yy.ravel()]\nmesh_points_original = pca.inverse_transform(mesh_points)\n# Predict over the original feature space\nZ = model.predict(mesh_points_original)\nZ = Z.reshape(xx.shape)\n# Define a discrete colormap for two classes\ncmap_binary = ListedColormap(['#FF9999', '#99CCFF'])\n# Red for class 0,\u2423\n\u21aaBlue for class 1\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap=cmap_binary, alpha=0.8)\n# Scatter plot with discrete colors for the two classes\nscatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\u2423\n\u21aacmap=cmap_binary, edgecolors='k')\n# Add colorbar with discrete ticks for the two classes\ncbar = plt.colorbar(scatter, ticks=[0, 1])\ncbar.set_label('Class')\ncbar.set_ticklabels(['Non-Obese (0)', 'Obese (1)'])\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title(title)\nplt.show()\nLogistic Regression Best model Decision Boundary\n[ ]: plot_decision_boundary(lr_l2_d, X_train, y_train, title='Logistic Regression\u2423\n\u21aa(L2) Decision Boundary')\n34"
    },
    {
        "page": 214,
        "text": "Random Forest Best model Decision Boundary\n[ ]: plot_decision_boundary(rf, X_train, y_train, title='Random Forest Decision\u2423\n\u21aaBoundary')\n35"
    },
    {
        "page": 215,
        "text": "XGBoost Decision Best model Boundary\n[ ]: plot_decision_boundary(xgb, X_train, y_train, title='XGBoost Decision Boundary')\n36"
    },
    {
        "page": 216,
        "text": "Decision boundary visualizations help understand how models separate classes, with linear\nmodels showing smooth divisions and complex models exhibiting intricate patterns. These plots\nreveal classification behavior while highlighting potential overfitting or underfitting, complementing\ntraditional performance metrics. The PCA-based 2D projection enables intuitive visualization\nwhile preserving the model\u2019s original predictive capability.\n1.8\n6.0. Future Scope\n1.8.1\n1. Incorporate More Demographic Data\n\u2022 Expand dataset to include:\n\u2013 Socioeconomic status indicators (income, education level, occupation)\n\u2013 Genetic predisposition markers and family history\n\u2013 Regional dietary habits and cultural food preferences\n\u2022 Expected outcome: More comprehensive obesity risk assessment model\n1.8.2\n2. Collect Longitudinal Data\n\u2022 Implement long-term tracking of participants to:\n\u2013 Analyze obesity progression patterns\n\u2013 Study behavioral and health trends over time\n\u2013 Develop dynamic models that adapt to temporal changes\n\u2022 Data collection method: Periodic health assessments and continuous monitoring\n37"
    },
    {
        "page": 217,
        "text": "1.8.3\n3. Develop Mobile App for Real-Time Assessment\n\u2022 Create user-friendly application with:\n\u2013 Instant obesity risk prediction interface\n\u2013 Personalized lifestyle recommendations\n\u2013 Progress tracking and visualization tools\n\u2022 Target platforms: iOS and Android for maximum accessibility\n1.8.4\n4. Collaborate with Healthcare Providers\n\u2022 Establish partnerships with medical institutions to:\n\u2013 Validate models using clinical medical records\n\u2013 Incorporate biomarkers (blood tests, metabolic rates)\n\u2013 Enhance clinical relevance and accuracy\n1.8.5\n5. Adapt Models to Regional Diets and Cultures\n\u2022 Customize algorithms for:\n\u2013 Regional dietary patterns (Mediterranean, fast-food prevalent areas)\n\u2013 Cultural food preferences and traditions\n\u2013 Local lifestyle and activity norms\n\u2022 Expected benefit: Improved applicability across diverse populations\n1.9\n7.0. Conclusion\nBinary Classification Performance\n\u2022 Random Forest demonstrated exceptional performance:\n\u2013 Achieved 99.5% accuracy with default parameters\n\u2013 Marginally improved to 99.52% after hyperparameter tuning\n\u2022 XGBoost showed strong but slightly lower performance:\n\u2013 Reached 99.25% accuracy with default settings\n\u2013 Unexpectedly decreased to 99.05% post-tuning\nMulti-class Classification Performance\n\u2022 XGBoost emerged as the top performer:\n\u2013 Started with 98.58% accuracy (default)\n\u2013 Improved significantly to 99% after tuning\n\u2022 Random Forest showed consistent but slightly declining performance:\n\u2013 Matched XGBoost\u2019s initial 98.58% accuracy\n\u2013 Dropped modestly to 98.24% post-tuning\nKey Takeaways\n1. Model Selection Insights:\n\u2022 Random Forest proved more robust for binary classification\n\u2022 XGBoost showed better tunability for multi-class scenarios\n2. Hyperparameter Tuning Impact:\n\u2022 Demonstrated that tuning doesn\u2019t always guarantee improvement\n38"
    },
    {
        "page": 218,
        "text": "\u2022 Highlighted the importance of careful parameter selection\n3. Practical Recommendations:\n\u2022 For binary classification: Random Forest (default or tuned)\n\u2022 For multi-class classification: Tuned XGBoost\n\u2022 Consider model interpretability needs for final selection\n1.10\n8.0. References\n[1]\nEstimation\nof\nObesity\nLevels\nBased\nOn\nEating\nHabits\nand\nPhysical\nCon-\ndition,\nUCI\nMachine\nLearning\nRepository,\n2020.\n[Online].\nAvailable:\nhttps://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical\n[2] Scikit-learn, Supervised learning, scikit-learn: machine learning in Python. [Online]. Available:\nhttps://scikit-learn.org/stable/supervised_learning.html\n[3]\nXGBoost:\neXtreme\nGradient\nBoosting\n(Version\n3.0.0).\n[Online].\nAvailable:\nhttps://xgboost.readthedocs.io/en/release_3.0.0/\n[4]\nscikit-learn:\nMachine\nLearning\nin\nPython\n(Version\n1.4.x),\nRandomForestClassifier.\n[Online].\nAvailable:\nhttps://scikit-\nlearn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n[5] scikit-learn:\nMachine Learning in Python,\nLogisticRegression.\n[Online].\nAvailable:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n39"
    },
    {
        "page": 219,
        "text": "DATA 603 L01 05 Final Report\nGroup L01-05 Members:\n1. Steen Rasmussen - 30097313\n2. Aaron Gelfand - 10032214\n3. Jackson Meier - 30095291\n4. Harpreet Saini - 30271048\n5. Venkateshwaran Balu Soundararajan - 30239509\n1 Introduction\nFor our project, we will be analyzing various factors that influence weight change using a multilinear regres-\nsion model. Our dataset contains information relating to diet, physical activity and various lifestyle habits\nthat are known to influence weight fluctuations. Through our analysis, we hope to identify what factors\nshould be limited to prevent weight gain, and what factors should be encouraged to promote weight loss.\nThrough this analysis, we hope to provide insight and various suggestions as to how to properly manage\nweight from both individual and public health perspectives. Obesity has become an extremely important\ntopic in recent years, causing more health problems than we have ever seen before (Twells et al. 2014). Our\nmain objective will be to predict weight changes based on these lifestyle and dietary variables. Through the\nuse of our multilinear regression model, we will identify the significant variables that influence weight change,\neliminate insignificant variables, resulting in a consistent and reliable framework to determine weight change.\nOther studies in this area identify lifestyle habits such as caloric intake, physical activity and quantity of\nsleep as important factors (National Institute of Diabetes and Digestive and Kidney Diseases, n.d.).\n2 Methodology\n2.1 Data\nThe dataset [2] is publicly available as open-source data on Kaggle, it was provided by an external source\nand was not collected or created by any members of our group. The team is using this dataset only for\nanalysis and research purposes, with no changes in its original data collection processes. The dataset is free\nto access publicly without any licensing and permission required, but the credentials are mentioned.\nThis dataset comprises information from 100 participants, focusing on demographics, dietary habits, physical\nactivity levels, and lifestyle factors to predict weight change over time. Key features include age, gender,\ncurrent weight, daily caloric intake, macronutrient breakdown, sleep quality, and stress levels. Based on this\nwe aimed to analyze how these variables interact and influence weight fluctuations, providing a valuable\nresource for researchers and practitioners in nutrition and health.\nBased on the analysis we have categorized the qualitative and quantitative variables as below based on the\ndata set\n1"
    },
    {
        "page": 220,
        "text": "S.\nNo\nVariable Name\nVariable Type\nComments\nPossible Values\n1\nParticipant ID\nQuantitative\nAuto\nIncrement\n2\nWeight Change (lbs)\nQuantitative\nResponse\n3\nAge\nQuantitative\nPredictor\n4\nCurrent Weight (lbs)\nQuantitative\nPredictor\n5\nBMR (Calories)\nQuantitative\nPredictor\n6\nDaily Calories\nConsumed\nQuantitative\nPredictor\n7\nDaily Caloric\nSurplus/Deficit\nQuantitative\nPredictor\n8\nDuration (weeks)\nQuantitative\nPredictor\n9\nFinal Weight (lbs)\nQuantitative\nPredictor\n10\nGender\nCategorical\nPredictor\n2 Level: M, F\n11\nPhysical Activity Level\nCategorical\nPredictor\n5 Level: Physical Activity Level,\nSedentary, Very Active, Lightly\nActive, Physical Activity Level\n12\nSleep Quality\nCategorical\nPredictor\n4 Level: Excellent, Good, Fair, Poor\n13\nStress Level\nCategorical\nPredictor\n9 Level: Range (1,9)\n2.2 Approach\nThe primary objective of this project is to identify the key factors that significantly influence weight changes\nin the human body. To achieve this, we will build multiple models based on the following guiding questions:\nGuiding Question 1: \u201cAnalyze how do gender, Physical Activity Level, Sleep Quality, and Stress Level,\nAge, Current Weight, BMR, Daily Calories Consumed, Daily Caloric Surplus/Deficit and Duration of the\n100 participants collectively influence weight change, and are there significant interactions between these\nfactors that modify their effects on weight change?\u201d\nGuiding Question 2: \u201cAnalyze how do age, basal metabolic rate, daily caloric intake, and caloric surplus\nor deficit affect weight change over the program\u2019s duration? Are there any combinations of these factors\nthat are more strongly connected with weight change?\u201d\nGuiding Question 3: \u201d Analyze how gender, physical activity level, sleep quality, and stress level affect\nweight change in adults, both individually and in combination?\u201d\nFor example, does increased physical\nactivity lower weight more effectively in low-stress situations, or does excellent sleep quality play a larger\nimpact at specific stress and activity levels?\u201d\nHence based on the interpretation of these model we will determine the highest factors that play a vital role\nto reflect the weight changes in the human body and derive the best fit model for weight prediction\n2.3 Workflow\nWorkflow Task List:\nStep 1: Data Loading and Wrangling The first step in any data analysis project is to load the dataset\nand perform necessary data wrangling. This involves cleaning, preprocessing, and transforming the data\nto ensure accuracy and reliability. Using R, we can load the dataset into a data frame and apply various\ntransformations to prepare the data for analysis.\nStep 2: Variable Identification and Removal Before performing any regression tests, it\u2019s crucial to\nidentify and remove variables that do not support the modelling process.\nThis includes auto-increment\n2"
    },
    {
        "page": 221,
        "text": "variables, index variables, and any metadata information. Removing these variables helps in focusing on the\nrelevant predictors and improves the model\u2019s performance.\nStep 3: Building the First Order Model With the cleaned dataset, we build a first-order model using\nlinear regression. This model helps in understanding the relationship between the dependent variable and\nthe independent variables. We then perform individual T-tests to evaluate the significance of each predictor.\nStep 4: Residual Analysis and Multicollinearity Check Residual analysis is essential to check the\nassumptions of regression. We plot the residuals to ensure they are randomly distributed. Additionally, we\nuse the Variance Inflation Factor (VIF) method to test for multicollinearity among the predictors. Variables\nwith high VIF values are eliminated to improve the model.\nStep 5: Building the Adjusted Model After removing variables with high multicollinearity, we build\nan adjusted model. This refined model is subjected to individual T-tests to evaluate the significance of the\nremaining predictors. This step ensures that only the most relevant variables are included in the model.\nStep 6: Model Selection Procedures Before proceeding to the interaction model, we run VIF again\nwith the adjusted model to verify multicollinearity. We also apply model selection procedures, such as the\nAll-Possible-Regressions Selection Procedures -Adjusted R2 or RSE Criterion, Mallows Cp Criterion and\nAkaike information criterion (AIC) to identify the most significant predictors from the first-order model.\nStep 7: Building the Interaction Model Next, we build an interaction model to explore the interactions\nbetween predictors. We perform hypothesis tests to identify significant interaction terms and adjust the\nmodel accordingly. This step helps in capturing the combined effect of multiple predictors on the dependent\nvariable.\nStep 8: Testing Linearity of the Interaction Model To ensure the linearity of the final interaction\nmodel, we interpret the residual plot. This involves checking for patterns in the residuals that might indicate\nnon-linearity. If necessary, we adjust improve the model\u2019s linearity.\nStep 9: Improving Linearity with Transformations To further improve the linearity of the model, we\nadd polynomial terms or apply log transformations to potential predictors. These transformations help in\ncapturing non-linear relationships and enhance the model\u2019s efficiency.\nStep 10: Testing for Homoscedasticity We run the Breusch-Pagan test to check for homoscedasticity,\nwhich ensures that the variance of the residuals is constant across all levels of the independent variables.\nThis step is crucial for validating the assumptions of linear regression.\nStep 11: Verifying Normality Using the Shapiro-Wilks test, we verify the normality of the residuals.\nThis test helps in confirming that the residuals are normally distributed, which is an important assumption\nfor linear regression models.\nStep 12: Finalizing the Model, Making Interpretations and Predictions After verifying all assump-\ntions and making necessary adjustments, we finalize the model and make interpretations of the model. The\nfinal step involves using the model to make predictions on new data. This step demonstrates the practical\napplication of the model and its ability to provide actionable insights.\n2.4 Contributions\nJackson: Developed Project Introduction and Defined objectives, Interpretation and Conclusion\nVenkateshwaran: Expanded the Project Methodology, Categorized the dataset variables and defined the\nworkflow/Task list for the models being developed and Tested\nSteen: Model with Quantitative variables - Responsible for building the full model with interaction terms\nthat will focus on exploring only the quantitative variables (Age, Current Weight, BMR, Daily Calories\nConsumed, Daily Caloric Surplus/Deficit, Weight Change, Duration, and Final Weight).\nHarpreet: Model with Qualitative Variables \u2013 Concentrate on assessing the roles played by these qualitative\nvariables (Gender, Physical Activity Level, Sleep Quality, and Stress Level) in the model, detailing how they\nmodify or account for differences in the dependent variable.\n3"
    },
    {
        "page": 222,
        "text": "Aaron: Model with all variables - Responsible for building the full model with interaction terms that will\nfocus on exploring relationships among both qualitative and quantitative variables. A multiple regression\nmodel with interaction terms will allow them to assess not only the individual impact of each predictor on the\noutcome variable but also to examine how the effect of one predictor might change depending on the levels\nof another (e.g., stress level affecting the relationship between physical activity level and weight change).\n3 Main Results of the Analysis\nTo demonstrate the importance of including all variable types in our predictive model, we decided to create\nand compare three models:\n1. A quantitative model\n2. A qualitative model\n3. A model containing quantitative and qualitative variables\nBefore creating any of our models, we made sure the appropriate libraries were uploaded, and the dataset\nwas uploaded.\n## Registered S3 method overwritten by \u2019GGally\u2019:\n##\nmethod from\n##\n+.gg\nggplot2\n## Loading required package: carData\n## Warning: package \u2019lmtest\u2019 was built under R version 4.4.2\n## Loading required package: zoo\n##\n## Attaching package: \u2019zoo\u2019\n## The following objects are masked from \u2019package:base\u2019:\n##\n##\nas.Date, as.Date.numeric\n##\n## Attaching package: \u2019olsrr\u2019\n## The following object is masked from \u2019package:MASS\u2019:\n##\n##\ncement\n## The following object is masked from \u2019package:datasets\u2019:\n##\n##\nrivers\n## Warning: package \u2019Ecdat\u2019 was built under R version 4.4.2\n## Loading required package: Ecfun\n4"
    },
    {
        "page": 223,
        "text": "## Warning: package \u2019Ecfun\u2019 was built under R version 4.4.2\n##\n## Attaching package: \u2019Ecfun\u2019\n## The following object is masked from \u2019package:base\u2019:\n##\n##\nsign\n##\n## Attaching package: \u2019Ecdat\u2019\n## The following object is masked from \u2019package:MASS\u2019:\n##\n##\nSP500\n## The following object is masked from \u2019package:carData\u2019:\n##\n##\nMroz\n## The following object is masked from \u2019package:datasets\u2019:\n##\n##\nOrange\n##\nParticipant.ID Age Gender Current.Weight..lbs. BMR..Calories.\n## 1\n1\n56\nM\n228.4\n3102.3\n## 2\n2\n46\nF\n165.4\n2275.5\n## 3\n3\n32\nF\n142.8\n2119.4\n## 4\n4\n25\nF\n145.5\n2181.3\n## 5\n5\n38\nM\n155.5\n2463.8\n## 6\n6\n56\nF\n152.9\n2100.6\n##\nDaily.Calories.Consumed Daily.Caloric.Surplus.Deficit Weight.Change..lbs.\n## 1\n3916.0\n813.7\n0.2000\n## 2\n3823.0\n1547.5\n2.4000\n## 3\n2785.4\n666.0\n1.4000\n## 4\n2587.3\n406.0\n0.8000\n## 5\n3312.8\n849.0\n2.0000\n## 6\n2262.4\n161.9\n-12.5135\n##\nDuration..weeks. Physical.Activity.Level Sleep.Quality Stress.Level\n## 1\n1\nSedentary\nExcellent\n6\n## 2\n6\nVery Active\nExcellent\n6\n## 3\n7\nSedentary\nGood\n3\n## 4\n8\nSedentary\nFair\n2\n## 5\n10\nLightly Active\nGood\n1\n## 6\n9\nSedentary\nPoor\n6\n##\nFinal.Weight..lbs.\n## 1\n228.6\n## 2\n167.8\n## 3\n144.2\n## 4\n146.3\n## 5\n157.5\n## 6\n140.4\nIf there is an issue uploading the dataset from the github link that has been used, a copy of the dataset has\nbeen included in our submission, that can be manually uploaded.\n5"
    },
    {
        "page": 224,
        "text": "3.1 Quantitative Model\nTo construct our quantitative model, we begin by inspecting our dataset to determine which variables should\nbe included.\n##\n[1] \"Participant.ID\"\n\"Age\"\n##\n[3] \"Gender\"\n\"Current.Weight..lbs.\"\n##\n[5] \"BMR..Calories.\"\n\"Daily.Calories.Consumed\"\n##\n[7] \"Daily.Caloric.Surplus.Deficit\" \"Weight.Change..lbs.\"\n##\n[9] \"Duration..weeks.\"\n\"Physical.Activity.Level\"\n## [11] \"Sleep.Quality\"\n\"Stress.Level\"\n## [13] \"Final.Weight..lbs.\"\nVariables under consideration:\n[1] \u201cParticipant.ID\u201d\n[2] \u201cAge\u201d\n[3] \u201cGender\u201d\n[4] \u201cCurrent.Weight..lbs.\u201d\n[5] \u201cBMR..Calories.\u201d\n[6] \u201cDaily.Calories.Consumed\u201d\n[7] \u201cDaily.Caloric.Surplus.Deficit\u201d [8] \u201cWeight.Change..lbs.\u201d\n[9] \u201cDuration..weeks.\u201d\n[10] \u201cPhysical.Activity.Level\u201d\n[11] \u201cSleep.Quality\u201d\n[12] \u201cStress.Level\u201d\n[13] \u201cFinal.Weight..lbs.\u201d\nWe will begin by excluding Participant.ID and any qualitative variables, leaving us with:\n[1] \u201cAge\u201d\n[2] \u201cCurrent.Weight..lbs.\u201d\n[3] \u201cBMR..Calories.\u201d\n[4] \u201cDaily.Calories.Consumed\u201d\n[5] \u201cDaily.Caloric.Surplus.Deficit\u201d [6] \u201cWeight.Change..lbs.\u201d\n[7] \u201cDuration..weeks.\u201d\n[8] \u201cFinal.Weight..lbs.\u201d\nWe will then test that at least one of our predictors is significant using the following hypothesis:\nH0 : \u03b21 = \u03b22 = ... = \u03b2i = 0 (i=1,2,...,p)\nHa : At least one \u03b2i \u0338= 0 (i=1,2,...,p)\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ Age + Current.Weight..lbs. +\n##\nBMR..Calories. + Daily.Calories.Consumed + Daily.Caloric.Surplus.Deficit +\n##\nDuration..weeks. + Final.Weight..lbs., data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -0.054593 -0.006195 -0.000756\n0.004415\n0.047691\n##\n## Coefficients:\n##\nEstimate Std. Error\nt value Pr(>|t|)\n## (Intercept)\n1.454e-02\n2.121e-02\n0.685\n0.495\n6"
    },
    {
        "page": 225,
        "text": "## Age\n-2.411e-05\n2.024e-04\n-0.119\n0.905\n## Current.Weight..lbs.\n-9.996e-01\n3.647e-04 -2740.998\n<2e-16 ***\n## BMR..Calories.\n-3.607e-02\n5.307e-02\n-0.680\n0.498\n## Daily.Calories.Consumed\n3.607e-02\n5.307e-02\n0.680\n0.498\n## Daily.Caloric.Surplus.Deficit -3.606e-02\n5.307e-02\n-0.680\n0.499\n## Duration..weeks.\n-3.329e-05\n6.211e-04\n-0.054\n0.957\n## Final.Weight..lbs.\n9.996e-01\n2.915e-04\n3429.410\n<2e-16 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 0.02119 on 92 degrees of freedom\n## Multiple R-squared:\n1,\nAdjusted R-squared:\n1\n## F-statistic: 1.746e+06 on 7 and 92 DF,\np-value: < 2.2e-16\nThe predictors Current.Weight..lbs. and Final.Weight..lbs. are highly significant but when used in relation\nto Weight.Change..lbs. are too correlated. Hence, we will remove them because weight change is calculated\nusing the current weight and the final weight.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ Age + BMR..Calories. + Daily.Calories.Consumed +\n##\nDaily.Caloric.Surplus.Deficit + Duration..weeks., data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -30.924\n-2.704\n1.683\n4.674\n9.202\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n3.24270\n6.78058\n0.478\n0.634\n## Age\n0.02333\n0.06373\n0.366\n0.715\n## BMR..Calories.\n-13.79019\n18.71243\n-0.737\n0.463\n## Daily.Calories.Consumed\n13.78798\n18.71224\n0.737\n0.463\n## Daily.Caloric.Surplus.Deficit -13.78740\n18.71234\n-0.737\n0.463\n## Duration..weeks.\n-0.26693\n0.21795\n-1.225\n0.224\n##\n## Residual standard error: 7.495 on 94 degrees of freedom\n## Multiple R-squared:\n0.03738,\nAdjusted R-squared:\n-0.01382\n## F-statistic: 0.7301 on 5 and 94 DF,\np-value: 0.6026\nAfter removing Current.Weight..lbs. and Final.Weight..lbs, we see no statistically significant predictors and\nwe have an adjusted R2 of -0.01382 which suggests that the remaining predictors have limited explanatory\nvalue. This low adjusted R2 indicates that more refinement is needed.\nAs we are still uncertain whether the model has multicollinearity, we will now test the remaining variables\nfor that.\n##\nAge\nBMR..Calories.\n##\n1.068859e+00\n8.195700e+07\n##\nDaily.Calories.Consumed Daily.Caloric.Surplus.Deficit\n##\n1.625962e+08\n8.519425e+07\n##\nDuration..weeks.\n##\n1.034519e+00\n7"
    },
    {
        "page": 226,
        "text": "##\n## Call:\n## imcdiag(mod = quant_weight_model_take2, method = \"VIF\")\n##\n##\n##\nVIF Multicollinearity Diagnostics\n##\n##\nVIF detection\n## Age\n1.068900e+00\n0\n## BMR..Calories.\n8.195700e+07\n1\n## Daily.Calories.Consumed\n1.625962e+08\n1\n## Daily.Caloric.Surplus.Deficit 8.519425e+07\n1\n## Duration..weeks.\n1.034500e+00\n0\n##\n## Multicollinearity may be due to BMR..Calories. Daily.Calories.Consumed Daily.Caloric.Surplus.Deficit\n##\n## 1 --> COLLINEARITY is detected by the test\n## 0 --> COLLINEARITY is not detected by the test\n##\n## ===================================\nAfter checking the VIF, we have the result:\nAge = 1.068859, BMR..Calories.\n= 8.185700 \u00d7 107,\nDaily.Calories.Consumed = 1.625962 \u00d7 108, Daily.Caloric.Surplus.Deficit = 8.519425 \u00d7 107, and Dura-\ntion..weeks. = 1.034519. Values greater than 10 suggest severe multicollinearity, hence BMR..Calories.,\nDaily.Calories.Consumed, and Daily.Caloric.Surplus.Deficit are caught in VIF detection and need to be ad-\ndressed. Based on our data dictionary we can see that Daily.Caloric.Surplus.Deficit is the difference between\nDaily.Calories.Consumed and BMR..Calories., therefore we have chosen to keep Daily.Caloric.Surplus.Deficit\nwhile removing the other two variables. Leaving us with the model below:\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ Age + Daily.Caloric.Surplus.Deficit +\n##\nDuration..weeks., data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -31.819\n-2.310\n1.403\n4.602\n9.156\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n-2.6238347\n3.4349578\n-0.764\n0.447\n## Age\n0.0298644\n0.0618788\n0.483\n0.630\n## Daily.Caloric.Surplus.Deficit\n0.0006742\n0.0020356\n0.331\n0.741\n## Duration..weeks.\n-0.2835862\n0.2155103\n-1.316\n0.191\n##\n## Residual standard error: 7.475 on 96 degrees of freedom\n## Multiple R-squared:\n0.02201,\nAdjusted R-squared:\n-0.008555\n## F-statistic: 0.7201 on 3 and 96 DF,\np-value: 0.5424\nFrom our output we can see that the p-value of our model is quite large, at 0.5424, and none of our predictors\nour significant. We can attempt a stepwise selection to see if any predictors should be included.\nNOTE The codeline below is only included to demonstrate the code that was used. The output returns an\nerror indicating that none of the variables are appropriate to select.\n8"
    },
    {
        "page": 227,
        "text": "#stepmod=ols_step_both_p(quant_weight_model_take3, p_enter=0.05,p_remove=0.3,details=TRUE)\nBased on our output, the large p-value of our model, and the fact that no predictors have a p-value < 0.05,\nwe conclude that this model is not significantly different than a model with no predictors. This suggests\nthat we should attempt other modelling approaches, such as a generalized additive model. However, as this\nis not covered in the scope of our course, we will simply conclude that using only the quantitative variables\nis not a good method for determining weight change given our dataset.\n3.2 Qualitative Model\n##\nParticipant.ID Age Gender Current.Weight..lbs. BMR..Calories.\n## 1\n1\n56\nM\n228.4\n3102.3\n## 2\n2\n46\nF\n165.4\n2275.5\n## 3\n3\n32\nF\n142.8\n2119.4\n## 4\n4\n25\nF\n145.5\n2181.3\n## 5\n5\n38\nM\n155.5\n2463.8\n## 6\n6\n56\nF\n152.9\n2100.6\n##\nDaily.Calories.Consumed Daily.Caloric.Surplus.Deficit Weight.Change..lbs.\n## 1\n3916.0\n813.7\n0.2000\n## 2\n3823.0\n1547.5\n2.4000\n## 3\n2785.4\n666.0\n1.4000\n## 4\n2587.3\n406.0\n0.8000\n## 5\n3312.8\n849.0\n2.0000\n## 6\n2262.4\n161.9\n-12.5135\n##\nDuration..weeks. Physical.Activity.Level Sleep.Quality Stress.Level\n## 1\n1\nSedentary\nExcellent\n6\n## 2\n6\nVery Active\nExcellent\n6\n## 3\n7\nSedentary\nGood\n3\n## 4\n8\nSedentary\nFair\n2\n## 5\n10\nLightly Active\nGood\n1\n## 6\n9\nSedentary\nPoor\n6\n##\nFinal.Weight..lbs.\n## 1\n228.6\n## 2\n167.8\n## 3\n144.2\n## 4\n146.3\n## 5\n157.5\n## 6\n140.4\nMeet our, 100 participants who has decided to embark on a transformative journey to improve their health\nand well-being. They are determined to contribute to body test which is going to be really helpful in the\nhealth department to understand how various lifestyle factors influence the body weight.\nFor the purposes of this section, we will be creating a model that focuses on qualitative variables, starting\nwith the following variables -\n1. Gender- M or F\n2. Physical.Activity.Level\n3. Sleep.Quality\n4. Stress.Level\nBoth genders commits to track their progress with how their physical activity level is throughout that period\nof time. Additionally, mental aspects were recorded such as their sleep quality and stress levels.\n9"
    },
    {
        "page": 228,
        "text": "We can determine whether our model contains any significant predictors using the hypothesis:\nH0 : \u03b21 = \u03b22 = ... = \u03b2i = 0 (i=1,2,...,p)\nHa : At least one \u03b2i \u0338= 0 (i=1,2,...,p)\n#Full Model\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Gender) + factor(Physical.Activity.Level) +\n##\nfactor(Sleep.Quality) + factor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -19.313\n-1.369\n-0.074\n2.093\n12.709\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n1.7189\n1.8081\n0.951\n## factor(Gender)M\n-0.8365\n0.9712\n-0.861\n## factor(Physical.Activity.Level)Moderately Active\n0.0540\n1.2103\n0.045\n## factor(Physical.Activity.Level)Sedentary\n-0.6371\n1.2509\n-0.509\n## factor(Physical.Activity.Level)Very Active\n1.4788\n1.2152\n1.217\n## factor(Sleep.Quality)Fair\n0.5978\n1.3964\n0.428\n## factor(Sleep.Quality)Good\n1.1141\n1.4241\n0.782\n## factor(Sleep.Quality)Poor\n-8.1191\n1.2927\n-6.281\n## factor(Stress.Level)2\n0.2977\n1.7632\n0.169\n## factor(Stress.Level)3\n1.2671\n1.7525\n0.723\n## factor(Stress.Level)4\n1.5876\n2.0526\n0.773\n## factor(Stress.Level)5\n1.8938\n1.8458\n1.026\n## factor(Stress.Level)6\n-0.2817\n1.7490\n-0.161\n## factor(Stress.Level)7\n-0.2628\n1.8777\n-0.140\n## factor(Stress.Level)8\n-10.8574\n1.8308\n-5.930\n## factor(Stress.Level)9\n-9.1820\n1.8864\n-4.867\n##\nPr(>|t|)\n## (Intercept)\n0.344\n## factor(Gender)M\n0.392\n## factor(Physical.Activity.Level)Moderately Active\n0.965\n## factor(Physical.Activity.Level)Sedentary\n0.612\n## factor(Physical.Activity.Level)Very Active\n0.227\n## factor(Sleep.Quality)Fair\n0.670\n## factor(Sleep.Quality)Good\n0.436\n## factor(Sleep.Quality)Poor\n1.42e-08 ***\n## factor(Stress.Level)2\n0.866\n## factor(Stress.Level)3\n0.472\n## factor(Stress.Level)4\n0.441\n## factor(Stress.Level)5\n0.308\n## factor(Stress.Level)6\n0.872\n## factor(Stress.Level)7\n0.889\n## factor(Stress.Level)8\n6.50e-08 ***\n## factor(Stress.Level)9\n5.23e-06 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 4.159 on 84 degrees of freedom\n10"
    },
    {
        "page": 229,
        "text": "## Multiple R-squared:\n0.7351, Adjusted R-squared:\n0.6878\n## F-statistic: 15.54 on 15 and 84 DF,\np-value: < 2.2e-16\nBased on our output, we can see that there is at least on predictor with a p-value < 0.05, allowing us to\nreject the null hypothesis and conclude that at least one predictor is significant.\nTo determine which predictors are significant, we can test the hypothesis:\nH0 : \u03b2i = 0 (i=1,2,...,p)\nHa : \u03b2i \u0338= 0 (i=1,2,...,p)\nFrom our output above we can see that the predictors Sleep.Quality and Stress.Level contain at least one\nfactor with a p-value < 0.05. We therefore reject the null hypothesis and conclude that Sleep.Quality and\nStress.Level significantly affect weight change and should be included in our model.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + factor(Stress.Level),\n##\ndata = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -20.0668\n-1.5978\n-0.1951\n2.0673\n14.2170\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n1.66777\n1.64232\n1.015\n0.313\n## factor(Sleep.Quality)Fair\n0.66673\n1.37884\n0.484\n0.630\n## factor(Sleep.Quality)Good\n0.86678\n1.41286\n0.613\n0.541\n## factor(Sleep.Quality)Poor\n-7.89997\n1.28104\n-6.167 2.06e-08 ***\n## factor(Stress.Level)2\n-0.16713\n1.69144\n-0.099\n0.922\n## factor(Stress.Level)3\n0.72075\n1.68828\n0.427\n0.670\n## factor(Stress.Level)4\n1.12429\n2.01231\n0.559\n0.578\n## factor(Stress.Level)5\n1.45804\n1.78473\n0.817\n0.416\n## factor(Stress.Level)6\n-0.05374\n1.71822\n-0.031\n0.975\n## factor(Stress.Level)7\n-0.14142\n1.82197\n-0.078\n0.938\n## factor(Stress.Level)8\n-11.34225\n1.77596\n-6.387 7.78e-09 ***\n## factor(Stress.Level)9\n-9.37917\n1.86760\n-5.022 2.65e-06 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 4.145 on 88 degrees of freedom\n## Multiple R-squared:\n0.7244, Adjusted R-squared:\n0.69\n## F-statistic: 21.03 on 11 and 88 DF,\np-value: < 2.2e-16\nFrom our output we can see that our refined model has a slightly better Adjusted R-squared (0.69) than\nthe full model (0.6878), indicating an improvement in model simplicity and explanatory power.\nThe variables factor(Sleep.Quality)Poor, factor(Stress.Level)8, and factor(Stress.Level)9 remain\nhighly significant and have the greatest impact on weight change.\nTon ensure no issues with multicollinearity we can run a VIF test on our model\n##\nGVIF Df GVIF^(1/(2*Df))\n## factor(Sleep.Quality) 1.227473\n3\n1.034750\n## factor(Stress.Level)\n1.227473\n8\n1.012892\n11"
    },
    {
        "page": 230,
        "text": "##\n## Call:\n## imcdiag(mod = weight_refined_model, method = \"VIF\")\n##\n##\n##\nVIF Multicollinearity Diagnostics\n##\n##\nVIF detection\n## factor(Sleep.Quality)Fair 2.0186\n0\n## factor(Sleep.Quality)Good 1.9939\n0\n## factor(Sleep.Quality)Poor 2.2506\n0\n## factor(Stress.Level)2\n2.0051\n0\n## factor(Stress.Level)3\n1.9976\n0\n## factor(Stress.Level)4\n1.5345\n0\n## factor(Stress.Level)5\n1.8152\n0\n## factor(Stress.Level)6\n1.9436\n0\n## factor(Stress.Level)7\n1.7391\n0\n## factor(Stress.Level)8\n1.7974\n0\n## factor(Stress.Level)9\n1.6628\n0\n##\n## NOTE:\nVIF Method Failed to detect multicollinearity\n##\n##\n## 0 --> COLLINEARITY is not detected by the test\n##\n## ===================================\nFrom our output we can see that the VIF is less than 5 so all predictors are kept as there is no indication of\nsignificant multicollinearity.\nNow that we have concluded the absence of multicollinearity, we can begin strengthening our model. To\nbegin, we will look at adding interaction terms. Interaction terms are useful when we suspect that the effect\nof one predictor on the response variable depends on the level of another predictor.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ (factor(Sleep.Quality) + factor(Stress.Level))^2,\n##\ndata = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -17.509\n-1.190\n0.000\n1.251\n16.775\n##\n## Coefficients: (2 not defined because of singularities)\n##\nEstimate Std. Error t value\n## (Intercept)\n0.90000\n4.36708\n0.206\n## factor(Sleep.Quality)Fair\n2.00000\n5.04267\n0.397\n## factor(Sleep.Quality)Good\n0.96667\n5.04267\n0.192\n## factor(Sleep.Quality)Poor\n-6.86346\n4.88254\n-1.406\n## factor(Stress.Level)2\n1.20000\n5.04267\n0.238\n## factor(Stress.Level)3\n-0.30000\n5.04267\n-0.059\n## factor(Stress.Level)4\n4.15260\n3.78200\n1.098\n## factor(Stress.Level)5\n0.85000\n5.34856\n0.159\n## factor(Stress.Level)6\n1.50000\n4.88254\n0.307\n12"
    },
    {
        "page": 231,
        "text": "## factor(Stress.Level)7\n-0.40000\n5.34856\n-0.075\n## factor(Stress.Level)8\n-4.57874\n6.17598\n-0.741\n## factor(Stress.Level)9\n-12.20581\n3.08799\n-3.953\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2\n-3.14000\n5.96657\n-0.526\n## factor(Sleep.Quality)Good:factor(Stress.Level)2\n-0.60667\n5.96657\n-0.102\n## factor(Sleep.Quality)Poor:factor(Stress.Level)2\n2.14010\n7.01910\n0.305\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3\n1.80000\n7.13141\n0.252\n## factor(Sleep.Quality)Good:factor(Stress.Level)3\n0.36667\n6.17598\n0.059\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3\n1.89297\n5.73767\n0.330\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4\n-4.25260\n5.49512\n-0.774\n## factor(Sleep.Quality)Good:factor(Stress.Level)4\n-4.11927\n5.19786\n-0.792\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\nNA\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5\n-2.15000\n6.67082\n-0.322\n## factor(Sleep.Quality)Good:factor(Stress.Level)5\n1.68333\n7.35089\n0.229\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5\n1.46086\n6.04595\n0.242\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6\n-2.46667\n6.04595\n-0.408\n## factor(Sleep.Quality)Good:factor(Stress.Level)6\n-1.56667\n6.30334\n-0.249\n## factor(Sleep.Quality)Poor:factor(Stress.Level)6\n-2.00745\n5.77710\n-0.347\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7\n0.06667\n6.42817\n0.010\n## factor(Sleep.Quality)Good:factor(Stress.Level)7\n3.33333\n7.35089\n0.453\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7\n-0.37889\n6.17598\n-0.061\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8\n-6.58104\n7.35089\n-0.895\n## factor(Sleep.Quality)Good:factor(Stress.Level)8\n-2.98419\n7.35089\n-0.406\n## factor(Sleep.Quality)Poor:factor(Stress.Level)8\n-9.31800\n6.78890\n-1.373\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9\n4.63575\n4.71699\n0.983\n## factor(Sleep.Quality)Good:factor(Stress.Level)9\n5.04842\n5.04267\n1.001\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\nNA\nNA\n##\nPr(>|t|)\n## (Intercept)\n0.837357\n## factor(Sleep.Quality)Fair\n0.692931\n## factor(Sleep.Quality)Good\n0.848568\n## factor(Sleep.Quality)Poor\n0.164499\n## factor(Stress.Level)2\n0.812642\n## factor(Stress.Level)3\n0.952740\n## factor(Stress.Level)4\n0.276198\n## factor(Stress.Level)5\n0.874216\n## factor(Stress.Level)6\n0.759646\n## factor(Stress.Level)7\n0.940611\n## factor(Stress.Level)8\n0.461094\n## factor(Stress.Level)9\n0.000191 ***\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2 0.600468\n## factor(Sleep.Quality)Good:factor(Stress.Level)2 0.919321\n## factor(Sleep.Quality)Poor:factor(Stress.Level)2 0.761404\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3 0.801513\n## factor(Sleep.Quality)Good:factor(Stress.Level)3 0.952837\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3 0.742506\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4 0.441761\n## factor(Sleep.Quality)Good:factor(Stress.Level)4 0.430913\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5 0.748245\n## factor(Sleep.Quality)Good:factor(Stress.Level)5 0.819579\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5 0.809819\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6 0.684604\n## factor(Sleep.Quality)Good:factor(Stress.Level)6 0.804484\n13"
    },
    {
        "page": 232,
        "text": "## factor(Sleep.Quality)Poor:factor(Stress.Level)6 0.729334\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7 0.991757\n## factor(Sleep.Quality)Good:factor(Stress.Level)7 0.651705\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7 0.951266\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8 0.373896\n## factor(Sleep.Quality)Good:factor(Stress.Level)8 0.686083\n## factor(Sleep.Quality)Poor:factor(Stress.Level)8 0.174544\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9 0.329306\n## factor(Sleep.Quality)Good:factor(Stress.Level)9 0.320415\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 4.367 on 66 degrees of freedom\n## Multiple R-squared:\n0.7705, Adjusted R-squared:\n0.6558\n## F-statistic: 6.716 on 33 and 66 DF,\np-value: 3.128e-11\nThe interaction model does not improve predictive performance or interpretability over the refined model.\nThe refined model remains the best choice for explaining the relationship between weight change, Sleep\nQuality, and Stress Level.\nFurthermore, the Adjusted R sqaure in our interaction model has decreased from our refined model (0.6558,\ndown from 0.69), and the RSE has increased (4.367, up from 4.145). We also see that only one predictor\nhas a p-value < 0.05. Based on all of this, we conclude that the refined model is better than the presented\ninteraction model.\nTo further improve our model we can attempt the addition of higher order terms. Note Use of I(factor()\u02c62)\nis for continuous variables where you want to square the variable, but it\u2019s not applicable to categorical\nvariables. As here we have all categorical variables we will just do it like -\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ (factor(Sleep.Quality) + factor(Stress.Level))^3,\n##\ndata = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -17.509\n-1.190\n0.000\n1.251\n16.775\n##\n## Coefficients: (2 not defined because of singularities)\n##\nEstimate Std. Error t value\n## (Intercept)\n0.90000\n4.36708\n0.206\n## factor(Sleep.Quality)Fair\n2.00000\n5.04267\n0.397\n## factor(Sleep.Quality)Good\n0.96667\n5.04267\n0.192\n## factor(Sleep.Quality)Poor\n-6.86346\n4.88254\n-1.406\n## factor(Stress.Level)2\n1.20000\n5.04267\n0.238\n## factor(Stress.Level)3\n-0.30000\n5.04267\n-0.059\n## factor(Stress.Level)4\n4.15260\n3.78200\n1.098\n## factor(Stress.Level)5\n0.85000\n5.34856\n0.159\n## factor(Stress.Level)6\n1.50000\n4.88254\n0.307\n## factor(Stress.Level)7\n-0.40000\n5.34856\n-0.075\n## factor(Stress.Level)8\n-4.57874\n6.17598\n-0.741\n## factor(Stress.Level)9\n-12.20581\n3.08799\n-3.953\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2\n-3.14000\n5.96657\n-0.526\n## factor(Sleep.Quality)Good:factor(Stress.Level)2\n-0.60667\n5.96657\n-0.102\n14"
    },
    {
        "page": 233,
        "text": "## factor(Sleep.Quality)Poor:factor(Stress.Level)2\n2.14010\n7.01910\n0.305\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3\n1.80000\n7.13141\n0.252\n## factor(Sleep.Quality)Good:factor(Stress.Level)3\n0.36667\n6.17598\n0.059\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3\n1.89297\n5.73767\n0.330\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4\n-4.25260\n5.49512\n-0.774\n## factor(Sleep.Quality)Good:factor(Stress.Level)4\n-4.11927\n5.19786\n-0.792\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\nNA\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5\n-2.15000\n6.67082\n-0.322\n## factor(Sleep.Quality)Good:factor(Stress.Level)5\n1.68333\n7.35089\n0.229\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5\n1.46086\n6.04595\n0.242\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6\n-2.46667\n6.04595\n-0.408\n## factor(Sleep.Quality)Good:factor(Stress.Level)6\n-1.56667\n6.30334\n-0.249\n## factor(Sleep.Quality)Poor:factor(Stress.Level)6\n-2.00745\n5.77710\n-0.347\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7\n0.06667\n6.42817\n0.010\n## factor(Sleep.Quality)Good:factor(Stress.Level)7\n3.33333\n7.35089\n0.453\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7\n-0.37889\n6.17598\n-0.061\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8\n-6.58104\n7.35089\n-0.895\n## factor(Sleep.Quality)Good:factor(Stress.Level)8\n-2.98419\n7.35089\n-0.406\n## factor(Sleep.Quality)Poor:factor(Stress.Level)8\n-9.31800\n6.78890\n-1.373\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9\n4.63575\n4.71699\n0.983\n## factor(Sleep.Quality)Good:factor(Stress.Level)9\n5.04842\n5.04267\n1.001\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\nNA\nNA\n##\nPr(>|t|)\n## (Intercept)\n0.837357\n## factor(Sleep.Quality)Fair\n0.692931\n## factor(Sleep.Quality)Good\n0.848568\n## factor(Sleep.Quality)Poor\n0.164499\n## factor(Stress.Level)2\n0.812642\n## factor(Stress.Level)3\n0.952740\n## factor(Stress.Level)4\n0.276198\n## factor(Stress.Level)5\n0.874216\n## factor(Stress.Level)6\n0.759646\n## factor(Stress.Level)7\n0.940611\n## factor(Stress.Level)8\n0.461094\n## factor(Stress.Level)9\n0.000191 ***\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2 0.600468\n## factor(Sleep.Quality)Good:factor(Stress.Level)2 0.919321\n## factor(Sleep.Quality)Poor:factor(Stress.Level)2 0.761404\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3 0.801513\n## factor(Sleep.Quality)Good:factor(Stress.Level)3 0.952837\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3 0.742506\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4 0.441761\n## factor(Sleep.Quality)Good:factor(Stress.Level)4 0.430913\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5 0.748245\n## factor(Sleep.Quality)Good:factor(Stress.Level)5 0.819579\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5 0.809819\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6 0.684604\n## factor(Sleep.Quality)Good:factor(Stress.Level)6 0.804484\n## factor(Sleep.Quality)Poor:factor(Stress.Level)6 0.729334\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7 0.991757\n## factor(Sleep.Quality)Good:factor(Stress.Level)7 0.651705\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7 0.951266\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8 0.373896\n15"
    },
    {
        "page": 234,
        "text": "## factor(Sleep.Quality)Good:factor(Stress.Level)8 0.686083\n## factor(Sleep.Quality)Poor:factor(Stress.Level)8 0.174544\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9 0.329306\n## factor(Sleep.Quality)Good:factor(Stress.Level)9 0.320415\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 4.367 on 66 degrees of freedom\n## Multiple R-squared:\n0.7705, Adjusted R-squared:\n0.6558\n## F-statistic: 6.716 on 33 and 66 DF,\np-value: 3.128e-11\nNotice that this gives the same output as before. Similar to before we will stick with our refined model.\nNow we will test for some of the assumptions in our model, using plots and stastical tests.\nWe can test for linearity by inspecting our residual plots.\n## \u2018geom_smooth()\u2018 using formula = \u2019y ~ x\u2019\n\u221220\n\u221210\n0\n10\n\u221215\n\u221210\n\u22125\n0\n5\nFitted Values\nResiduals\nResiduals vs Fitted Values\nThis plot suggests that the model does not quite pass the assumption of linearity. We can see that the\nresiduals do not appear randomly scattered, rather they are clustered in 3 different groups, additionally we\ncan see the presence of a curve in our line. Normally we could try transforming our data, but in this case,\nas mentioned before, we cannot add higher order terms. We also cannot perform a log transformation on\nour predictors or our responding variable, since our dummy variables contain 0 and our responding variable\ncontains negative values.\nTo test our independence assumption, we can inspect the residuals in the figure below.\n16"
    },
    {
        "page": 235,
        "text": "0\n20\n40\n60\n80\n100\n\u221220\n\u221210\n0\n5\n10\n15\nResiduals vs Observation Number\nObservation Number\nResiduals\nGiven the randomly scattered point, and lack of trends, we can suggest that independence has most likely\nbeen met.\nAlthough there a few outliers, they do not appear to follow any pattern.\nAdditionally, our\nresponding variable is not considered time-series data.\nTo test for equal variance, we can inspect our residual plots and perform a Breusch-Pagan Test, where the\nhypothesis would be:\nH0 : \u03c32\n1 = \u03c32\n2 = ... = \u03c32\nn (heteroscedasticity is not present)Ha : at least one \u03c32\ni is different from the others i = 1, 2, ..., n (he\n## \u2018geom_smooth()\u2018 using formula = \u2019y ~ x\u2019\n17"
    },
    {
        "page": 236,
        "text": "\u221220\n\u221210\n0\n10\n\u221215\n\u221210\n\u22125\n0\n5\nFitted Values\nResiduals\nResiduals vs Fitted Values\n##\n##\nstudentized Breusch-Pagan test\n##\n## data:\nweight_interaction_model\n## BP = 52.041, df = 33, p-value = 0.01872\nAs we can see the presence of patterns in our data points, and the line is curved, we believe heteroscedasticity\nis present. Our Breusch-Pagan test returned a p-value of 0.01872, which is less than 0.05. Therefore we\nreject the null hypothesis, and conclude there is statistically significant evidence of heteroscedasticity in the\nmodel.\nTo test our assumptions of normality, we can create our Q-Q plot and run a Shapiro-Wilks test, using the\nthe hypothesis:\nH0 : the residuals are normally distributedHa : the residuals are not normally distributed\n18"
    },
    {
        "page": 237,
        "text": "\u22122\n\u22121\n0\n1\n2\n\u221220\n\u221210\n0\n5\n10\n15\nNormal Q\u2212Q Plot\nTheoretical Quantiles\nSample Quantiles\n##\n##\nShapiro-Wilk normality test\n##\n## data:\nresiduals(weight_refined_model)\n## W = 0.83622, p-value = 3.793e-09\nThe Q-Q plot suggests that the data may not be perfectly normally distributed. The outliers might be\nindicative of non-normality. As our Shapiro-Wilks test returned a p-value of 3.793e-09, which is < 0.05, we\nreject the null hypothesis and conclude that our residuals are not normally distributed.\nTo check for outliers we can look at our Cook\u2019s Distance and Leverage plots.\n19"
    },
    {
        "page": 238,
        "text": "0\n20\n40\n60\n80\n100\n0.00\n0.10\n0.20\n0.30\nCook's Distance Plot\nObs. number\nCook's distance\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + factor(Stress.Level))\nCook's distance\n8\n72\n38\n##\n[1] Participant.ID\nAge\n##\n[3] Gender\nCurrent.Weight..lbs.\n##\n[5] BMR..Calories.\nDaily.Calories.Consumed\n##\n[7] Daily.Caloric.Surplus.Deficit Weight.Change..lbs.\n##\n[9] Duration..weeks.\nPhysical.Activity.Level\n## [11] Sleep.Quality\nStress.Level\n## [13] Final.Weight..lbs.\n## <0 rows> (or 0-length row.names)\n20"
    },
    {
        "page": 239,
        "text": "0\n20\n40\n60\n80\n100\n0.10\n0.12\n0.14\n0.16\nLeverage Plot\nObservation Index\nLeverage\n2p/n\n3p/n\nThere are three influential points in the data, highlighted by the red vertical lines at observations 8, 38, and\n72. Cook\u2019s Distance Values: These points have high Cook\u2019s distances, exceeding the threshold, suggesting\nthey significantly influence the regression model.\nHigh leverage: The outlier has an unusual combination of predictor values, making it influential.\nHigh\nresidual: The outlier has a large difference between its observed and predicted values, suggesting it\u2019s not\nwell explained by the model.\nThe data points at observations 8, 38, and 72 are influential because they significantly affect the estimated\ncoefficients of the regression model.\nAlthough all of our assumptions were not met, we will still present our final model.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + factor(Stress.Level),\n##\ndata = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -20.0668\n-1.5978\n-0.1951\n2.0673\n14.2170\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n1.66777\n1.64232\n1.015\n0.313\n## factor(Sleep.Quality)Fair\n0.66673\n1.37884\n0.484\n0.630\n## factor(Sleep.Quality)Good\n0.86678\n1.41286\n0.613\n0.541\n## factor(Sleep.Quality)Poor\n-7.89997\n1.28104\n-6.167 2.06e-08 ***\n21"
    },
    {
        "page": 240,
        "text": "## factor(Stress.Level)2\n-0.16713\n1.69144\n-0.099\n0.922\n## factor(Stress.Level)3\n0.72075\n1.68828\n0.427\n0.670\n## factor(Stress.Level)4\n1.12429\n2.01231\n0.559\n0.578\n## factor(Stress.Level)5\n1.45804\n1.78473\n0.817\n0.416\n## factor(Stress.Level)6\n-0.05374\n1.71822\n-0.031\n0.975\n## factor(Stress.Level)7\n-0.14142\n1.82197\n-0.078\n0.938\n## factor(Stress.Level)8\n-11.34225\n1.77596\n-6.387 7.78e-09 ***\n## factor(Stress.Level)9\n-9.37917\n1.86760\n-5.022 2.65e-06 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 4.145 on 88 degrees of freedom\n## Multiple R-squared:\n0.7244, Adjusted R-squared:\n0.69\n## F-statistic: 21.03 on 11 and 88 DF,\np-value: < 2.2e-16\nWeight.Change (lbs) = 1.66777+0.66673\u00b7SleepQualityFair+0.86678\u00b7SleepQualityGood\u22127.89997\u00b7SleepQualityPoor\u22120.16713\u00b7\nIn conclusion the refined model explores the relationship between weight change and two key factors: sleep\nquality and stress level.\nNow that we have our final model, we can begin interpreting it.\nIntercept: When sleep quality is excellent, and our stress level is 1, we can expect weight change to increase\nby 1.66777 pounds.\nSleep Quality:\n\u2022 Participants with fair sleep quality gain 0.66673 pounds more weight on average compared to those\nwith excellent sleep quality.\n\u2022 Participants with good sleep quality gain 0.86678 pounds more weight on average compared to those\nwith excellent sleep quality.\n\u2022 Participants with poor sleep quality lose 7.89997 pounds more weight on average compared to those\nwith excellent sleep quality.\nStress Level:\n\u2022 Participants with a stress level of 2 lose 0.16713 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 3 gain 0.72075 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 4 gain 1.12429 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 5 gain 1.45804 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 6 lose 0.05374 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 7 lose 0.14142 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 8 lose 11.34225 pounds more weight on average compared to those\nwith a stress level of 1.\n\u2022 Participants with a stress level of 9 lose 9.37917 pounds more weight on average compared to those\nwith a stress level of 1.\n22"
    },
    {
        "page": 241,
        "text": "These findings suggest that worse sleep quality and higher stress levels are associated with greater weight\nloss.\nOverall, the model explains a moderate proportion of variance in weight change (Adjusted R-squared =\n0.69), indicating that these two factors contribute to understanding weight changes, but other un-explored\nfactors might also play a significant role.\n3.3 Model Including All Variables\nTo determine the best model for predicting weight change, given our dataset, we begin by inspecting our\ndataset.\n##\nParticipant.ID Age Gender Current.Weight..lbs. BMR..Calories.\n## 1\n1\n56\nM\n228.4\n3102.3\n## 2\n2\n46\nF\n165.4\n2275.5\n## 3\n3\n32\nF\n142.8\n2119.4\n## 4\n4\n25\nF\n145.5\n2181.3\n## 5\n5\n38\nM\n155.5\n2463.8\n## 6\n6\n56\nF\n152.9\n2100.6\n##\nDaily.Calories.Consumed Daily.Caloric.Surplus.Deficit Weight.Change..lbs.\n## 1\n3916.0\n813.7\n0.2000\n## 2\n3823.0\n1547.5\n2.4000\n## 3\n2785.4\n666.0\n1.4000\n## 4\n2587.3\n406.0\n0.8000\n## 5\n3312.8\n849.0\n2.0000\n## 6\n2262.4\n161.9\n-12.5135\n##\nDuration..weeks. Physical.Activity.Level Sleep.Quality Stress.Level\n## 1\n1\nSedentary\nExcellent\n6\n## 2\n6\nVery Active\nExcellent\n6\n## 3\n7\nSedentary\nGood\n3\n## 4\n8\nSedentary\nFair\n2\n## 5\n10\nLightly Active\nGood\n1\n## 6\n9\nSedentary\nPoor\n6\n##\nFinal.Weight..lbs.\n## 1\n228.6\n## 2\n167.8\n## 3\n144.2\n## 4\n146.3\n## 5\n157.5\n## 6\n140.4\nOur first step is to build our full model, and determine whether any assumptions have been broken. Do\nnote, that we chose to remove the variables \u201cCurrent.Weight..lbs.\u201d and \u201cFinal.Weight..lbs.\u201d because our\nresponding variable, \u201cWeight.Change..lbs.\u201d is just a result of subtracting the two variables, making weight\nchange dependent on both current and final weight. To ensure that at least one of our predictors is significant,\nwe test the hypothesis:\nH0 : \u03b21 = \u03b22 = ... = \u03b2i = 0 (i=1,2,...,p)\nHa : At least one \u03b2i \u0338= 0 (i=1,2,...,p)\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ Age + factor(Gender) + BMR..Calories. +\n##\nDaily.Calories.Consumed + Daily.Caloric.Surplus.Deficit +\n23"
    },
    {
        "page": 242,
        "text": "##\nDuration..weeks. + factor(Physical.Activity.Level) + factor(Sleep.Quality) +\n##\nfactor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -16.5881\n-1.4426\n0.2319\n2.1582\n9.7510\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n7.032845\n4.940503\n1.424\n## Age\n-0.002834\n0.036018\n-0.079\n## factor(Gender)M\n0.263772\n1.150845\n0.229\n## BMR..Calories.\n-6.915519\n10.273016\n-0.673\n## Daily.Calories.Consumed\n6.913309\n10.272867\n0.673\n## Daily.Caloric.Surplus.Deficit\n-6.911143\n10.273082\n-0.673\n## Duration..weeks.\n-0.395738\n0.122177\n-3.239\n## factor(Physical.Activity.Level)Moderately Active\n-0.316972\n1.247677\n-0.254\n## factor(Physical.Activity.Level)Sedentary\n0.007914\n1.327862\n0.006\n## factor(Physical.Activity.Level)Very Active\n0.751174\n1.520393\n0.494\n## factor(Sleep.Quality)Fair\n1.544141\n1.366646\n1.130\n## factor(Sleep.Quality)Good\n2.109521\n1.416478\n1.489\n## factor(Sleep.Quality)Poor\n-7.411684\n1.271290\n-5.830\n## factor(Stress.Level)2\n-0.153266\n1.717603\n-0.089\n## factor(Stress.Level)3\n1.489659\n1.714714\n0.869\n## factor(Stress.Level)4\n1.006707\n1.974665\n0.510\n## factor(Stress.Level)5\n0.770360\n1.798465\n0.428\n## factor(Stress.Level)6\n-0.849362\n1.690039\n-0.503\n## factor(Stress.Level)7\n0.727956\n1.848089\n0.394\n## factor(Stress.Level)8\n-11.058953\n1.754401\n-6.304\n## factor(Stress.Level)9\n-9.822984\n1.838339\n-5.343\n##\nPr(>|t|)\n## (Intercept)\n0.15853\n## Age\n0.93748\n## factor(Gender)M\n0.81931\n## BMR..Calories.\n0.50280\n## Daily.Calories.Consumed\n0.50293\n## Daily.Caloric.Surplus.Deficit\n0.50307\n## Duration..weeks.\n0.00176 **\n## factor(Physical.Activity.Level)Moderately Active\n0.80012\n## factor(Physical.Activity.Level)Sedentary\n0.99526\n## factor(Physical.Activity.Level)Very Active\n0.62263\n## factor(Sleep.Quality)Fair\n0.26195\n## factor(Sleep.Quality)Good\n0.14040\n## factor(Sleep.Quality)Poor\n1.15e-07 ***\n## factor(Stress.Level)2\n0.92912\n## factor(Stress.Level)3\n0.38762\n## factor(Stress.Level)4\n0.61160\n## factor(Stress.Level)5\n0.66957\n## factor(Stress.Level)6\n0.61667\n## factor(Stress.Level)7\n0.69472\n## factor(Stress.Level)8\n1.55e-08 ***\n## factor(Stress.Level)9\n8.58e-07 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n24"
    },
    {
        "page": 243,
        "text": "##\n## Residual standard error: 3.925 on 79 degrees of freedom\n## Multiple R-squared:\n0.7781, Adjusted R-squared:\n0.7219\n## F-statistic: 13.85 on 20 and 79 DF,\np-value: < 2.2e-16\nBased on our output, we can see that at least one predictor has a p-value < 0.05, therefore we reject our null\nhypothesis and conclude that at least one predictor significantly affects weight change. Before determining\nwhich predictors to include in our model, we want to test for multicollinearity. This can be done using the\nVIF method.\n##\nGVIF Df GVIF^(1/(2*Df))\n## Age\n1.244553e+00\n1\n1.115595\n## factor(Gender)\n2.106715e+00\n1\n1.451453\n## BMR..Calories.\n9.005157e+07\n1\n9489.550825\n## Daily.Calories.Consumed\n1.786538e+08\n1\n13366.144669\n## Daily.Caloric.Surplus.Deficit\n9.361072e+07\n1\n9675.263124\n## Duration..weeks.\n1.185115e+00\n1\n1.088630\n## factor(Physical.Activity.Level) 4.324149e+00\n3\n1.276390\n## factor(Sleep.Quality)\n1.580526e+00\n3\n1.079279\n## factor(Stress.Level)\n3.048675e+00\n8\n1.072153\n##\n## Call:\n## imcdiag(mod = weight_model_full, method = \"VIF\")\n##\n##\n##\nVIF Multicollinearity Diagnostics\n##\n##\nVIF detection\n## Age\n1.244600e+00\n0\n## factor(Gender)M\n2.106700e+00\n0\n## BMR..Calories.\n9.005157e+07\n1\n## Daily.Calories.Consumed\n1.786538e+08\n1\n## Daily.Caloric.Surplus.Deficit\n9.361071e+07\n1\n## Duration..weeks.\n1.185100e+00\n0\n## factor(Physical.Activity.Level)Moderately Active 1.894200e+00\n0\n## factor(Physical.Activity.Level)Sedentary\n1.898400e+00\n0\n## factor(Physical.Activity.Level)Very Active\n2.656800e+00\n0\n## factor(Sleep.Quality)Fair\n2.210900e+00\n0\n## factor(Sleep.Quality)Good\n2.234400e+00\n0\n## factor(Sleep.Quality)Poor\n2.471100e+00\n0\n## factor(Stress.Level)2\n2.305200e+00\n0\n## factor(Stress.Level)3\n2.297400e+00\n0\n## factor(Stress.Level)4\n1.647400e+00\n0\n## factor(Stress.Level)5\n2.055000e+00\n0\n## factor(Stress.Level)6\n2.096500e+00\n0\n## factor(Stress.Level)7\n1.994900e+00\n0\n## factor(Stress.Level)8\n1.955600e+00\n0\n## factor(Stress.Level)9\n1.796200e+00\n0\n##\n## Multicollinearity may be due to BMR..Calories. Daily.Calories.Consumed Daily.Caloric.Surplus.Deficit\n##\n## 1 --> COLLINEARITY is detected by the test\n25"
    },
    {
        "page": 244,
        "text": "## 0 --> COLLINEARITY is not detected by the test\n##\n## ===================================\nFrom our output, we can see that three variables have a VIF score > 5, suggesting multicollinearity. These\nvariables are \u201cBMR..Calories.\u201d, \u201cDaily.Calories.Consumed\u201d, and \u201cDaily.Caloric.Surplus.Deficit\u201d. Based on\nthe definitions provided in the data set, \u201cDaily.Caloric.Surplus.Deficit\u201d is the difference between calories\nconsumed and BMR, therefore we chose to keep \u201cDaily.Caloric.Surplus.Deficit\u201d and remove \u201cBMR..Calories.\u201d,\nand \u201cDaily.Calories.Consumed\u201d.\nFollowing the removal of those variables, we will determine which predictors are significant using the hy-\npothesis test:\nH0 : \u03b2i = 0 (i=1,2,...,p)\nHa : \u03b2i \u0338= 0 (i=1,2,...,p)\nThis can be achieved by looking at the individual t-tests from the summary of our adjusted model.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ Daily.Caloric.Surplus.Deficit +\n##\nAge + factor(Gender) + Duration..weeks. + factor(Physical.Activity.Level) +\n##\nfactor(Sleep.Quality) + factor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -17.221\n-1.637\n0.325\n2.286\n9.717\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n1.098664\n3.061275\n0.359\n## Daily.Caloric.Surplus.Deficit\n0.002679\n0.001871\n1.432\n## Age\n0.003592\n0.035076\n0.102\n## factor(Gender)M\n-0.768568\n0.922416\n-0.833\n## Duration..weeks.\n-0.402046\n0.121077\n-3.321\n## factor(Physical.Activity.Level)Moderately Active\n-0.618016\n1.234619\n-0.501\n## factor(Physical.Activity.Level)Sedentary\n-0.016837\n1.330946\n-0.013\n## factor(Physical.Activity.Level)Very Active\n0.360470\n1.505470\n0.239\n## factor(Sleep.Quality)Fair\n1.587961\n1.365649\n1.163\n## factor(Sleep.Quality)Good\n2.256215\n1.418302\n1.591\n## factor(Sleep.Quality)Poor\n-7.353535\n1.273937\n-5.772\n## factor(Stress.Level)2\n0.382922\n1.680623\n0.228\n## factor(Stress.Level)3\n1.499079\n1.707093\n0.878\n## factor(Stress.Level)4\n1.272710\n1.961313\n0.649\n## factor(Stress.Level)5\n1.420505\n1.757602\n0.808\n## factor(Stress.Level)6\n-0.479314\n1.665477\n-0.288\n## factor(Stress.Level)7\n0.924742\n1.821196\n0.508\n## factor(Stress.Level)8\n-10.782512\n1.735068\n-6.214\n## factor(Stress.Level)9\n-9.275712\n1.804889\n-5.139\n##\nPr(>|t|)\n## (Intercept)\n0.72061\n## Daily.Caloric.Surplus.Deficit\n0.15610\n## Age\n0.91868\n## factor(Gender)M\n0.40718\n## Duration..weeks.\n0.00135 **\n26"
    },
    {
        "page": 245,
        "text": "## factor(Physical.Activity.Level)Moderately Active\n0.61803\n## factor(Physical.Activity.Level)Sedentary\n0.98994\n## factor(Physical.Activity.Level)Very Active\n0.81137\n## factor(Sleep.Quality)Fair\n0.24833\n## factor(Sleep.Quality)Good\n0.11555\n## factor(Sleep.Quality)Poor\n1.39e-07 ***\n## factor(Stress.Level)2\n0.82034\n## factor(Stress.Level)3\n0.38246\n## factor(Stress.Level)4\n0.51823\n## factor(Stress.Level)5\n0.42134\n## factor(Stress.Level)6\n0.77424\n## factor(Stress.Level)7\n0.61300\n## factor(Stress.Level)8\n2.11e-08 ***\n## factor(Stress.Level)9\n1.87e-06 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 3.939 on 81 degrees of freedom\n## Multiple R-squared:\n0.7709, Adjusted R-squared:\n0.72\n## F-statistic: 15.14 on 18 and 81 DF,\np-value: < 2.2e-16\nBased on this output, we can see there are 4 instances where the null hypothesis is rejected (p-value < 0.05),\nimplying that these variables are significant and should be included in our model. These variables are:\n\u2022 Duration..weeks.\n\u2022 factor(Sleep.Quality)Poor\n\u2022 factor(Stress.Level)8\n\u2022 factor(Stress.Level)9\nDespite the fact that only some of the dummies are significant, for the categories \u201cSleep.Quality\u201d and\n\u201cStress.Level\u201d, we must include the full variable in our model. Therefore our model will include the predictors\nSleep.Quality, Stress.Level and Duration..weeks.\nNOTE An attempt was made to use stepwise regression to determine which variables were best to keep,\nwith the code for that being included in the appendix. Upon running the stepwise, it was determined that\na 4th variable, Daily,Caloric.Surplus.Deficit should be included. In the end we chose not to include this\nvariable for a couple reasons. First, our categorical variables had many levels, in which case it is often better\nto just manually select significant predictors using t-tests from our summary table. When performing this\nmanual selection, we found the variable the stepwise selection wanted to include, had a p-value 0f 0.15610,\nand a coefficient of 0.002679. This implied the variable had no significant effect. Based on this, and to avoid\noverfitting and complicating our model, we chose to not include this variable\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + Duration..weeks. +\n##\nfactor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -18.2553\n-1.9040\n0.2041\n2.3282\n12.3789\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n3.7718\n1.7173\n2.196\n0.0307 *\n27"
    },
    {
        "page": 246,
        "text": "## factor(Sleep.Quality)Fair\n1.4083\n1.3413\n1.050\n0.2967\n## factor(Sleep.Quality)Good\n1.8466\n1.3895\n1.329\n0.1873\n## factor(Sleep.Quality)Poor\n-7.4041\n1.2362\n-5.990 4.62e-08 ***\n## Duration..weeks.\n-0.3650\n0.1204\n-3.032\n0.0032 **\n## factor(Stress.Level)2\n-0.3405\n1.6188\n-0.210\n0.8339\n## factor(Stress.Level)3\n0.4781\n1.6168\n0.296\n0.7682\n## factor(Stress.Level)4\n0.6451\n1.9312\n0.334\n0.7391\n## factor(Stress.Level)5\n0.9831\n1.7142\n0.573\n0.5678\n## factor(Stress.Level)6\n-0.3863\n1.6471\n-0.235\n0.8151\n## factor(Stress.Level)7\n0.4839\n1.7548\n0.276\n0.7834\n## factor(Stress.Level)8\n-11.3423\n1.6987\n-6.677 2.19e-09 ***\n## factor(Stress.Level)9\n-9.7760\n1.7911\n-5.458 4.48e-07 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 3.964 on 87 degrees of freedom\n## Multiple R-squared:\n0.7507, Adjusted R-squared:\n0.7164\n## F-statistic: 21.84 on 12 and 87 DF,\np-value: < 2.2e-16\nWe can see that all predictors are now significant. Before moving on to interaction terms, we should check\nagain for multicollinearity, just to be certain:\n##\nGVIF Df GVIF^(1/(2*Df))\n## factor(Sleep.Quality) 1.303780\n3\n1.045203\n## Duration..weeks.\n1.128093\n1\n1.062117\n## factor(Stress.Level)\n1.308020\n8\n1.016924\n##\n## Call:\n## imcdiag(mod = weight_model_adj2, method = \"VIF\")\n##\n##\n##\nVIF Multicollinearity Diagnostics\n##\n##\nVIF detection\n## factor(Sleep.Quality)Fair 2.0880\n0\n## factor(Sleep.Quality)Good 2.1080\n0\n## factor(Sleep.Quality)Poor 2.2907\n0\n## Duration..weeks.\n1.1281\n0\n## factor(Stress.Level)2\n2.0076\n0\n## factor(Stress.Level)3\n2.0025\n0\n## factor(Stress.Level)4\n1.5448\n0\n## factor(Stress.Level)5\n1.8305\n0\n## factor(Stress.Level)6\n1.9523\n0\n## factor(Stress.Level)7\n1.7634\n0\n## factor(Stress.Level)8\n1.7974\n0\n## factor(Stress.Level)9\n1.6718\n0\n##\n## NOTE:\nVIF Method Failed to detect multicollinearity\n##\n##\n## 0 --> COLLINEARITY is not detected by the test\n##\n## ===================================\n28"
    },
    {
        "page": 247,
        "text": "As we can see from our output, there are no issues with multicollinearity.\nNow we can begin checking for interaction terms. We can start by testing all interaction terms for our model\nusing the hypothesis test:\nH0 : \u03b2interaction = 0 (i=1,2,...,p)\nHa : \u03b2interaction \u0338= 0 (i=1,2,...,p)\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ (factor(Sleep.Quality) + Duration..weeks. +\n##\nfactor(Stress.Level))^2, data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -10.0955\n-0.5464\n0.0000\n0.9004\n9.1887\n##\n## Coefficients: (2 not defined because of singularities)\n##\nEstimate Std. Error t value\n## (Intercept)\n0.173136\n3.385089\n0.051\n## factor(Sleep.Quality)Fair\n0.630967\n4.305253\n0.147\n## factor(Sleep.Quality)Good\n-0.515159\n4.272899\n-0.121\n## factor(Sleep.Quality)Poor\n1.111739\n4.146862\n0.268\n## Duration..weeks.\n0.242288\n0.465477\n0.521\n## factor(Stress.Level)2\n0.403610\n4.337921\n0.093\n## factor(Stress.Level)3\n-1.956105\n3.862834\n-0.506\n## factor(Stress.Level)4\n-1.834648\n3.855770\n-0.476\n## factor(Stress.Level)5\n-4.257771\n4.621632\n-0.921\n## factor(Stress.Level)6\n0.391914\n3.849036\n0.102\n## factor(Stress.Level)7\n-2.320497\n4.601681\n-0.504\n## factor(Stress.Level)8\n1.171047\n5.307511\n0.221\n## factor(Stress.Level)9\n-3.418483\n3.543061\n-0.965\n## factor(Sleep.Quality)Fair:Duration..weeks.\n-0.009411\n0.393084\n-0.024\n## factor(Sleep.Quality)Good:Duration..weeks.\n0.045802\n0.439363\n0.104\n## factor(Sleep.Quality)Poor:Duration..weeks.\n-1.208733\n0.442719\n-2.730\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2 -1.610491\n4.861653\n-0.331\n## factor(Sleep.Quality)Good:factor(Stress.Level)2\n0.638105\n4.726767\n0.135\n## factor(Sleep.Quality)Poor:factor(Stress.Level)2 -0.271656\n5.459211\n-0.050\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3 -2.208279\n6.084939\n-0.363\n## factor(Sleep.Quality)Good:factor(Stress.Level)3 -3.121900\n5.144181\n-0.607\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3 -0.806757\n4.378219\n-0.184\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4 -1.857250\n7.660308\n-0.242\n## factor(Sleep.Quality)Good:factor(Stress.Level)4 -0.499270\n5.817921\n-0.086\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\nNA\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5 -0.155156\n5.133104\n-0.030\n## factor(Sleep.Quality)Good:factor(Stress.Level)5 -4.340225\n6.172293\n-0.703\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5 -0.442804\n4.533601\n-0.098\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6 -0.991839\n4.720897\n-0.210\n## factor(Sleep.Quality)Good:factor(Stress.Level)6 -2.044183\n5.054225\n-0.404\n## factor(Sleep.Quality)Poor:factor(Stress.Level)6 -3.334619\n4.372421\n-0.763\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7\n0.052776\n5.351148\n0.010\n## factor(Sleep.Quality)Good:factor(Stress.Level)7\n1.618175\n6.488306\n0.249\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7\n1.392698\n5.095646\n0.273\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8 -4.308983\n5.585545\n-0.771\n## factor(Sleep.Quality)Good:factor(Stress.Level)8 -3.359882\n5.572127\n-0.603\n29"
    },
    {
        "page": 248,
        "text": "## factor(Sleep.Quality)Poor:factor(Stress.Level)8 -4.244128\n5.215951\n-0.814\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9\n3.681779\n3.459445\n1.064\n## factor(Sleep.Quality)Good:factor(Stress.Level)9\n4.692112\n3.573977\n1.313\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\nNA\nNA\n## Duration..weeks.:factor(Stress.Level)2\n-0.043603\n0.388747\n-0.112\n## Duration..weeks.:factor(Stress.Level)3\n0.472603\n0.462794\n1.021\n## Duration..weeks.:factor(Stress.Level)4\n0.335902\n0.851910\n0.394\n## Duration..weeks.:factor(Stress.Level)5\n0.924639\n0.550379\n1.680\n## Duration..weeks.:factor(Stress.Level)6\n0.091339\n0.400593\n0.228\n## Duration..weeks.:factor(Stress.Level)7\n0.198939\n0.543024\n0.366\n## Duration..weeks.:factor(Stress.Level)8\n-1.079441\n0.518387\n-2.082\n## Duration..weeks.:factor(Stress.Level)9\n-1.245371\n0.377083\n-3.303\n##\nPr(>|t|)\n## (Intercept)\n0.95940\n## factor(Sleep.Quality)Fair\n0.88403\n## factor(Sleep.Quality)Good\n0.90448\n## factor(Sleep.Quality)Poor\n0.78965\n## Duration..weeks.\n0.60483\n## factor(Stress.Level)2\n0.92621\n## factor(Stress.Level)3\n0.61464\n## factor(Stress.Level)4\n0.63612\n## factor(Stress.Level)5\n0.36101\n## factor(Stress.Level)6\n0.91928\n## factor(Stress.Level)7\n0.61612\n## factor(Stress.Level)8\n0.82621\n## factor(Stress.Level)9\n0.33893\n## factor(Sleep.Quality)Fair:Duration..weeks.\n0.98099\n## factor(Sleep.Quality)Good:Duration..weeks.\n0.91736\n## factor(Sleep.Quality)Poor:Duration..weeks.\n0.00853 **\n## factor(Sleep.Quality)Fair:factor(Stress.Level)2\n0.74173\n## factor(Sleep.Quality)Good:factor(Stress.Level)2\n0.89312\n## factor(Sleep.Quality)Poor:factor(Stress.Level)2\n0.96050\n## factor(Sleep.Quality)Fair:factor(Stress.Level)3\n0.71809\n## factor(Sleep.Quality)Good:factor(Stress.Level)3\n0.54647\n## factor(Sleep.Quality)Poor:factor(Stress.Level)3\n0.85450\n## factor(Sleep.Quality)Fair:factor(Stress.Level)4\n0.80935\n## factor(Sleep.Quality)Good:factor(Stress.Level)4\n0.93193\n## factor(Sleep.Quality)Poor:factor(Stress.Level)4\nNA\n## factor(Sleep.Quality)Fair:factor(Stress.Level)5\n0.97600\n## factor(Sleep.Quality)Good:factor(Stress.Level)5\n0.48497\n## factor(Sleep.Quality)Poor:factor(Stress.Level)5\n0.92255\n## factor(Sleep.Quality)Fair:factor(Stress.Level)6\n0.83438\n## factor(Sleep.Quality)Good:factor(Stress.Level)6\n0.68748\n## factor(Sleep.Quality)Poor:factor(Stress.Level)6\n0.44899\n## factor(Sleep.Quality)Fair:factor(Stress.Level)7\n0.99217\n## factor(Sleep.Quality)Good:factor(Stress.Level)7\n0.80400\n## factor(Sleep.Quality)Poor:factor(Stress.Level)7\n0.78566\n## factor(Sleep.Quality)Fair:factor(Stress.Level)8\n0.44380\n## factor(Sleep.Quality)Good:factor(Stress.Level)8\n0.54905\n## factor(Sleep.Quality)Poor:factor(Stress.Level)8\n0.41940\n## factor(Sleep.Quality)Fair:factor(Stress.Level)9\n0.29194\n## factor(Sleep.Quality)Good:factor(Stress.Level)9\n0.19478\n## factor(Sleep.Quality)Poor:factor(Stress.Level)9\nNA\n## Duration..weeks.:factor(Stress.Level)2\n0.91111\n30"
    },
    {
        "page": 249,
        "text": "## Duration..weeks.:factor(Stress.Level)3\n0.31172\n## Duration..weeks.:factor(Stress.Level)4\n0.69492\n## Duration..weeks.:factor(Stress.Level)5\n0.09873 .\n## Duration..weeks.:factor(Stress.Level)6\n0.82050\n## Duration..weeks.:factor(Stress.Level)7\n0.71553\n## Duration..weeks.:factor(Stress.Level)8\n0.04207 *\n## Duration..weeks.:factor(Stress.Level)9\n0.00170 **\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 3.084 on 54 degrees of freedom\n## Multiple R-squared:\n0.9064, Adjusted R-squared:\n0.8284\n## F-statistic: 11.62 on 45 and 54 DF,\np-value: 7.362e-16\nBased on our output, we can see the significant variables (with a p-value <0.05) are \u201cfactor(Sleep.Quality)Poor:Duration..week\n\u201cDuration..weeks.:factor(Stress.Level)8\u201d, \u201cDuration..weeks.:factor(Stress.Level)9\u201d.\nAlthough none of the\ninitial predictors are considered significant, they are all parts of significant interaction terms, and\nmust therefore be included in the model, therefore our interaction model would include \u2018Sleep.Quality\u2019,\n\u2018Duration..weeks.\u2019, \u2018Stress.Level\u2019, \u2018Sleep.Quality\u2019:\u2018Duration..weeks.\u2019, and \u2018Duration..weeks.\u2019:\u2018Stress.Level\u2019.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + Duration..weeks. +\n##\nfactor(Stress.Level) + factor(Sleep.Quality) * Duration..weeks. +\n##\nDuration..weeks. * factor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -11.2934\n-0.7740\n0.0613\n0.9565\n8.9577\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n0.846572\n2.428365\n0.349\n## factor(Sleep.Quality)Fair\n-0.060956\n2.125053\n-0.029\n## factor(Sleep.Quality)Good\n-0.731905\n2.427593\n-0.301\n## factor(Sleep.Quality)Poor\n-0.876532\n2.005304\n-0.437\n## Duration..weeks.\n0.266336\n0.345765\n0.770\n## factor(Stress.Level)2\n-0.053039\n2.626820\n-0.020\n## factor(Stress.Level)3\n-1.509118\n2.582405\n-0.584\n## factor(Stress.Level)4\n-0.529393\n3.095647\n-0.171\n## factor(Stress.Level)5\n-2.710051\n2.845423\n-0.952\n## factor(Stress.Level)6\n-0.969032\n2.490558\n-0.389\n## factor(Stress.Level)7\n-2.717472\n3.578434\n-0.759\n## factor(Stress.Level)8\n-1.657444\n3.165466\n-0.524\n## factor(Stress.Level)9\n-0.269294\n2.631239\n-0.102\n## factor(Sleep.Quality)Fair:Duration..weeks. -0.051620\n0.307122\n-0.168\n## factor(Sleep.Quality)Good:Duration..weeks. -0.001166\n0.329358\n-0.004\n## factor(Sleep.Quality)Poor:Duration..weeks. -1.092225\n0.317033\n-3.445\n## Duration..weeks.:factor(Stress.Level)2\n-0.056465\n0.326628\n-0.173\n## Duration..weeks.:factor(Stress.Level)3\n0.222208\n0.324192\n0.685\n## Duration..weeks.:factor(Stress.Level)4\n0.058787\n0.396866\n0.148\n## Duration..weeks.:factor(Stress.Level)5\n0.568176\n0.397329\n1.430\n## Duration..weeks.:factor(Stress.Level)6\n0.042906\n0.318948\n0.135\n## Duration..weeks.:factor(Stress.Level)7\n0.312677\n0.395000\n0.792\n31"
    },
    {
        "page": 250,
        "text": "## Duration..weeks.:factor(Stress.Level)8\n-1.209145\n0.398373\n-3.035\n## Duration..weeks.:factor(Stress.Level)9\n-1.363695\n0.326722\n-4.174\n##\nPr(>|t|)\n## (Intercept)\n0.728340\n## factor(Sleep.Quality)Fair\n0.977191\n## factor(Sleep.Quality)Good\n0.763861\n## factor(Sleep.Quality)Poor\n0.663273\n## Duration..weeks.\n0.443521\n## factor(Stress.Level)2\n0.983944\n## factor(Stress.Level)3\n0.560692\n## factor(Stress.Level)4\n0.864669\n## factor(Stress.Level)5\n0.343901\n## factor(Stress.Level)6\n0.698303\n## factor(Stress.Level)7\n0.449961\n## factor(Stress.Level)8\n0.602079\n## factor(Stress.Level)9\n0.918752\n## factor(Sleep.Quality)Fair:Duration..weeks. 0.866971\n## factor(Sleep.Quality)Good:Duration..weeks. 0.997185\n## factor(Sleep.Quality)Poor:Duration..weeks. 0.000932 ***\n## Duration..weeks.:factor(Stress.Level)2\n0.863210\n## Duration..weeks.:factor(Stress.Level)3\n0.495164\n## Duration..weeks.:factor(Stress.Level)4\n0.882633\n## Duration..weeks.:factor(Stress.Level)5\n0.156819\n## Duration..weeks.:factor(Stress.Level)6\n0.893345\n## Duration..weeks.:factor(Stress.Level)7\n0.431065\n## Duration..weeks.:factor(Stress.Level)8\n0.003290 **\n## Duration..weeks.:factor(Stress.Level)9\n7.89e-05 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.822 on 76 degrees of freedom\n## Multiple R-squared:\n0.8897, Adjusted R-squared:\n0.8563\n## F-statistic: 26.65 on 23 and 76 DF,\np-value: < 2.2e-16\nOur final model, before testing for the rest of our assumptions, will include the following predictors and\ninteraction terms: Sleep.Quality, Duration..weeks., Stress.Level, Sleep.Quality:Duration..weeks., and Dura-\ntion..weeks.:Stress.Level.\nBased on this model, we can now test the rest of our assumptions. We will start with testing for linearity\nby inspecting our residual plots.\n## \u2018geom_smooth()\u2018 using method = \u2019loess\u2019 and formula = \u2019y ~ x\u2019\n32"
    },
    {
        "page": 251,
        "text": "\u221210\n\u22125\n0\n5\n\u221220\n\u221210\n0\n.fitted\n.resid\nBased on our residuals, there appears to be a pattern, with most data points on the right side, as well\nas the presence of some funnel shape. We can try to correct this by adding some polynomial terms, or\ntransforming the data. We will start with polynomial terms.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + Duration..weeks. +\n##\nI(Duration..weeks.^2) + factor(Stress.Level) + factor(Sleep.Quality) *\n##\nDuration..weeks. + Duration..weeks. * factor(Stress.Level),\n##\ndata = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -11.3791\n-0.7311\n-0.0065\n0.9976\n8.4209\n##\n## Coefficients:\n##\nEstimate Std. Error t value Pr(>|t|)\n## (Intercept)\n1.66345\n2.54941\n0.652 0.516083\n## factor(Sleep.Quality)Fair\n0.13760\n2.13220\n0.065 0.948715\n## factor(Sleep.Quality)Good\n-0.74349\n2.42611\n-0.306 0.760108\n## factor(Sleep.Quality)Poor\n-0.87654\n2.00406\n-0.437 0.663091\n## Duration..weeks.\n-0.09704\n0.48997\n-0.198 0.843543\n## I(Duration..weeks.^2)\n0.02828\n0.02704\n1.046 0.298892\n## factor(Stress.Level)2\n0.11348\n2.63001\n0.043 0.965698\n## factor(Stress.Level)3\n-1.46360\n2.58117\n-0.567 0.572388\n## factor(Stress.Level)4\n-0.59995\n3.09446\n-0.194 0.846794\n## factor(Stress.Level)5\n-2.49747\n2.85091\n-0.876 0.383813\n33"
    },
    {
        "page": 252,
        "text": "## factor(Stress.Level)6\n-1.06770\n2.49080\n-0.429 0.669402\n## factor(Stress.Level)7\n-2.34457\n3.59394\n-0.652 0.516159\n## factor(Stress.Level)8\n-1.37165\n3.17528\n-0.432 0.666996\n## factor(Stress.Level)9\n-0.38409\n2.63190\n-0.146 0.884363\n## factor(Sleep.Quality)Fair:Duration..weeks. -0.09137\n0.30928\n-0.295 0.768475\n## factor(Sleep.Quality)Good:Duration..weeks. -0.01614\n0.32946\n-0.049 0.961055\n## factor(Sleep.Quality)Poor:Duration..weeks. -1.09414\n0.31684\n-3.453 0.000914\n## Duration..weeks.:factor(Stress.Level)2\n-0.06645\n0.32656\n-0.203 0.839304\n## Duration..weeks.:factor(Stress.Level)3\n0.22627\n0.32401\n0.698 0.487124\n## Duration..weeks.:factor(Stress.Level)4\n0.08923\n0.39769\n0.224 0.823079\n## Duration..weeks.:factor(Stress.Level)5\n0.55714\n0.39722\n1.403 0.164866\n## Duration..weeks.:factor(Stress.Level)6\n0.05918\n0.31913\n0.185 0.853376\n## Duration..weeks.:factor(Stress.Level)7\n0.27307\n0.39657\n0.689 0.493213\n## Duration..weeks.:factor(Stress.Level)8\n-1.23041\n0.39864\n-3.086 0.002837\n## Duration..weeks.:factor(Stress.Level)9\n-1.36556\n0.32652\n-4.182 7.75e-05\n##\n## (Intercept)\n## factor(Sleep.Quality)Fair\n## factor(Sleep.Quality)Good\n## factor(Sleep.Quality)Poor\n## Duration..weeks.\n## I(Duration..weeks.^2)\n## factor(Stress.Level)2\n## factor(Stress.Level)3\n## factor(Stress.Level)4\n## factor(Stress.Level)5\n## factor(Stress.Level)6\n## factor(Stress.Level)7\n## factor(Stress.Level)8\n## factor(Stress.Level)9\n## factor(Sleep.Quality)Fair:Duration..weeks.\n## factor(Sleep.Quality)Good:Duration..weeks.\n## factor(Sleep.Quality)Poor:Duration..weeks. ***\n## Duration..weeks.:factor(Stress.Level)2\n## Duration..weeks.:factor(Stress.Level)3\n## Duration..weeks.:factor(Stress.Level)4\n## Duration..weeks.:factor(Stress.Level)5\n## Duration..weeks.:factor(Stress.Level)6\n## Duration..weeks.:factor(Stress.Level)7\n## Duration..weeks.:factor(Stress.Level)8\n**\n## Duration..weeks.:factor(Stress.Level)9\n***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.82 on 75 degrees of freedom\n## Multiple R-squared:\n0.8913, Adjusted R-squared:\n0.8565\n## F-statistic: 25.62 on 24 and 75 DF,\np-value: < 2.2e-16\n## \u2018geom_smooth()\u2018 using method = \u2019loess\u2019 and formula = \u2019y ~ x\u2019\n34"
    },
    {
        "page": 253,
        "text": "\u221210\n\u22125\n0\n5\n\u221220\n\u221210\n0\n.fitted\n.resid\nAdding a polynomial term appears to not be the solution, as not only does the residual plot appear the\nsame, but the adjust R-sqaured barely increases. There is no point to preventing our interpretation for such\na small increase.\nNext we will try performing a log transformation on Duration..weeks.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) +\n##\nfactor(Stress.Level) + factor(Sleep.Quality) * log(Duration..weeks.) +\n##\nlog(Duration..weeks.) * factor(Stress.Level), data = weight)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -12.5056\n-0.9419\n0.2044\n1.2345\n6.3533\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n0.27355\n3.42430\n0.080\n## factor(Sleep.Quality)Fair\n0.11340\n2.82176\n0.040\n## factor(Sleep.Quality)Good\n-2.57483\n3.44092\n-0.748\n## factor(Sleep.Quality)Poor\n1.58927\n2.86099\n0.555\n## log(Duration..weeks.)\n1.28194\n1.93076\n0.664\n## factor(Stress.Level)2\n0.81976\n3.55617\n0.231\n## factor(Stress.Level)3\n-2.72618\n3.42733\n-0.795\n## factor(Stress.Level)4\n-2.26130\n3.85645\n-0.586\n## factor(Stress.Level)5\n-4.49656\n4.26194\n-1.055\n35"
    },
    {
        "page": 254,
        "text": "## factor(Stress.Level)6\n-0.34971\n3.35196\n-0.104\n## factor(Stress.Level)7\n-2.58341\n5.06097\n-0.510\n## factor(Stress.Level)8\n2.24593\n4.16139\n0.540\n## factor(Stress.Level)9\n0.06974\n3.32780\n0.021\n## factor(Sleep.Quality)Fair:log(Duration..weeks.) -0.08017\n1.58629\n-0.051\n## factor(Sleep.Quality)Good:log(Duration..weeks.)\n0.94134\n1.84057\n0.511\n## factor(Sleep.Quality)Poor:log(Duration..weeks.) -5.61445\n1.68822\n-3.326\n## log(Duration..weeks.):factor(Stress.Level)2\n-0.63803\n1.84765\n-0.345\n## log(Duration..weeks.):factor(Stress.Level)3\n1.58105\n1.79470\n0.881\n## log(Duration..weeks.):factor(Stress.Level)4\n1.03392\n2.02085\n0.512\n## log(Duration..weeks.):factor(Stress.Level)5\n3.12719\n2.36324\n1.323\n## log(Duration..weeks.):factor(Stress.Level)6\n-0.05704\n1.79414\n-0.032\n## log(Duration..weeks.):factor(Stress.Level)7\n1.26398\n2.41255\n0.524\n## log(Duration..weeks.):factor(Stress.Level)8\n-6.77947\n2.11771\n-3.201\n## log(Duration..weeks.):factor(Stress.Level)9\n-6.13711\n1.75701\n-3.493\n##\nPr(>|t|)\n## (Intercept)\n0.93654\n## factor(Sleep.Quality)Fair\n0.96805\n## factor(Sleep.Quality)Good\n0.45659\n## factor(Sleep.Quality)Poor\n0.58019\n## log(Duration..weeks.)\n0.50873\n## factor(Stress.Level)2\n0.81831\n## factor(Stress.Level)3\n0.42884\n## factor(Stress.Level)4\n0.55936\n## factor(Stress.Level)5\n0.29475\n## factor(Stress.Level)6\n0.91718\n## factor(Stress.Level)7\n0.61121\n## factor(Stress.Level)8\n0.59098\n## factor(Stress.Level)9\n0.98333\n## factor(Sleep.Quality)Fair:log(Duration..weeks.)\n0.95983\n## factor(Sleep.Quality)Good:log(Duration..weeks.)\n0.61053\n## factor(Sleep.Quality)Poor:log(Duration..weeks.)\n0.00136 **\n## log(Duration..weeks.):factor(Stress.Level)2\n0.73081\n## log(Duration..weeks.):factor(Stress.Level)3\n0.38112\n## log(Duration..weeks.):factor(Stress.Level)4\n0.61040\n## log(Duration..weeks.):factor(Stress.Level)5\n0.18971\n## log(Duration..weeks.):factor(Stress.Level)6\n0.97472\n## log(Duration..weeks.):factor(Stress.Level)7\n0.60186\n## log(Duration..weeks.):factor(Stress.Level)8\n0.00200 **\n## log(Duration..weeks.):factor(Stress.Level)9\n0.00080 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.917 on 76 degrees of freedom\n## Multiple R-squared:\n0.8821, Adjusted R-squared:\n0.8464\n## F-statistic: 24.72 on 23 and 76 DF,\np-value: < 2.2e-16\n## \u2018geom_smooth()\u2018 using method = \u2019loess\u2019 and formula = \u2019y ~ x\u2019\n36"
    },
    {
        "page": 255,
        "text": "\u221210\n\u22125\n0\n5\n\u221220\n\u221210\n0\n.fitted\n.resid\nTransforming Duration..weeks.\nusing a log transformation appears to not correct the linearity problem\neither. In fact the residual plot now looks worse than before.\nBased on these attempts, we may need to add more complex polynomial or transformations to our data.\nWe may also need to attempt some other model types, but for the sake of this class we will move forward\nusing our model before we attempting correcting linearity, and simply state that our model fails the linearity\nassumption.\nThe next assumption we will look at is independence. As our responding variable, Weight.Change..lbs. is\nnot considered time-series data we can conclude this assumption has been passed.\nTo test for homoscedasticity we can look at our residual plots, scale-location plots, and run a Breusch-Pagan\nTest, using the following hypothesis:\nH0 : \u03c32\n1 = \u03c32\n2 = ... = \u03c32\nn (heteroscedasticity is not present)Ha : at least one \u03c32\ni is different from the others i = 1, 2, ..., n (he\n37"
    },
    {
        "page": 256,
        "text": "\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n\u221210\n\u22125\n0\n5\n10\nFitted values\nResiduals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + Duration..weeks. + factor( ...\nResiduals vs Fitted\n8\n38\n86\n38"
    },
    {
        "page": 257,
        "text": "\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n0.0\n0.5\n1.0\n1.5\n2.0\nFitted values\nStandardized residuals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + Duration..weeks. + factor( ...\nScale\u2212Location\n8\n38\n86\n##\n##\nstudentized Breusch-Pagan test\n##\n## data:\nweight_model_int_final\n## BP = 44.674, df = 23, p-value = 0.004356\nBased on our plots and the fact that our p-value for our Breusch-Pagan Test (0.004356) is < 0.05, we can\nreject our null hypothesis, implying the presence of heteroscedasticity.\nTo try and correct this, we can\nattempt a log transformation.\n39"
    },
    {
        "page": 258,
        "text": "\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n\u221210\n\u22125\n0\n5\nFitted values\nResiduals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + fa ...\nResiduals vs Fitted\n8\n38\n86\n40"
    },
    {
        "page": 259,
        "text": "\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n0.0\n0.5\n1.0\n1.5\n2.0\nFitted values\nStandardized residuals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + fa ...\nScale\u2212Location\n8\n38\n86\n##\n##\nstudentized Breusch-Pagan test\n##\n## data:\nweight_model_log\n## BP = 30.059, df = 23, p-value = 0.1477\nOur plots look much better now than before, and we can see that our p-value for our Breusch-Pagan Test\n(0.1477) is > 0.05. This means that we now fail to reject our null hypotheis, implying heteroscedasticity is\nnot present. This suggests that our log transformation has improved our model, allowing it to now pass the\nhomoscedasticity assumption. It should be noted that the adjusted R-square of our log model has decreased\nslightly from our previous model (0.8464 down from 0.8563). Despite this loss, we are comfortable moving\nforward with the log model, as it helps us pass one of our assumptions that was previously not met.\nTo test whether our residuals are normally distributed, we can inspect our Q-Q plot, as well as run a\nShapiro-Wilk test, using the following hypothesis:\nH0 : the residuals are normally distributedHa : the residuals are not normally distributed\n41"
    },
    {
        "page": 260,
        "text": "\u22122\n\u22121\n0\n1\n2\n\u22124\n\u22122\n0\n2\nTheoretical Quantiles\nStandardized residuals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + fa ...\nQ\u2212Q Residuals\n8\n38\n86\n##\n##\nShapiro-Wilk normality test\n##\n## data:\nresiduals(weight_model_log)\n## W = 0.85478, p-value = 1.788e-08\nBased on our Q-Q plot, we can see that the residuals are likely not normally distributed. This is further\nreinforced by our Shapiro Wilks test, which returned a p-value 1.788e-08. As this is < 0.05, we reject our null\nhypothesis, implying that the residuals are not normally distributed. We can attempt to correct this using\na Box-Cox transformation. Unfortunately, as our responding variable is weight change, there are values less\nthan 0. Therefore a Box-Cox transformation cannot be done. For the time being we will simply conclude\nthat our model has not passed the assumption of normality.\nFinally, we will check for outliers, using our residuals vs. leverage plot.\n42"
    },
    {
        "page": 261,
        "text": "0.0\n0.2\n0.4\n0.6\n0.8\n\u22124\n\u22122\n0\n2\nLeverage\nStandardized residuals\nlm(Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) + fa ...\nCook's distance\n1\n0.5\n0.5\n1\nResiduals vs Leverage\n49\n8\n72\nBased on our plot, we can see that observation 49 is an outlier. We can attempt to remove the data point\nto see how it affects our model.\n##\n## Call:\n## lm(formula = Weight.Change..lbs. ~ factor(Sleep.Quality) + log(Duration..weeks.) +\n##\nfactor(Stress.Level) + factor(Sleep.Quality) * log(Duration..weeks.) +\n##\nlog(Duration..weeks.) * factor(Stress.Level), data = weight_clean)\n##\n## Residuals:\n##\nMin\n1Q\nMedian\n3Q\nMax\n## -12.7111\n-0.8341\n0.1531\n1.1537\n8.3484\n##\n## Coefficients:\n##\nEstimate Std. Error t value\n## (Intercept)\n0.21766\n3.39093\n0.064\n## factor(Sleep.Quality)Fair\n0.18485\n2.79447\n0.066\n## factor(Sleep.Quality)Good\n-1.59689\n3.46262\n-0.461\n## factor(Sleep.Quality)Poor\n1.24320\n2.84135\n0.438\n## log(Duration..weeks.)\n1.21315\n1.91233\n0.634\n## factor(Stress.Level)2\n0.54944\n3.52544\n0.156\n## factor(Stress.Level)3\n-2.44803\n3.39827\n-0.720\n## factor(Stress.Level)4\n-1.93746\n3.82412\n-0.507\n## factor(Stress.Level)5\n-4.19952\n4.22433\n-0.994\n## factor(Stress.Level)6\n-0.32160\n3.31916\n-0.097\n## factor(Stress.Level)7\n-2.49840\n5.01166\n-0.499\n## factor(Stress.Level)8\n13.14533\n8.01703\n1.640\n43"
    },
    {
        "page": 262,
        "text": "## factor(Stress.Level)9\n-0.12103\n3.29738\n-0.037\n## factor(Sleep.Quality)Fair:log(Duration..weeks.)\n-0.06075\n1.57079\n-0.039\n## factor(Sleep.Quality)Good:log(Duration..weeks.)\n0.55598\n1.83868\n0.302\n## factor(Sleep.Quality)Poor:log(Duration..weeks.)\n-5.29987\n1.68341\n-3.148\n## log(Duration..weeks.):factor(Stress.Level)2\n-0.46614\n1.83275\n-0.254\n## log(Duration..weeks.):factor(Stress.Level)3\n1.45515\n1.77888\n0.818\n## log(Duration..weeks.):factor(Stress.Level)4\n0.89513\n2.00296\n0.447\n## log(Duration..weeks.):factor(Stress.Level)5\n2.98467\n2.34180\n1.275\n## log(Duration..weeks.):factor(Stress.Level)6\n-0.02623\n1.77666\n-0.015\n## log(Duration..weeks.):factor(Stress.Level)7\n1.23485\n2.38898\n0.517\n## log(Duration..weeks.):factor(Stress.Level)8\n-12.05261\n3.93280\n-3.065\n## log(Duration..weeks.):factor(Stress.Level)9\n-6.05003\n1.74066\n-3.476\n##\nPr(>|t|)\n## (Intercept)\n0.948991\n## factor(Sleep.Quality)Fair\n0.947436\n## factor(Sleep.Quality)Good\n0.646004\n## factor(Sleep.Quality)Poor\n0.662978\n## log(Duration..weeks.)\n0.527760\n## factor(Stress.Level)2\n0.876570\n## factor(Stress.Level)3\n0.473533\n## factor(Stress.Level)4\n0.613893\n## factor(Stress.Level)5\n0.323358\n## factor(Stress.Level)6\n0.923071\n## factor(Stress.Level)7\n0.619579\n## factor(Stress.Level)8\n0.105261\n## factor(Stress.Level)9\n0.970818\n## factor(Sleep.Quality)Fair:log(Duration..weeks.) 0.969254\n## factor(Sleep.Quality)Good:log(Duration..weeks.) 0.763198\n## factor(Sleep.Quality)Poor:log(Duration..weeks.) 0.002357 **\n## log(Duration..weeks.):factor(Stress.Level)2\n0.799930\n## log(Duration..weeks.):factor(Stress.Level)3\n0.415939\n## log(Duration..weeks.):factor(Stress.Level)4\n0.656233\n## log(Duration..weeks.):factor(Stress.Level)5\n0.206415\n## log(Duration..weeks.):factor(Stress.Level)6\n0.988259\n## log(Duration..weeks.):factor(Stress.Level)7\n0.606752\n## log(Duration..weeks.):factor(Stress.Level)8\n0.003028 **\n## log(Duration..weeks.):factor(Stress.Level)9\n0.000851 ***\n## ---\n## Signif. codes:\n0 \u2019***\u2019 0.001 \u2019**\u2019 0.01 \u2019*\u2019 0.05 \u2019.\u2019 0.1 \u2019 \u2019 1\n##\n## Residual standard error: 2.889 on 75 degrees of freedom\n## Multiple R-squared:\n0.8859, Adjusted R-squared:\n0.8509\n## F-statistic: 25.32 on 23 and 75 DF,\np-value: < 2.2e-16\nBased on the output of our model with and without the outlier, we can see that removing the outlier slightly\nimproved our adjusted R-squared (0.8509 from 0.8464), while reducing or RSE (2.889 from 2.917), without\nchanging the significance of any of our predictors. Based on this we feel comfortable removing the indicated\noutlier.\nAlthough our model does fail the assumptions of linearity and normality, for the sake of this project we will\ngo forward with our final model being:\nXW eight.Change..lbs. = 0.21766+0.18485XSleep.QualityF air\u22121.59689XSleep.QualityGood+1.24320XSleep.QualityP oor+1.21315Xlog(\n44"
    },
    {
        "page": 263,
        "text": "Now that we have our final model, we can begin interpreting it.\nIntercept: When sleep quality is excellent, stress level is 1, and log(duration) is 0 (which would be a duration\nof 1 week), the expected weight change is 0.21766 pounds.\nSleep Quality:\n\u2022 If sleep quality is fair, the predicted weight change is (0.18485 \u2212 0.06075Xlog(Duration..weeks.)) pounds.\n\u2022 If sleep quality is good, the predicted weight change is (\u22121.59689 + 0.55598Xlog(Duration..weeks.))\npounds.\n\u2022 If sleep quality is poor, the predicted weight change is (1.24320\u22125.29987Xlog(Duration..weeks.)) pounds.\n\u2022 If sleep quality is excellent, the predicted weight change would simply be our model equation excluding\nany Sleep.Qualityi terms or interactions, where i = Fair, Good, or Poor, as all values would simply be\n0 for those variables.\nDuration in weeks:\n\u2022 For every one-unit increase in log(Duration..weeks.), weight change increases by:\n(1.21315\u22120.06075XSleep.QualityF air+0.55598XSleep.QualityGood\u22125.29987XSleep.QualityP oor\u22120.46614XStress.Level2+1.455\nStress Level:\n\u2022 If stress level is 2, the predicted weight change is (0.54944 \u2212 0.46614Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 3, the predicted weight change is (\u22122.44803 + 1.45515Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 4, the predicted weight change is (\u22121.93746 + 0.89513Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 5, the predicted weight change is (\u22124.19952 + 2.98467Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 6, the predicted weight change is (\u22120.32160 \u2212 0.02623Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 7, the predicted weight change is (\u22122.49840 + 1.23485Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 8, the predicted weight change is (13.14533 \u2212 12.05261Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 9, the predicted weight change is (\u22120.12103 \u2212 6.05003Xlog(Duration..weeks.)) pounds.\n\u2022 If stress level is 1, the predicted weight change would simply be our model equation excluding any\nStress.Leveli terms or interactions, where i = 2 to 9, as all values would simply be 0 for those\nvariables.\nLastly, our Adjusted R2 for our model is 0.8509, implying that our 85.09% of the variability in weight change\nis explained by our model.\nTo demonstrate the predictiveness of our model, we have included an example. Assume a participant records\ntheir weight change over a period of 6 weeks, where they recorded poor sleep quality, and a stress level of 5.\nWhat would be there predicted weight change?\n##\n1\n## -4.713261\nBased on our finalized model, and assuming a participant records their weight change over a period of 6\nweeks, where they recorded poor sleep quality, and a stress level of 5, we would predict their weight to\ndecrease by 4.713261 pounds.\n45"
    },
    {
        "page": 264,
        "text": "4 Conclusion and Discussion\n4.1 Approach\n4.2 Future Work\n5 References\n1. THE WEBSITE FOR THE DATASET\n46"
    },
    {
        "page": 265,
        "text": "Unlocking Insights in Calgary Child \nCare Program Compliance: An \nAnalytical Approach\nSri Harsha Tuttaganti\nVenkateshwaran Balu Soundararajan\nGroup 39\n1"
    },
    {
        "page": 266,
        "text": "Introduction\n2\n\u2751 Childcare is essential for early childhood \ndevelopment.\n\u2751 Provides a foundation for learning, \nsocialization, and well-being.\n\u2751 High-quality childcare significantly \nimpacts a child's growth and future \nsuccess."
    },
    {
        "page": 267,
        "text": "3\n\u27a2 Parents: Searching for reliable childcare options.\n\u27a2 Researchers: Analyzing childcare availability and \ndistribution.\n\u27a2 Policymakers: Evaluating childcare needs and planning \nTarget Audience\n\u2756 Utilize the Calgary Childcare Information dataset.\n\u2756 Analyze licensed childcare programs.\n\u2756 Focus on capacity, compliance status, and \ninspection history.\n\u2756 Ensure high standards of care for children in \nCalgary resources.\nObjective"
    },
    {
        "page": 268,
        "text": "Child Care Information Dataset\nhttps://data.calgary.ca/Health-and-Safety/Child-Care-\nInformation/qdxh-qngy/about_data\nOverview\n\u25aa Dataset Name: Child Care Information\n\u25aa Source: City of Calgary Open Data Portal\n\u25aa Description:\nThis dataset provides information about \nlicensed Child Care facilities in Calgary. It \nincludes details such as facility names, \nlocations, types of care offered, capacity, \nand contact information. (2014-2023)\n4"
    },
    {
        "page": 269,
        "text": "Data Fields and Its Significance\n5\nField Name\nData Type\nSignificance\nProgram Name\nText\nUniquely identifies each program\nType of Program\nText\nCategorizes programs for analysis\nProgram Address\nText\nProvides location details\nProgram City\nText\nIdentifies the city for location-based analysis\nPostal Code\nText\nUsed for geographic and demographic analysis\nPhone Number\nText\nProvides a means of contact for inquiries\nCapacity\nNumber\nIndicates the program's scale and capacity\nInspection Date\nFloating Timestamp\nTracks the timeline of inspections\nInspection Reason\nText\nProvides insights into compliance and regulatory focus areas\nNon Compliance\nText\nAssesses compliance status and identifies areas needing improvement\nEnforcement Action\nText\nDetails corrective actions implemented\nRemedy Date\nFloating Timestamp\nTracks deadlines for resolving non-compliance issues"
    },
    {
        "page": 270,
        "text": "Data Model with Supporting Datasets\n6"
    },
    {
        "page": 271,
        "text": "Data Cleaning and Transformation\n7\nStep 1 : Cleaned the data by removing duplicate values.\nStep 2 : Added a dataset containing latitude and longitude information.\nStep 3 : Removed duplicates caused by merging location dataset.\nStep 4 : Addressed normalization issues by cross-checking addresses in both datasets and \nrectifying discrepancies.\nStep 5 : Added another dataset to group data into communities and Quadrants."
    },
    {
        "page": 272,
        "text": "Data Preprocessing- Iterative Imputation\n8\nStep 1: Removed rows containing null values\n\u2022\nCreated a cleaned dataset by removing rows with null values.\nStep 2: Split the cleaned dataset\n\u2022\nDivided the cleaned dataset into test (20%) and train (80%) datasets.\nStep 3: Applied iterative imputation using random forest\n\u2022\nUsed the random forest model for iterative imputation to predict missing values.\nStep 4: Integrated the predicted values\n\u2022\nIntegrated the predicted values back into the original dataset.\nStep 5: Performed imputation on specific columns\n\u2022\nApplied the imputation process on the \"Program Name\" and \"Type of Program\" columns.\nType of Program column:\n\u2022\nAccuracy: 0.9126\nProgram Name column:\n\u2022\nAccuracy: 0.9517"
    },
    {
        "page": 273,
        "text": "GUIDING \nQUESTIONS\n9\n7\nProgram Types\n1004\nUnique Programs\n14\nInspection Types"
    },
    {
        "page": 274,
        "text": "Program Types and its distribution in Calgary\n10\nWhat is the average capacity of child care programs by its types, how does it vary by its Quadrant \nInsights:\n\u2022\nHighest Average Capacity: Across all \nquadrants, the FAMILY DAY HOME program \nconsistently has the highest average \ncapacity, indicating its dominance in each \nregion.\n\u2022\nVariation Across Quadrants: The average \ncapacity of program types varies \nsignificantly across different quadrants.\n\u2022\nProgram Diversity: Each quadrant offers a \nmix of program types, with notable \nvariation in their capacities."
    },
    {
        "page": 275,
        "text": "Year on Year Inspections and its Non- Compliance Trends\n11\nHow do the average number of inspections and the rate of non-compliance change year-on-year across \ndifferent childcare programs in Calgary?\nInsights:\n\u2022\n2016-2019 Increase: There was a \nnoticeable increase in non-\ncompliance percentages from \n2016 to 2019.\n\u2022\n2020 Dip: Both inspection counts \nand non-compliance percentages \ndipped significantly in 2020.\n\u2022\nPost-2020 Recovery: There was a \nrecovery in inspection counts \npost-2020, but non-compliance \npercentages remain fluctuating."
    },
    {
        "page": 276,
        "text": "Inspections Reasons frequency with Program Type\n12\nWhat are the most common reasons for \ninspections across different types of \nchildcare programs in Calgary?\nInsights:\nHigh-Frequency Inspections: INSPECTION and \nCOMPLAINT INVESTIGATION are common across \nmost program types, indicating regular oversight \nand response to complaints.\nProgram-Specific Trends: Certain program types \nlike Facility-Based Programs and Family Day \nHomes are more frequently inspected.\nFocus Areas: The high counts in FOLLOW UP TO \nENFORCEMENT ACTION and RENEWAL LICENCE \nINSPECTION suggest these are critical areas \nneeding close monitoring."
    },
    {
        "page": 277,
        "text": "Inspection Frequency (in Days) V/S Program Type\n13\nIs there a correlation between the \ntype of childcare program and the \nfrequency of inspections?\nInsights:\n\u2022\nFrequent Inspections: Innovative Child Care \nPrograms experience more frequent and \nconsistent inspections compared to other \nprogram types.\n\u2022\nLess Frequent Inspections: Facility-Based \nPrograms have the least frequent and most \nvariable inspection intervals, indicating less \nregulatory scrutiny."
    },
    {
        "page": 278,
        "text": "Understanding the Non-Compliance% spread by Quadrant \nfor better Choices\n14\nHow is the Non-Compliance Spread across the four Quadrants and Communities of the city ?\nInsights:\n\u2022\nHighest Non-Compliance Rate: The SE \n(Southeast) quadrant has the highest non-\ncompliance rate at 28.67%, indicating more \nfrequent issues in this area.\n\u2022\nFrequent Inspections: The SW (Southwest) \nquadrant has the highest number of \ninspections (10,909), highlighting intensive \nmonitoring."
    },
    {
        "page": 279,
        "text": "Program Inspections and Non-Compliance against Communities\n15\nIdentify the Top-10 communities with lowest and highest compliance rates"
    },
    {
        "page": 280,
        "text": "Analyzing Regulatory Compliance \u2013License Cancellation\n16\nHow does the certain program types or locations have more enforcement actions pertaining to licence \ncancellation and suspension ?\nInsights:\n\u2022\nFacility-Based Programs face significant \nenforcement actions, primarily \nsuspensions.\n\u2022\nWhile Pre-School Programs have the \nfewest actions, indicating relatively \nbetter compliance. \n\u2022\nOut of School Care Programs mostly \nencounter conditions on their licenses."
    },
    {
        "page": 281,
        "text": "MACHINE \nLEARNING\n17"
    },
    {
        "page": 282,
        "text": "Text Analysis \u2013 Identification of Reasons in Non-\nCompliance Programs\n18\nInnovative Childcare Program has no data"
    },
    {
        "page": 283,
        "text": "Sentimental Analysis on Non - Compliance\n19\nFrequent words used in Programs"
    },
    {
        "page": 284,
        "text": "LDA Topic Modeling and its insights on Non-Compliance \nReasons\n20\nTopic 1: Staffing & Supervision Compliance Issues\nKey Terms:\n\u2022\nchild, record, staff, administration, attendance, \nsupervision, requirement, criminal, licence, \nholder, check\nNon-compliance issues may involve:\n\u2022\nInsufficient supervision of children.\n\u2022\nFailure to maintain proper attendance records.\n\u2022\nIssues with staff qualifications, criminal \nbackground checks, and required licensing.\nSuggestion:\n\u2022\nChildcare program should  focus on ensuring \nqualified, well-trained staff for childcare."
    },
    {
        "page": 285,
        "text": "LDA Insights\n21\nTopic 2: Childcare Facility & Safety Regulations\nKey Terms:\n\u2022\nprogram, child, care, day, keeping, schedule, \nratio, school, equipment, children, repair\nNon-compliance issues may involve:\n\u2022\nViolation of child-to-staff ratios.\n\u2022\nPoor maintenance of facilities and equipment.\n\u2022\nFailure to follow proper schedules and care \nprograms.\nSuggestion:\n\u2022\nChildcare should prioritize safety, well-\nmaintained environments for child \ndevelopment."
    },
    {
        "page": 286,
        "text": "LDA insights\n22\nTopic 3: Medication & Hygiene Compliance\nKey Terms: \n\u2022\nmedication, diapering, change, incident, \nadministered, procedure, washing, reporting, \nnutrition, stored\nNon-compliance issues may involve:\n\u2022\nImproper storage or administration of medication.\n\u2022\nLack of hygiene standards in diapering and \nhandwashing.\n\u2022\nFailure to report incidents related to medication or \nhealth.\nSuggestion:\n\u2022\nThis highlights the importance of strict health and \nsafety protocols in childcare environments."
    },
    {
        "page": 287,
        "text": "LDA Over the Time\n23\nAre there trends or patterns in identified topics over time?"
    },
    {
        "page": 288,
        "text": "Predictive Analysis of Non-Compliance: \n24\nCan we predict non-compliance likelihood based on program attributes and inspection history, and \nwhat features indicate compliance status?\nData Preparation\nBinary Target Column: Created a \nbinary target column \n(Non_Compliance_Indicator) to \nindicate non-compliance.)\nHandling Missing Values: \nDropped rows with missing \nvalues in selected columns to \nensure data quality.\nFeature Selection: Selected \nrelevant columns for modeling.\n \nCategorical Encoding\nOne-Hot Encoding: Used one-\nhot encoding to convert \ncategorical variables into a \nformat suitable for machine \nlearning algorithms.\nData Splitting: Split the data into \ntraining and testing sets.\n \nSMOTE\nSMOTE Application: Applied \nSMOTE (Synthetic Minority \nOver-sampling Technique) to \noversample the minority class..\n \nTraining the Model: Trained a \nlogistic regression model on the \nresampled training data. \nCode for Prediction\nCode to Encode and \nPredict\nLogistic Regression Model"
    },
    {
        "page": 289,
        "text": "Model Results and Predictions\n25\nPrediction Scenarios\nScenario 1: Group Family Child Program\n\u2022Capacity: 10\n\u2022Type of Program: Group Family Child Program\n\u2022Postal Code: T2M3T4 \n\u2022Prediction: [0] Likelihood of Non-Compliance: Unlikely\nScenario 2: Innovative Child Care Program\n\u2022Capacity: 6050\n\u2022Type of Program: Innovative Child Care Program\n\u2022Postal Code: T2M1L9 \n\u2022Prediction: Predicted likelihood of non-compliance.\n\u2022Prediction: [1] Likelihood of Non-Compliance: Likely"
    },
    {
        "page": 290,
        "text": "Conclusion\n26\n\u2751 Key Non-Compliance Issues Identified.\n\u2751 Non-Compliance Trends by Quadrant.\n\u2751 Yearly Trends in Non-Compliance & Inspections.\n\u2751 We identified key inspection reasons from LDA model\n\u2751 Predictive Modeling on non-compliance likelihood.\n\u2751 Refine Predictive Models \u2013 Improve non-compliance forecasting accuracy.\n\u2751 Policy Recommendations \u2013 Use trends to guide better regulations.\n\u2751 Community Engagement \u2013 Provide recommendations to Conduct training programs for \nchildcare providers \nFuture Steps"
    },
    {
        "page": 291,
        "text": "References\n27\n[1] Government of Alberta, \"Childcare,\" Alberta.ca, 2025. [Online]. Available: \nhttps://www.alberta.ca/child-care.\n[2] City of Calgary, \"Communities by Sector,\" Data Calgary, 2025. [Online]. Available: \nhttps://data.calgary.ca/Base-Maps/Communities-by-Sector/e6xg-kaxf. \n[3] City of Calgary, \"City Quadrants,\" Data Calgary, 2025. [Online]. Available: \nhttps://data.calgary.ca/Base-Maps/City-Quadrants/g8ma-syw. \n[4] Statistics Canada, \"Open Database of Addresses (ODA),\" Government of Canada, 2025. [Online]. \nAvailable: https://www.statcan.gc.ca/en/lode/databases/oda."
    },
    {
        "page": 292,
        "text": "Thank You! \n28"
    },
    {
        "page": 293,
        "text": "socialmediapurchaseinfluence\nJuly 13, 2025\n1\nThe Influence of Social Media Usage on Consumer Purchasing\nDecisions\nGroup Members\n1. Ayush Senthil Nelli\n2. Hritvik Gaind\n3. Rehan Chanegaon\n4. Satyam Kapoor\n5. Venkateshwaran Balu Soundararajan\n[2]: #Importing Required Libraries\nimport pandas as pd\nimport plotly.express as px\nimport scipy.stats as stats\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import chi2\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Disable all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.mode.chained_assignment = None\n1.1\nIntroduction\nIn today\u2019s digital era, social media plays a pivotal role in shaping consumer behavior and influencing\npurchasing decisions.\nWith platforms like Instagram, Facebook, and TikTok at the forefront,\nbrands are increasingly leveraging digital marketing strategies to capture attention and build trust.\nThis report investigates how social media influences consumer choices, explores the impact of\n1"
    },
    {
        "page": 294,
        "text": "digital content and influencer recommendations, and highlights data-driven approaches that help\nbusinesses optimize their marketing strategies.\nBy analyzing survey data collected from active\nsocial media users, the study aims to provide insights that can support more targeted and effective\nmarketing efforts.\n1.2\nBackground and Significance\nSocial media has revolutionized the way consumers interact with brands and make purchasing\ndecisions. The ubiquitous presence of platforms such as Instagram, Facebook, and TikTok has not\nonly altered consumer expectations but also redefined marketing practices. Brands now rely on\ndigital marketing, influencer promotions, and user-generated content to attract buyers. Moreover,\nconsumers increasingly trust online reviews and recommendations, which serve as powerful social\nproof before making purchases.\nKey aspects of this transformation include:\nUnderstanding Consumer Behavior: Gaining insights into how consumers interact with digital\ncontent allows brands to better align their products and messaging with audience preferences.\nImpact of Influencers & Reviews: The rise of influencer marketing and the proliferation of\nonline reviews have made it possible to measure the tangible effects of social proof on purchasing\ndecisions.\nData-Driven Marketing Strategies: Leveraging consumer data enables businesses to optimize\ntheir advertisements and engagement strategies, ensuring that marketing efforts are both e\ufb00icient\nand effective.\nIdentifying Trends & Preferences: Continuous analysis of consumer behavior helps identify\nemerging trends and preferences, which is crucial for maintaining competitive advantage in a rapidly\nevolving market.\nThis project underscores the importance of data-driven insights in understanding and adapting to\nthe shifting dynamics of consumer behavior in the digital age.\n1.3\nAbout the Dataset\nThe analysis is grounded in a custom-designed survey aimed at capturing the nuances of social\nmedia\u2019s influence on consumer behavior. Below are the key details of the dataset:\nPrimary Data Source:\nA custom-designed survey administered via Google Forms.\nlink -\nhttps://forms.gle/mhwQoduno3mTWTQ26\nTarget Audience: The survey focused on consumers from the University of Calgary, covering a\ndiverse age range of individuals who actively use social media for making shopping decisions.\nResponse Count: The dataset comprises responses from 125 participants.\nSurvey Structure: The survey was structured into several sections:\nDemographics: Collecting data on age, gender, etc.\nSocial Media Usage: Assessing frequency of use, preferred platforms, and daily time spent.\nAdvertisement Exposure: Evaluating the impact of social media ads, influencer recommenda-\ntions, and brand collaborations.\n2"
    },
    {
        "page": 295,
        "text": "Purchasing Behavior: Documenting the types of products purchased through social media, trust\nin online reviews, and key decision-making factors.\nInfluencing Factors: Identifying the type of content, limited time offers, and other factors influ-\nencing consumer decisions.\nData Cleaning & Stratified Sampling: To ensure a balanced analysis, the dataset underwent\nseveral cleaning steps:\nRenaming Columns: Survey questions were renamed to more appropriate column names.\nStratified Sampling: - Data was segmented into four age groups: Below 18, 18\u201336, 37\u201354, and\nAbove 55. - The minimum count among these groups was identified to allow proportional allocation.\n- Random resampling with replacement was performed within each group to ensure equal sample\nsizes. - Finally, the resampled data was combined and shuffled to maintain randomness.\nThe careful design and cleaning of the dataset ensure that the analysis accurately reflects consumer\nbehavior across different age groups, providing a reliable foundation for data-driven marketing in-\nsights.\n[3]: df = pd.read_excel('FinalSurveyDataa.xlsx')\ndf_original=pd.read_csv('606Project_Original.csv')\n#df.head(5)\n#df_original=df.copy()\n#df = pd.read_csv(\"Data_606_Project.csv\")\n#df.drop('Unnamed: 0', axis=1, inplace=True)\ndf.columns\n[3]: Index(['Timestamp', 'What is your age group?', 'What is your gender?',\n'On average, how many hours do you spend on social media daily?',\n'Which social media platform do you use most frequently?',\n'How often do you use this platform?',\n'How often do you see product advertisements on social media?',\n'How often do you encounter promotions by influencers on social media?',\n'How many times in the past month have you purchased a product after\nseeing it on social media?',\n'How often do you make unplanned purchases after seeing a product on\nsocial media?',\n'Do you regret purchases made impulsively due to social media\ninfluence?',\n'Which type of content influences your purchasing decisions the most?',\n'How often do you purchase a product based on a limited time offer on\nsocial media?',\n'Do you usually compare products on other websites/apps before buying\nsomething seen on social media?',\n'If you compare products, what factor influences your choice the most?\n(Select Not Applicable if previous question choices as No)',\n'Have you ever purchased a product to fit in with trends seen on social\nmedia?',\n'How often do peers or social trends influence your purchasing decisions\n3"
    },
    {
        "page": 296,
        "text": "on social media?',\n'Do you feel that social media increases your spending on products?',\n'How do you typically pay for purchases influenced by social media?',\n'How often do you exceed your budget because of products seen on social\nmedia?',\n'Have you ever taken a loan or gone into debt to purchase a product\nadvertised on social media?',\n'How trustworthy do you find social media advertisements?',\n'Are you more likely to trust ads promoted by influencers or official\nbrand accounts?',\n'Email address'],\ndtype='object')\n1.4\nData Cleaning\n[4]: df.rename(columns={\n\"What is your age group?\": \"age_group\",\n\"What is your gender?\": \"gender\",\n\"On average, how many hours do you spend on social media daily?\":\u2423\n\u21aa\"social_media_hours_per_day\",\n\"Which social media platform do you use most frequently?\":\u2423\n\u21aa\"most_frequent_platform\",\n\"How often do you use this platform?\": \"platform_usage_frequency\",\n\"How often do you see product advertisements on social media?\":\u2423\n\u21aa\"ad_frequency_on_social_media\",\n\"How often do you encounter promotions by influencers on social media?\":\u2423\n\u21aa\"influencer_promotions_frequency\",\n\"How many times in the past month have you purchased a product after seeing it\u2423\n\u21aaon social media?\": \"purchased_after_seeing_on_social_media\",\n\"How often do you make unplanned purchases after seeing a product on social\u2423\n\u21aamedia?\": \"unplanned_purchases_after_social_media\",\n\"Do you regret purchases made impulsively due to social media influence?\":\u2423\n\u21aa\"regret_impulsive_purchases\",\n\"Which type of content influences your purchasing decisions the most?\":\u2423\n\u21aa\"content_type_influences_purchasing\",\n\"How often do you purchase a product based on a limited time offer on social\u2423\n\u21aamedia?\": \"limited_time_offer_purchases\",\n\"Do you usually compare products on other websites/apps before buying something\u2423\n\u21aaseen on social media?\": \"product_comparison_before_purchasing\",\n\"If you compare products, what factor influences your choice the most? (Select\u2423\n\u21aaNot Applicable if previous question choices as No)\": \"comparison_factor\",\n\"Have you ever purchased a product to fit in with trends seen on social media?\":\n\u21aa \"purchased_to_fit_in_with_trends\",\n\"How often do peers or social trends influence your purchasing decisions on\u2423\n\u21aasocial media?\": \"peer_influence_on_purchasing\",\n4"
    },
    {
        "page": 297,
        "text": "\"Do you feel that social media increases your spending on products?\":\u2423\n\u21aa\"social_media_increases_spending\",\n\"How do you typically pay for purchases influenced by social media?\":\u2423\n\u21aa\"payment_method\",\n\"How often do you exceed your budget because of products seen on social media?\":\n\u21aa \"exceed_budget_due_to_social_media\",\n\"Have you ever taken a loan or gone into debt to purchase a product advertised\u2423\n\u21aaon social media?\": \"loan_for_social_media_purchases\",\n\"How trustworthy do you find social media advertisements?\":\u2423\n\u21aa\"trustworthiness_of_social_media_ads\",\n\"Are you more likely to trust ads promoted by influencers or official brand\u2423\n\u21aaaccounts?\": \"influencer_vs_brand_ads_trust\"\n}, inplace=True)\n[5]: df.isna().sum()\n[5]: Timestamp\n0\nage_group\n0\ngender\n0\nsocial_media_hours_per_day\n0\nmost_frequent_platform\n0\nplatform_usage_frequency\n0\nad_frequency_on_social_media\n0\ninfluencer_promotions_frequency\n0\npurchased_after_seeing_on_social_media\n0\nunplanned_purchases_after_social_media\n0\nregret_impulsive_purchases\n0\ncontent_type_influences_purchasing\n1\nlimited_time_offer_purchases\n0\nproduct_comparison_before_purchasing\n0\ncomparison_factor\n0\npurchased_to_fit_in_with_trends\n0\npeer_influence_on_purchasing\n0\nsocial_media_increases_spending\n0\npayment_method\n1\nexceed_budget_due_to_social_media\n0\nloan_for_social_media_purchases\n0\ntrustworthiness_of_social_media_ads\n0\ninfluencer_vs_brand_ads_trust\n0\nEmail address\n41\ndtype: int64\n[6]: df['age_group'].value_counts()\n[6]: age_group\n18 - 36\n80\nAbove 55\n33\n5"
    },
    {
        "page": 298,
        "text": "Below 18\n28\n37 - 54\n23\nName: count, dtype: int64\n[7]: df['gender'].value_counts()\n[7]: gender\nMale\n85\nFemale\n79\nName: count, dtype: int64\n[8]: df['most_frequent_platform'] = df['most_frequent_platform'].str.lower()\n# To standardize the names like \"YouTube\" or \"Youtube\"\n1.5\nStratified Sampling\n[9]: # Count the number of samples in each gender group\nage_group_counts = df['age_group'].value_counts()\nprint(\"Original Age Group Distribution:\\n\", age_group_counts)\n# Separate by gender (stratify by 'gender')\ndf_18_36 = df[df['age_group'] == '18 - 36']\ndf_above_55 = df[df['age_group'] == 'Above 55']\ndf_below_18 = df[df['age_group'] == 'Below 18']\ndf_37_54 = df[df['age_group'] == '37 - 54']\n# Perform bootstrapping within each gender group to balance sample sizes\n# Get the minimum count between Male and Female for proportional allocation\nmin_count = min(age_group_counts)\n# Apply bootstrapping for Male and Female to make sample sizes equal\nbootstrap_18_36 = df_18_36.sample(n=min_count, replace=True)\nbootstrap_above_55 = df_above_55.sample(n=min_count, replace=True)\nbootstrap_below_18 = df_below_18.sample(n=min_count, replace=True)\nbootstrap_37_54 = df_37_54.sample(n=min_count, replace=True)\n# Combine the bootstrapped samples\nbootstrapped_df = pd.concat([bootstrap_18_36, bootstrap_above_55,\u2423\n\u21aabootstrap_below_18, bootstrap_37_54])\n# Shuffle to randomize the combined data\nbootstrapped_df = bootstrapped_df.sample(frac=1).reset_index(drop=True)\n# New gender distribution after bootstrapping\nnew_gender_counts = bootstrapped_df['age_group'].value_counts()\nprint(\"\\nBootstrapped Gender Distribution (Equal counts):\\n\", new_gender_counts)\n6"
    },
    {
        "page": 299,
        "text": "# Display the bootstrapped DataFrame\n#df = bootstrapped_df.drop(columns = ['Email address','Timestamp'])\nbootstrapped_df.head(5)\nOriginal Age Group Distribution:\nage_group\n18 - 36\n80\nAbove 55\n33\nBelow 18\n28\n37 - 54\n23\nName: count, dtype: int64\nBootstrapped Gender Distribution (Equal counts):\nage_group\n18 - 36\n23\nBelow 18\n23\nAbove 55\n23\n37 - 54\n23\nName: count, dtype: int64\n[9]:\nTimestamp age_group\ngender social_media_hours_per_day\n\\\n0\n1/26/2025 0:51:58\n18 - 36\nFemale\n1 - 4 hours\n1\n1/26/2025 15:26:46\nBelow 18\nMale\n1 - 4 hours\n2\n2025-02-02 23:37:00\n18 - 36\nFemale\n1 - 4 hours\n3\n2025-01-25 00:12:00\nAbove 55\nFemale\nLess than 1 hour\n4\n1/25/2025 5:32:20\nBelow 18\nMale\n1 - 4 hours\nmost_frequent_platform platform_usage_frequency\n\\\n0\ninstagram\nRarely\n1\ninstagram\nMultiple times daily\n2\ninstagram\nMultiple times daily\n3\nfacebook\nDaily\n4\ninstagram\nDaily\nad_frequency_on_social_media influencer_promotions_frequency\n\\\n0\nFrequently\nFrequently\n1\nAlmost every time\nAlmost every time\n2\nAlmost every time\nAlmost every time\n3\nOccasionally\nRarely\n4\nFrequently\nOccasionally\npurchased_after_seeing_on_social_media\n\\\n0\n1-2\n1\n3-5\n2\n0\n3\n0\n7"
    },
    {
        "page": 300,
        "text": "4\n3-5\nunplanned_purchases_after_social_media\n\u2026 comparison_factor\n\\\n0\nRarely\n\u2026\nReviews\n1\nRarely\n\u2026\nPrice\n2\nRarely\n\u2026\nReviews\n3\nNever\n\u2026\nNot Applicable\n4\nRarely\n\u2026\nPrice\npurchased_to_fit_in_with_trends peer_influence_on_purchasing\n\\\n0\nYes\nRarely\n1\nYes\nOccasionally\n2\nNo\nRarely\n3\nNo\nNever\n4\nNo\nRarely\nsocial_media_increases_spending\npayment_method\n\\\n0\n3\nCredit Card\n1\n4\nPersonal Income\n2\n3\nCredit Card\n3\n1\nPersonal Income\n4\n2\nParental Support, Savings\nexceed_budget_due_to_social_media loan_for_social_media_purchases\n\\\n0\nRarely\nNo\n1\nOccasionally\nNo\n2\nNever\nNo\n3\nNever\nNo\n4\nRarely\nNo\ntrustworthiness_of_social_media_ads influencer_vs_brand_ads_trust\n\\\n0\n2\nNeither\n1\n3\nInfluencers\n2\n3\nBrand accounts\n3\n1\nNeither\n4\n4\nBrand accounts\nEmail address\n0\nnazreenathani736@gmail.com\n1\nfaaiz.tariq10@gmail.com\n2\nalisha.jyoti@ucalgary.ca\n3\nNaN\n4\nvighneshhonamore@gmail.com\n[5 rows x 24 columns]\n8"
    },
    {
        "page": 301,
        "text": "1.5.1\nDiscussion\nIn this process, we first count the number of samples in each age group to understand the original\ndistribution. Then, we separate the dataset into four age categories: \u201c18 - 36,\u201d \u201cAbove 55,\u201d \u201cBelow\n18,\u201d and \u201c37 - 54.\u201d\nNext, we determine the minimum sample size among these groups (min_count) to ensure propor-\ntional allocation. To balance the dataset, we perform bootstrapping by randomly sampling (with\nreplacement) from each age group so that all groups have the same number of samples as the small-\nest group. After bootstrapping, we combine all the sampled data into a new dataset and shuffle it\nto maintain randomness. Finally, we check the new distribution to confirm that the sample sizes\nare now equal across age groups, ensuring a more balanced dataset for further analysis.\n1.6\nExploratory Data Analysis\n1. Purchase Frequency and Most used platform\n[21]: ### ERROR\n# df is the dataframe to be used->\n# Define relevant columns\npurchase_col = \"purchased_after_seeing_on_social_media\"\ngroup_col = \"gender\"\n# Change to \"Age Group\" if needed\n# Group by purchase column and gender, then count occurrences\ngrouped_df = df.groupby([purchase_col, group_col]).size().\n\u21aareset_index(name='count')\n# Create bar chart using Plotly with custom colors\ncustom_colors = px.colors.qualitative.Prism\n# Updated to a different color\u2423\n\u21aascheme\nfig = px.bar(grouped_df, x=purchase_col, y='count', color=group_col,\u2423\n\u21aatext='count',\ntitle=\"Purchases after Social Media Exposure by Gender/Age Group\",\nlabels={purchase_col: \"Purchase Frequency\", \"count\": \"Number of\u2423\n\u21aaRespondents\"},\nbarmode='group', color_discrete_sequence=custom_colors)\n# Remove bar outlines\nfig.update_traces(marker=dict(line=dict(width=0)))\n# Show plot\nfig.show()\n1.6.1\nDiscussion\nThe analysis reveals distinct purchase behaviors influenced by social media exposure across genders.\nA significant number of females (28) did not make any purchases, suggesting they are more resistant\nor less influenced by social media marketing compared to 13 males. Conversely, males are more\nlikely to make 1-2 purchases (22 males) than females (16 females), indicating moderate influence on\nmales. A smaller segment of both genders made 3-5 purchases, with females (7) slightly outpacing\nmales (3). Notably, only males made 6-10 purchases (4 males), highlighting a small but highly\n9"
    },
    {
        "page": 302,
        "text": "influenced group. These insights suggest that while social media marketing can influence purchase\nbehavior, it does not often result in high-frequency purchases. Therefore, strategies targeting males\nmight be more effective, optimizing marketing campaigns to enhance engagement and conversion\nrates. Personalized and engaging content could improve the effectiveness of campaigns targeting\nfemales, aiming to convert them into buyers.\n2. Purchasing Trends\n[22]: # Pivot the data for heatmap (categorical correlation by count)\nheatmap_data = df.pivot_table(index=\"age_group\",\u2423\n\u21aacolumns=\"purchased_after_seeing_on_social_media\", aggfunc=\"size\",\u2423\n\u21aafill_value=0)\n# Order the pivot table based on age group\nage_order = ['Below 18', '18 - 36', '37 - 54', 'Above 55']\n# Customize this\u2423\n\u21aaorder as per your age groups\nheatmap_data = heatmap_data.reindex(age_order)\n# Create heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(heatmap_data, cmap=\"RdBu\", annot=True, fmt=\"d\", linewidths=0.5)\nplt.title(\"Purchasing Trends Across Age Groups\")\nplt.xlabel(\"Purchase Frequency\")\nplt.ylabel(\"Age Group\")\nplt.show()\n10"
    },
    {
        "page": 303,
        "text": "1.6.2\nDiscussion\nThe analysis reveals distinct patterns in purchase behavior influenced by social media exposure\nacross different age groups. A significant number of individuals in the younger age group (e.g., 18-\n24 years) did not make any purchases, indicating they are more resistant or less influenced by social\nmedia marketing. In contrast, individuals in the middle age group (e.g., 25-34 years) are more likely\nto make 1-2 purchases after exposure, suggesting a moderate influence of social media marketing on\nthis demographic. A smaller segment of both younger and middle-aged groups made 3-5 purchases,\npointing to a group of moderately influenced individuals. Notably, only older individuals (e.g.,\n35-44 years) made 6-10 purchases, highlighting a small yet highly influenced group. These insights\nsuggest that while social media marketing impacts purchase behavior, it generally results in low\nto moderate purchase frequency across different age groups.\nTherefore, targeting the middle-\naged demographic might be more effective in optimizing campaigns to enhance engagement and\nconversion rates. Additionally, personalized and engaging content could improve the effectiveness\nof campaigns targeting younger individuals, converting them into buyers.\n3. Purchasing to fit in with Trends\n[23]: g = sns.FacetGrid(\nbootstrapped_df,\ncol='age_group',\ncol_wrap=3,\nheight=4,\ncol_order=['Below 18', '18 - 36', '37 - 54', 'Above 55']\n)\ng.map(sns.histplot, 'purchased_to_fit_in_with_trends', bins=3, kde=False,\u2423\n\u21aacolor='purple')\ng.set_titles(col_template=\"{col_name} Age Group\")\nplt.show()\n11"
    },
    {
        "page": 304,
        "text": "1.6.3\nDiscussion\nThe graphs compare purchasing behavior across age groups, showing whether people bought items\nto \u201cfit in with trends.\u201d The x-axis shows \u201cYes\u201d or \u201cNo\u201d for purchases, and the y-axis shows the\ncount of individuals. The data is split into four age groups: \u201cBelow 18,\u201d \u201c18 - 36,\u201d \u201c37 - 54,\u201d and\n\u201cAbove 55,\u201d with each group displayed side by side for easy comparison.\nThe insights suggest that younger age groups, especially \u201c18 - 36,\u201d are more likely to buy items to\nfit in with trends, while older groups, particularly \u201cAbove 55,\u201d tend to refrain from such purchases.\nThis can inform targeted marketing strategies based on age.\n4. Ad Frequency vs Platform Usage Frequency\n[24]: count_df = df.groupby(['platform_usage_frequency',\u2423\n\u21aa'ad_frequency_on_social_media']).size().reset_index(name='count')\nfig6 = px.bar(count_df,\nx=\"platform_usage_frequency\",\ny=\"count\",\ncolor=\"ad_frequency_on_social_media\",\ntitle=\"Ad Frequency vs Platform Usage Frequency\",\nlabels={\"platform_usage_frequency\": \"Platform Usage Frequency\",\n\"ad_frequency_on_social_media\": \"Ad Frequency\"},\nbarmode=\"stack\")\nfig6.show()\n12"
    },
    {
        "page": 305,
        "text": "1.6.4\nDiscussion\nUsers Who Engage Daily See Ads the Most: The largest group of users falls under \u201cDaily\u201d\nplatform usage. They mostly encounter ads \u201cOccasionally\u201d and \u201cFrequently\u201d, with a few seeing\nads \u201cAlmost every time\u201d.\nUsers Engaging Multiple Times Daily Also Experience Frequent Ads: This group also\nsees a high proportion of ads \u201cFrequently\u201d and \u201cOccasionally\u201d. Fewer users in this category report\nads appearing \u201cAlmost every time.\u201d\nOccasional and Rare Users See Fewer Ads: Those who use social media \u201cOccasionally\u201d or\n\u201cRarely\u201d encounter ads much less frequently. The ad frequency for these groups is mostly \u201cRarely\u201d\nand \u201cOccasionally\u201d, with very few experiencing frequent ads.\n5. Trustworthiness of Ads\n[25]: age_group_order = [\"Below 18\", \"18 - 36\", \"37 - 54\", \"Above 55\"]\nfig7 = px.box(df,\nx=\"age_group\",\ny=\"trustworthiness_of_social_media_ads\",\ntitle=\"Trustworthiness of Ads by Age Group\",\nlabels={\"age_group\": \"Age Group\",\n\"trustworthiness_of_social_media_ads\": \"Trust Level\"},\ncategory_orders={\"age_group\": age_group_order})\nfig7.show()\n1.6.5\nDiscussion\nYounger Users (Below 18) Trust Ads More: The median trust level is higher than other age\ngroups, around 3 to 4. The interquartile range (IQR) is wider, indicating varying opinions within\nthis group. Some users even rated trustworthiness at the highest level (5).\nMiddle Age Groups (18-36, 37-54) Show Skepticism: Their median trust levels are lower\n(around 2 to 3). The IQR is narrower, meaning most individuals in this range have similar views\non ad trustworthiness. A few outliers suggest that some people trust ads more than their peers.\nOlder Adults (Above 55) Have the Lowest Trust in Ads: The median trust level is around\n2, showing significant skepticism. The IQR extends from 1 to 3, meaning most users in this group\nrate ad trustworthiness quite low. There are no extreme outliers, suggesting a general consensus\namong older users.\n6. Most Frequent Social Media Platforms\n[26]: # Set a modern theme\nsns.set_theme(style=\"whitegrid\")\n# Create the figure and axis\nplt.figure(figsize=(12, 6))\n# Create the countplot with enhanced aesthetics\nsns.countplot(\n13"
    },
    {
        "page": 306,
        "text": "data=bootstrapped_df,\nx='most_frequent_platform',\norder=bootstrapped_df['most_frequent_platform'].value_counts().index,\npalette='magma',\n# Try 'crest' or 'pastel' for different vibes\nedgecolor='black'\n# Adds a clean border to the bars\n)\n# Customize labels and title\nplt.xticks(rotation=45, fontsize=12, ha='right')\n# Rotate x-axis labels for\u2423\n\u21aareadability\nplt.xlabel('Social Media Platform', fontsize=14, fontweight='bold')\nplt.ylabel('Count', fontsize=14, fontweight='bold')\nplt.title('Most Frequent Social Media Platforms', fontsize=16,\u2423\n\u21aafontweight='bold')\n# Remove top and right borders for a cleaner look\nsns.despine()\n# Show the plot\nplt.show()\n1.6.6\nDiscussion\nThe graphs here show the most frequently used social media platforms.\nThe x-axis represents\ndifferent platforms, including YouTube, Instagram, Facebook, and others, while the y-axis shows the\n14"
    },
    {
        "page": 307,
        "text": "count of users for each platform. YouTube and Instagram seem to be the most popular platforms,\nwith a significant number of users, followed by Facebook, while platforms like Twitter and TikTok\nare used much less frequently.\nFrom these visualizations, it\u2019s clear that YouTube and Instagram dominate social media usage in\nthis dataset, with Facebook also being widely used. Platforms like Snapchat, Reddit, and LinkedIn\nhave moderate usage, while TikTok, Twitter, and WhatsApp appear to be the least frequent among\nthe given options. This data could help in understanding platform preference for different marketing\nor content strategies.\n7. Purchasing trends across social media user vs Social media hours per day\n[27]: import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Define the custom order for the y-axis labels\ny_order = ['Less than 1 hour', '1 - 4 hours', '4 - 7 hours', 'More than 7\u2423\n\u21aahours']\ndf[\"purchased_after_seeing_on_social_media\"] =\u2423\n\u21aadf[\"purchased_after_seeing_on_social_media\"].astype(str)\n# Ensure 'social_media_hours_per_day' is a categorical variable with the\u2423\n\u21aaspecified order\nbootstrapped_df['social_media_hours_per_day'] = pd.Categorical(\nbootstrapped_df['social_media_hours_per_day'], categories=y_order,\u2423\n\u21aaordered=True\n)\n# Create pivot table with the correct indexing for rows and columns\nheatmap_data = df.pivot_table(\nindex=\"social_media_hours_per_day\",\ncolumns=\"purchased_after_seeing_on_social_media\",\naggfunc=\"size\",\nfill_value=0\n)\n# Create heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(heatmap_data, cmap=\"PuOr\", annot=True, fmt=\"d\", linewidths=0.5)\nplt.title(\"Purchasing Trends Across Social Media Usage\")\nplt.xlabel(\"Purchase Frequency\")\nplt.ylabel(\"Social media hours per day\")\nplt.show()\n15"
    },
    {
        "page": 308,
        "text": "1.6.7\nDiscussion\nThe heatmap shows purchasing trends based on social media usage. The x-axis represents purchase\nfrequency (0, 1-2, 3-5, 6-10), while the y-axis shows daily social media hours (<1 hour, 1-4 hours,\n4-7 hours). Darker shades indicate higher counts, with most users in the \u201c1-4 hours\u201d group making\n1-2 purchases.\nUsers spending \u201c1-4 hours\u201d on social media tend to purchase more, likely due to optimal ad ex-\nposure. Both heavy (4-7 hours) and light (<1 hour) users show lower purchase activity. Higher\npurchase categories (3-5, 6-10) are rare, suggesting frequent buying from social media is uncommon.\n1.7\nGuiding Questions\n1.7.1\n1. Estimating Population Parameters\nThe focus of this analysis is to estimate the population mean, variance, and their confidence intervals\n(CI) using Simple Random Sampling (SRS) and Stratified Sampling. Stratified sampling is\nparticularly useful to ensure that different subgroups within the population are represented, which\nhelps reduce bias and provides more accurate estimates.\n\u2022 Population Size: ~35,000\n\u2022 Sample Size: 92 (after stratification)\nThe key metric we are estimating is the average number of purchases influenced by social\nmedia for specific subgroups within the population. This allows us to understand how social media\nimpacts consumer behavior across various demographics.\n16"
    },
    {
        "page": 309,
        "text": "By focusing on these estimates, we aim to gain insights into the variability in purchasing\nbehavior across different demographics, helping to better understand how purchasing decisions\nare influenced by social media in diverse groups.\n[28]: def convert_purchases(value):\nif value == '0':\nreturn 0\nelif value == '1-2':\nreturn 1\nelif value == '3-5':\nreturn 3\nelif value == '6-10':\nreturn 6\nelif value == 'More than 10':\nreturn 10\nelse:\nreturn 0\n# Handle any unexpected values\n# Apply the conversion function to the 'purchased_after_seeing_on_social_media'\u2423\n\u21aacolumn\ndf_original['purchased_numeric'] = df_original['How many times in the past\u2423\n\u21aamonth have you purchased a product after seeing it on social media?'].\n\u21aaapply(convert_purchases)\nThe convert_purchases function is designed to convert categorical responses about the number\nof purchases made after seeing a product on social media into numeric values. This conversion\nsimplifies the data for analysis by transforming text-based categories into quantifiable numbers\n[29]: # Estimate overall population mean and variance from the sample\npopulation_mean = df_original['purchased_numeric'].mean()\npopulation_variance = df_original['purchased_numeric'].var(ddof=1)\n# Sample\u2423\n\u21aavariance (unbiased estimate)\nprint(f\"Estimated Population Mean: {population_mean:.2f}\")\nprint(f\"Estimated Population Variance: {population_variance:.2f}\")\nEstimated Population Mean: 0.65\nEstimated Population Variance: 0.76\n[30]: N = 35000\n# Assume total population size\nn = len(df_original)\nSE_corrected = (df_original['purchased_numeric'].std(ddof=1) / np.sqrt(n)) * np.\n\u21aasqrt((N - n) / (N - 1))\n# 95% Confidence Interval for Population Mean\nz_critical = 1.96\n# For 95% CI (normal approximation)\nci_lower = population_mean - z_critical * SE_corrected\n17"
    },
    {
        "page": 310,
        "text": "ci_upper = population_mean + z_critical * SE_corrected\nprint(f\"95% CI for Population Mean: ({ci_lower:.2f}, {ci_upper:.2f})\")\n95% CI for Population Mean: (0.50, 0.81)\nPopulation Parameters Interpretation using SRS\n\u2022 Population Mean: The estimated average number of purchases made after seeing products\non social media is 0.85 per month.\n\u2022 Population Variance: The variance of 0.76 indicates moderate variability in purchasing\nbehavior.\n\u2022 95% Confidence Interval: The true population mean is estimated to lie between 0.50 and\n0.81 with 95% confidence.\nImplication: Social media has a modest but measurable influence on purchasing decisions, with\nmost individuals making 0 to 1 purchase per month influenced by social media.\nUsing Stratified Sample\n[31]: data=df.copy()\n# Apply the conversion function to the 'purchased_after_seeing_on_social_media'\u2423\n\u21aacolumn\ndata['purchased_numeric'] = data['purchased_after_seeing_on_social_media'].\n\u21aaapply(convert_purchases)\n# Group data by 'age_group' and 'gender', then calculate mean, variance, and\u2423\n\u21aasample count\ngrouped_data = data.groupby(['age_group', 'gender'])['purchased_numeric'].\n\u21aaagg(['mean', 'var', 'count'])\nprint(\"\\nStratified Mean and Variance:\")\nprint(grouped_data)\nStratified Mean and Variance:\nmean\nvar\ncount\nage_group gender\n18 - 36\nFemale\n0.545455\n0.272727\n11\nMale\n1.000000\n2.727273\n12\n37 - 54\nFemale\n0.526316\n0.596491\n19\nMale\n0.500000\n0.333333\n4\nAbove 55\nFemale\n0.333333\n1.000000\n9\nMale\n2.071429\n4.994505\n14\nBelow 18\nFemale\n1.500000\n1.909091\n12\nMale\n0.818182\n0.763636\n11\n[32]: # Assume total population size\nN = 35000\nprint(\"Total Population Size:\", N)\n18"
    },
    {
        "page": 311,
        "text": "# Apply FPC adjustment for each stratified group\ngrouped_data['SE_corrected'] = (np.sqrt(grouped_data['var']) / np.\n\u21aasqrt(grouped_data['count'])) * np.sqrt((N - grouped_data['count']) / (N - 1))\n# Recalculate Confidence Intervals with FPC\ngrouped_data['ci_lower_fpc'] = grouped_data['mean'] - 1.96 *\u2423\n\u21aagrouped_data['SE_corrected']\ngrouped_data['ci_upper_fpc'] = grouped_data['mean'] + 1.96 *\u2423\n\u21aagrouped_data['SE_corrected']\nprint(\"\\nConfidence Intervals for Each Stratified Group with FPC:\")\nprint(grouped_data[['mean', 'ci_lower_fpc', 'ci_upper_fpc']])\nTotal Population Size: 35000\nConfidence Intervals for Each Stratified Group with FPC:\nmean\nci_lower_fpc\nci_upper_fpc\nage_group gender\n18 - 36\nFemale\n0.545455\n0.236879\n0.854030\nMale\n1.000000\n0.065754\n1.934246\n37 - 54\nFemale\n0.526316\n0.179124\n0.873508\nMale\n0.500000\n-0.065779\n1.065779\nAbove 55\nFemale\n0.333333\n-0.319925\n0.986592\nMale\n2.071429\n0.900966\n3.241891\nBelow 18\nFemale\n1.500000\n0.718353\n2.281647\nMale\n0.818182\n0.301836\n1.334528\n[33]: # Calculate the overall mean\noverall_mean = grouped_data[\"mean\"].mean()\n# Variance of y-bar\nV_y_hat = np.sum((1 - grouped_data['count'] / 4375) * (4375 / N)**2 *\u2423\n\u21aagrouped_data['var'] / grouped_data['count'])\n# Standard error of y-bar\nSE_y_hat = np.sqrt(V_y_hat)\nprint(f\"\\nEstimated mean of purchase frequency (Stratified Proportional\u2423\n\u21aaAllocation): {overall_mean:.2f}\")\nprint(f\"Estimated variance of purchase frequency (Stratified Proportional\u2423\n\u21aaAllocation): {V_y_hat:.2f}\")\nprint(f\"Estimated standard Error of purchase frequency (Stratified Proportional\u2423\n\u21aaAllocation): {SE_y_hat:.2f}\")\nEstimated mean of purchase frequency (Stratified Proportional Allocation): 0.91\n19"
    },
    {
        "page": 312,
        "text": "Estimated variance of purchase frequency (Stratified Proportional Allocation):\n0.02\nEstimated standard Error of purchase frequency (Stratified Proportional\nAllocation): 0.13\nStratified Group Analysis:\n18 - 36: - Females: On average, females in this group purchase about half a product per month\nafter seeing it on social media. This suggests 1 purchase every 2 months. - Males: Males in this\ngroup purchase 1 product per month on average, indicating they are twice as likely as females to\nmake a purchase influenced by social media.\n37 - 54: - Females: Females in this group purchase about half a product per month, similar to\nyounger females, suggesting consistent behavior across age groups. - Males: Males in this group also\npurchase about half a product per month, showing no significant difference compared to females in\nthe same age group.\nAbove 55: - Females: Females in this group purchase about one-third of a product per month,\nindicating 1 purchase every 3 months. This suggests lower engagement with social media-driven\npurchases. - Males: Males in this group purchase 2 products per month on average, making them\nthe most influenced demographic. This could indicate higher trust in social media ads or greater\ndisposable income.\nBelow 18: - Females: Females in this group purchase 1.5 products per month, the highest among\nall female groups. This suggests younger females are highly influenced by social media trends. -\nMales: Males in this group purchase 0.8 products per month, indicating 1 purchase every 1.25\nmonths. While lower than females in the same group, they are still more influenced than older\nmales.\nConfidence Intervals: The 95% confidence intervals for each group indicate the range within\nwhich the true mean likely falls. For example, males Above 55 have a mean between 0.90 and 3.24\npurchases, showing significant variability.\nOverall Population Estimates: - Mean Purchase Frequency: 0.91 purchases per month. - Vari-\nance: 0.02, indicating low variability in the overall population. - Standard Error: 0.13, suggesting\nhigh precision in the estimate.\n1.7.2\n2.\nImpact of Social Media Usage, Age Group, and Platform on Unplanned\nPurchases\nIn this analysis, we aim to explore the impact of social media usage, age group, and platform\non the likelihood of making unplanned purchases. Specifically, we want to understand how these\nfactors influence consumer behavior in terms of impulsive buying decisions.\nThe dependent variable in this study is unplanned purchases after seeing a product on social\nmedia, which is an ordinal variable with categories such as - \u201cNever\u201d - \u201cRarely\u201d - \u201cOccasionally\u201d\n- \u201cFrequently\u201d - \u201cAlways\u201d\nThe primary predictors include:\n\u2022 Social Media Usage (measured in hours per day: \"Less than 1 hour\", \"1\u20133 hours\",\n\"4\u20136 hours\", \"7\u20139 hours\", \"10+ hours\")\n\u2022 Age Group (e.g., \"18\u201336\", \"37\u201354\", \"Above 55\")\n20"
    },
    {
        "page": 313,
        "text": "\u2022 Platform (how often ads are seen: \"Instagram\", \"Youtube\", \"Twitter\", etc.)\nThe goal is to determine whether social media usage, age group, and platform have a significant\neffect on the likelihood of making unplanned purchases after seeing a product on social media. By\nunderstanding these relationships, we can provide insights into how social media usage and other\ndemographic factors drive impulsive purchasing behavior.\nLikelihood Ratio Test\nTo assess the significance of the predictors, we will perform a Like-\nlihood Ratio Test (LRT) to compare the reduced and full models.\nThe test will evaluate if\nadding social_media_usage, age_group, and most_frequent_platform significantly improves\nthe model\u2019s fit.\n\u2022 L0: Log-likelihood of the reduced model (without the predictors).\n\u2022 L1: Log-likelihood of the full model (with the predictors).\nThe LRT will help us determine whether the addition of these factors significantly improves the\nmodel\u2019s ability to explain unplanned purchases.\nHypothesis 1\n\u2022 Null Hypothesis (H0):\nThere is no significant relationship between social media usage and unplanned purchases\nafter seeing products on social media.\n\u2022 Alternative Hypothesis (H1):\nSocial media usage significantly influences unplanned purchases after seeing products on social\nmedia.\n[34]: df_1 = df.copy()\n# Select relevant columns\ndf_1 = df_1[['social_media_hours_per_day',\u2423\n\u21aa'unplanned_purchases_after_social_media']]\n# Convert categorical variables to ordered numeric values\nsocial_media_mapping = {\n\"Less than 1 hour\": 0,\n\"1 - 4 hours\": 1,\n\"4 - 7 hours\": 2,\n\"More than 7 hours\": 3\n}\npurchases_mapping = {\n\"Never\": 1,\n\"Rarely\": 2,\n\"Occasionally\": 3,\n\"Frequently\": 4,\n\"Always\": 5\n}\n21"
    },
    {
        "page": 314,
        "text": "df_1['social_media_hours_per_day'] = df_1['social_media_hours_per_day'].\n\u21aamap(social_media_mapping)\ndf_1['unplanned_purchases_after_social_media'] =\u2423\n\u21aadf_1['unplanned_purchases_after_social_media'].map(purchases_mapping)\n# Drop NaN values\ndf_1.dropna(inplace=True)\n[35]: # Fit the reduced model (without social media usage)\nreduced_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~ 1\",\u2423\n\u21aadata=df_1).fit()\n# Fit the full model (with social media usage)\nfull_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~\u2423\n\u21aasocial_media_hours_per_day\", data=df_1).fit()\n# Likelihood Ratio Test\nL0 = reduced_model.llf\nL1 = full_model.llf\nD = -2 * (L0 - L1)\n# LRT statistic\ndf_diff = full_model.df_model - reduced_model.df_model\n# Degrees of freedom\u2423\n\u21aadifference\np_value = chi2.sf(D, df_diff)\nOptimization terminated successfully.\nCurrent function value: 1.090785\nIterations 7\nOptimization terminated successfully.\nCurrent function value: 0.953934\nIterations 8\n[36]: print(f\"LRT Statistic: {D:.4f}\")\nprint(f\"Degrees of Freedom: {df_diff}\")\nprint(f\"p-value: {p_value:.4f}\")\nif p_value < 0.05:\nprint(\"Reject the null hypothesis: Social media usage frequency\u2423\n\u21aasignificantly affects unplanned purchases.\")\nelse:\nprint(\"Fail to reject the null hypothesis: No significant effect of social\u2423\n\u21aamedia usage frequency on unplanned purchases.\")\nLRT Statistic: 25.1805\nDegrees of Freedom: 3.0\np-value: 0.0000\nReject the null hypothesis: Social media usage frequency significantly affects\n22"
    },
    {
        "page": 315,
        "text": "unplanned purchases.\nThe Likelihood Ratio Test (LRT) statistic is 25.1805 with 3 degrees of freedom, and the\nresulting p-value is 0.0000. Since the p-value is significantly lower than the common significance\nlevel of 0.05, we reject the null hypothesis.\nConclusion: Social media usage frequency significantly affects unplanned purchases after seeing\nproducts on social media.\nHypothesis 2\n\u2022 Null Hypothesis (H0):\nThere is no significant relationship between age group and unplanned purchases after seeing\nproducts on social media.\n\u2022 Alternative Hypothesis (H1):\nAge group significantly influences unplanned purchases after seeing products on social media.\n[37]: df_2=df.copy()\n# Encode age_group as a categorical variable\nage_group_mapping = {\n\"Below 18\": 0,\n\"18 - 36\": 1,\n\"37 - 54\": 2,\n\"Above 55\": 3\n}\ndf_2['age_group'] = df_2['age_group'].map(age_group_mapping)\n# Check unique values for unplanned_purchases_after_social_media to ensure it's\u2423\n\u21aacategorical\nprint(df_2['unplanned_purchases_after_social_media'].unique())\n# Convert unplanned_purchases_after_social_media to numeric values if needed\npurchases_mapping = {\n\"Never\": 1,\n\"Rarely\": 2,\n\"Occasionally\": 3,\n\"Frequently\": 4,\n\"Always\": 5\n}\ndf_2['unplanned_purchases_after_social_media'] =\u2423\n\u21aadf_2['unplanned_purchases_after_social_media'].map(purchases_mapping)\ndf_2.dropna(inplace=True)\n['Never' 'Rarely' 'Occasionally' 'Frequently']\n23"
    },
    {
        "page": 316,
        "text": "[38]: # Fit the reduced model (without age_group)\nreduced_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~ 1\",\u2423\n\u21aadata=df_2).fit()\n# Fit the full model (with age_group)\nfull_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~ age_group\",\u2423\n\u21aadata=df_2).fit()\n# Likelihood Ratio Test\nL0 = reduced_model.llf\nL1 = full_model.llf\nD = -2 * (L0 - L1)\ndf_diff = full_model.df_model - reduced_model.df_model\np_value = chi2.sf(D, df_diff)\nOptimization terminated successfully.\nCurrent function value: 1.090785\nIterations 7\nWarning: Maximum number of iterations has been exceeded.\nCurrent function value: 1.018325\nIterations: 35\n[39]: print(f\"LRT Statistic: {D:.4f}\")\nprint(f\"Degrees of Freedom: {df_diff}\")\nprint(f\"p-value: {p_value:.4f}\")\nif p_value < 0.05:\nprint(\"Reject the null hypothesis: Age group significantly affects\u2423\n\u21aaunplanned purchases.\")\nelse:\nprint(\"Fail to reject the null hypothesis: No significant effect of age\u2423\n\u21aagroup on unplanned purchases.\")\nLRT Statistic: 13.3326\nDegrees of Freedom: 3.0\np-value: 0.0040\nReject the null hypothesis: Age group significantly affects unplanned purchases.\nThe Likelihood Ratio Test (LRT) statistic is 13.3326 with 3 degrees of freedom, and the\nresulting p-value is 0.0040. Since the p-value is less than the common significance level of 0.05,\nwe reject the null hypothesis.\nConclusion: Age group significantly affects unplanned purchases after seeing products on social\nmedia.\nHypothesis 3\n24"
    },
    {
        "page": 317,
        "text": "\u2022 Null Hypothesis (H0):\nThere is no significant relationship between most_frequent_platform and unplanned pur-\nchases after seeing products on social media.\n\u2022 Alternative Hypothesis (H1):\nMost Frequent Platform frequency significantly influences unplanned purchases after seeing\nproducts on social media.\n[40]: df['most_frequent_platform'].value_counts()\n[40]: most_frequent_platform\nyoutube\n34\ninstagram\n34\nfacebook\n12\nsnapchat\n3\ntwitter\n3\nlinkedin\n2\nreddit\n1\nwhatsapp\n1\ntiktok\n1\nsnapchat\n1\nName: count, dtype: int64\nCombine platforms with very few observations into a single category (e.g., Other).\n[41]: df_3=df.copy()\nplatform_counts = df_3['most_frequent_platform'].value_counts()\nrare_platforms = platform_counts[platform_counts < 5].index\n# Platforms with\u2423\n\u21aafewer than 5 observations\ndf_3['most_frequent_platform'] = df_3['most_frequent_platform'].\n\u21aareplace(rare_platforms, 'Other')\nprint(df_3['most_frequent_platform'].value_counts())\nmost_frequent_platform\nyoutube\n34\ninstagram\n34\nfacebook\n12\nOther\n12\nName: count, dtype: int64\nConvert the platform names into numerical codes for modeling.\n[42]: platform_mapping = {\n\"instagram\": 0,\n\"youtube\": 1,\n\"facebook\": 2,\n\"Other\": 3\n}\n25"
    },
    {
        "page": 318,
        "text": "df_3['most_frequent_platform'] = df_3['most_frequent_platform'].\n\u21aamap(platform_mapping)\nprint(df_3['most_frequent_platform'].value_counts())\nmost_frequent_platform\n1\n34\n0\n34\n2\n12\n3\n12\nName: count, dtype: int64\n[43]: purchases_mapping = {\n\"Never\": 1,\n\"Rarely\": 2,\n\"Occasionally\": 3,\n\"Frequently\": 4,\n\"Always\": 5\n}\ndf_3['unplanned_purchases_after_social_media'] =\u2423\n\u21aadf_3['unplanned_purchases_after_social_media'].map(purchases_mapping)\nprint(df_3['unplanned_purchases_after_social_media'].value_counts())\ndf_3.dropna(inplace=True)\nunplanned_purchases_after_social_media\n2\n49\n1\n26\n3\n14\n4\n3\nName: count, dtype: int64\n[44]: reduced_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~ 1\",\u2423\n\u21aadata=df_3).fit()\nfull_model = smf.mnlogit(\"unplanned_purchases_after_social_media ~\u2423\n\u21aamost_frequent_platform\", data=df_3).fit(\nmaxiter=500, method='bfgs'\n)\nL0 = reduced_model.llf\nL1 = full_model.llf\nD = -2 * (L0 - L1)\ndf_diff = full_model.df_model - reduced_model.df_model\np_value = chi2.sf(D, df_diff)\nprint(f\"Likelihood Ratio Test Statistic: {D:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n26"
    },
    {
        "page": 319,
        "text": "if p_value < 0.05:\nprint(\"Reject the null hypothesis: Platform significantly affects unplanned\u2423\n\u21aapurchases.\")\nelse:\nprint(\"Fail to reject the null hypothesis: No significant effect of\u2423\n\u21aaplatform on unplanned purchases.\")\nOptimization terminated successfully.\nCurrent function value: 1.090785\nIterations 7\nOptimization terminated successfully.\nCurrent function value: 0.985016\nIterations: 44\nFunction evaluations: 45\nGradient evaluations: 45\nLikelihood Ratio Test Statistic: 19.4614\np-value: 0.0002\nReject the null hypothesis: Platform significantly affects unplanned purchases.\nThe Likelihood Ratio Test (LRT) statistic is 19.4614 with 3 degrees of freedom, and the\nresulting p-value is 0.0002. Since the p-value is significantly less than the common significance\nlevel of 0.05, we reject the null hypothesis.\nConclusion: The most frequent platform used significantly affects unplanned purchases after\nseeing products on social media.\n1.7.3\n3. Predicting consumer spending using social media factors\nIn this analysis, we aim to predict consumer spending behavior using various social media-related\nfactors. The target variable for prediction is whether social media increases spending, denoted as\nsocial_media_increases_spending. The key predictor variables include:\n\u2022 content_type_influences_purchasing: Indicates the impact of different content types\non purchasing decisions.\n\u2022 limited_time_offer_purchases: Reflects the frequency of consumer purchases driven by\nlimited-time offers encountered on social media.\n\u2022 comparison_factor:\nRepresents the factors that influence the decision-making process\nwhen comparing products or services on social media.\n\u2022 peer_influence_on_purchasing: Measures how peer influence (e.g., recommendations or\nshared purchases) impacts purchasing decisions.\n\u2022 trustworthiness_of_social_media_ads: Assesses consumer trust in advertisements en-\ncountered on social media platforms.\nData Preparation\nTo prepare the data for training our predictive model, we employed One-\nHot Encoding on the categorical predictor variables.\nThis transformation converts categori-\ncal features into a numerical format that is suitable for use in regression models.\nSpecifically,\nthe pd.get_dummies() function was used with the drop_first=True argument to avoid multi-\n27"
    },
    {
        "page": 320,
        "text": "collinearity\u2014which occurs when there are redundant features that could distort model perfor-\nmance.\nBy applying One-Hot Encoding, each categorical variable is expanded into multiple binary (0 or\n1) columns, representing the different categories. This ensures that our model can learn from each\nindividual factor without introducing unnecessary redundancy.\nModel 1:\nLinear Regression\nTo begin our analysis, we tested a Linear Regression\nmodel to explore the relationship between the predictor variables and the target variable\n(social_media_increases_spending). The Linear Regression model was selected due to its sim-\nplicity and effectiveness in identifying linear relationships between the features and the target.\n[45]: X = df[['content_type_influences_purchasing',\n'limited_time_offer_purchases',\n'comparison_factor',\n'peer_influence_on_purchasing',\n'trustworthiness_of_social_media_ads']]\ny = df['social_media_increases_spending']\nX_encoded = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.\n\u21aa2, random_state=42)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(\"R2 score:\", r2_score(y_test, y_pred))\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\nR2 score: 0.5495285069904818\nMean Squared Error: 0.5839907444001511\nModel Performance\nThe performance of the Linear Regression model was evaluated using the\nfollowing metrics:\n\u2022 R2 Score: 0.5495\nThe R2 score indicates that approximately 54.95% of the variance in the target variable\n(social_media_increases_spending) can be explained by the predictor variables.\nThis\nsuggests a moderate fit, with room for improvement by exploring more complex models or\nadditional features.\n\u2022 Mean Squared Error (MSE): 0.584\nThe MSE quantifies the average squared difference between the predicted values and the actual\nvalues. A lower MSE indicates a better fit, and this value of 0.584 provides an indication of\nthe model\u2019s prediction accuracy. While not perfect, it suggests that the model is rea\n28"
    },
    {
        "page": 321,
        "text": "Cross-Validation of the Linear Regression Model\nTo assess the stability and generalization\nperformance of the Linear Regression model, we performed cross-validation. Cross-validation\ninvolves splitting the dataset into multiple subsets (or folds) and training the model on different\ncombinations of these subsets. This helps ensure that the model\u2019s performance is not overly reliant\non any specific subset of the data, providing a more robust estimate of its predictive ability.\nBy performing cross-validation, we can:\n\u2022 Evaluate the consistency of the model\u2019s performance across different data splits.\n\u2022 Identify any potential issues such as overfitting or underfitting.\n\u2022 Ensure that the model generalizes well to unseen data, making it more reliable for real-world\npredictions.\nThis step helps to verify the model\u2019s stability and provides a more comprehensive assessment of its\npredictive power beyond the initial evaluation metrics.\n[46]: cv_scores_linear = cross_val_score(model, X_encoded, y, cv=5, scoring='r2')\nprint(\"R2 scores for each fold:\", cv_scores_linear)\nprint(f\"Average R2 from cross-validation: {np.mean(cv_scores_linear):.4f}\")\nR2 scores for each fold: [ 0.50474292 -0.00891203\n0.64863617\n0.48683517\n0.41121492]\nAverage R2 from cross-validation: 0.4085\nCross-Validation Results\nThe average R2 score from cross-validation was 0.4085, which\nis lower than the initial training R2 score of 0.5495. This difference suggests that the model\u2019s\nperformance is not consistent across all subsets of the data, indicating variability in its ability to\npredict the target variable.\nA lower R2 score in cross-validation can imply that the model may be overfitting to the training\ndata, capturing noise or specific patterns that don\u2019t generalize well to unseen data. This variability\nhighlights the need for further model refinement, such as feature engineering, or exploring more\ncomplex algorithms to improve generalization and stability.\nModel 2: Decision Tree Regressor\nIn the next phase of our analysis, we tested a Decision\nTree Regressor. Unlike Linear Regression, the Decision Tree Regressor is a non-linear model\nthat is capable of capturing more complex relationships between the predictor variables and the\ntarget variable (social_media_increases_spending). This model works by splitting the data into\nsubsets based on the feature values, allowing it to model non-linear patterns more effectively.\n[47]: dt_model = DecisionTreeRegressor(random_state=42)\ndt_model.fit(X_train, y_train)\ny_pred = dt_model.predict(X_test)\nprint(\"R2 score:\", r2_score(y_test, y_pred))\nprint(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\nR2 score: 0.3504273504273505\nMean Squared Error: 0.8421052631578947\n29"
    },
    {
        "page": 322,
        "text": "Model Performance\nThe performance of the Decision Tree Regressor was evaluated using the\nfollowing metrics:\n\u2022 R2 Score: 0.3504\nThe R\u00b2 score indicates that only about 35.04% of the variance in the target variable can be\nexplained by the predictor variables. This is a relatively low value, suggesting that the model\nis not capturing as much of the underlying variance in the data as expected.\n\u2022 Mean Squared Error (MSE): 0.8421\nThe MSE is higher than the value from the Linear Regression model (0.584), which indicates\nthat the Decision Tree Regressor is making larger errors in its predictions. This suggests that\nthe model may be overfitting to certain patterns in the training data or not generalizing well\nto the validation data.\nDespite being able to model non-linear relationships, the Decision Tree Regressor performed less\neffectively than the Linear Regression model in this case. Further tuning of hyperparameters or\nexploring other models may improve its performance.\nWe also performed cross-validation on the decision tree model.\n[48]: cv_scores_dt = cross_val_score(dt_model, X_encoded, y, cv=5, scoring='r2')\nprint(\"R2 scores for each fold:\", cv_scores_dt)\nprint(f\"Average R2 from cross-validation: {np.mean(cv_scores_dt):.4f}\")\nR2 scores for each fold: [-0.16634273\n0.08604336\n0.41135972\n0.55555556\n0.55371901]\nAverage R2 from cross-validation: 0.2881\nModel Comparison and Insights\nUpon comparing the two models, the Linear Regression\nmodel proved to be more effective at predicting consumer spending based on social media fac-\ntors. Despite its moderate performance, with an R2 score of 0.5495 on the training set, it still\noutperformed the Decision Tree model. However, when tested with cross-validation, the model\u2019s\nperformance showed variability, indicating that it could benefit from further improvements in terms\nof generalization and stability.\nOn the other hand, the Decision Tree Regressor performed worse than the Linear Regression\nmodel, with an R2 score of 0.3504 and a higher Mean Squared Error (MSE) of 0.8421. This suggests\nthat the Decision Tree model was not well-suited for capturing the patterns in this particular\ndataset. It may have struggled to generalize well or could have been overfitting the data, leading\nto subpar predictive performance.\nIn summary, while the Linear Regression model offered a better baseline, both models highlight the\nneed for further refinement, such as hyperparameter tuning or exploring more advanced models, to\nimprove prediction accuracy and generalization.\n1.7.4\n4. Are influencer promotions more effective than brand advertisements in in-\nfluencing purchases?\nThe purpose of this analysis is to determine whether influencer promotions are more effective than\nbrand advertisements in influencing purchase behavior.\nThis involves examining the impact of\n30"
    },
    {
        "page": 323,
        "text": "various predictors, such as advertisements, peer reviews, influencer posts, detailed unsponsored\nreviews, brand posts, and discounts, on the frequency of purchases. By analyzing the correlation\nmatrix, OLS regression results, and decision tree model, the study aims to identify the key factors\nthat drive consumer purchases and compare the effectiveness of different marketing strategies.\nThis information can help marketers optimize their campaigns, improve engagement, and increase\nconversion rates by leveraging the most influential content types.\nData Pre-Processing\n\u2022 The content_type_influences_purchasing column was split into binary columns (0 or 1)\nfor each content type.\nCorrelation Matrix of Key Variables\n\u2022 Purchase Frequency is negatively correlated with Advertisements (-0.40) and Brand\nPosts (-0.15).\n\u2022 Peer Reviews have a positive correlation with Purchase Frequency (0.20).\n\u2022 Influencer Posts show a moderate positive correlation with Purchase Frequency (0.31).\n\u2022 Detailed Unsponsored Reviews and Discounts have weaker correlations.\n[49]: import statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Assuming df is your DataFrame\n# Define the content types\ncontent_types = ['Advertisements', 'Peer reviews', 'Influencer Posts',\u2423\n\u21aa'Detailed unsponsored reviews', 'Brand posts', 'Discount']\n# Split and expand the content types into multiple columns\nfor content in content_types:\ndf[content] = df['content_type_influences_purchasing'].apply(lambda x: 1 if\u2423\n\u21aacontent in x else 0)\npurchase_mapping = {\n\"0\": 0,\n\"1-2\": 1,\n\"3-5\": 2,\n\"More than 5\": 3\n}\ndf[\"Purchase Frequency\"] = df[\"purchased_after_seeing_on_social_media\"].\n\u21aamap(purchase_mapping)\ndf = df.dropna(subset=['Purchase Frequency'])\n# Compute and plot the correlation matrix\n31"
    },
    {
        "page": 324,
        "text": "correlation_matrix = df[[\"Purchase Frequency\", \"Advertisements\", \"Peer reviews\",\n\"Influencer Posts\", \"Detailed unsponsored reviews\",\u2423\n\u21aa\"Brand posts\", \"Discount\"]].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\",\u2423\n\u21aalinewidths=0.5)\nplt.title(\"Correlation Matrix of Key Variables\")\nplt.show()\n1.7.5\nOLS Regression Analysis\n[50]: # Define independent and dependent variables for regression\nX = df[[\"Advertisements\", \"Peer reviews\",\n\"Influencer Posts\", \"Detailed unsponsored reviews\", \"Brand posts\",\u2423\n\u21aa\"Discount\"]]\n32"
    },
    {
        "page": 325,
        "text": "y = df[\"Purchase Frequency\"]\nX = sm.add_constant(X)\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\u2423\n\u21aarandom_state=42)\n# Initialize and train the OLS model\nmodel = sm.OLS(y_train, X_train).fit()\n# Display OLS regression summary\nprint(model.summary())\nOLS Regression Results\n==============================================================================\nDep. Variable:\nPurchase Frequency\nR-squared:\n0.266\nModel:\nOLS\nAdj. R-squared:\n0.196\nMethod:\nLeast Squares\nF-statistic:\n3.802\nDate:\nWed, 19 Feb 2025\nProb (F-statistic):\n0.00270\nTime:\n20:54:57\nLog-Likelihood:\n-57.426\nNo. Observations:\n70\nAIC:\n128.9\nDf Residuals:\n63\nBIC:\n144.6\nDf Model:\n6\nCovariance Type:\nnonrobust\n================================================================================\n================\ncoef\nstd err\nt\nP>|t|\n[0.025\n0.975]\n--------------------------------------------------------------------------------\n----------------\nconst\n0.8246\n0.118\n6.959\n0.000\n0.588\n1.061\nAdvertisements\n-0.5018\n0.147\n-3.417\n0.001\n-0.795\n-0.208\nPeer reviews\n1.1754\n0.591\n1.988\n0.051\n-0.006\n2.357\nInfluencer Posts\n0.1404\n0.178\n0.788\n0.434\n-0.215\n0.496\nDetailed unsponsored reviews\n-0.8246\n0.591\n-1.394\n0.168\n-2.006\n0.357\nBrand posts\n-0.8246\n0.591\n-1.394\n0.168\n-2.006\n0.357\nDiscount\n0.1754\n0.426\n0.411\n0.682\n-0.677\n1.028\n==============================================================================\nOmnibus:\n4.273\nDurbin-Watson:\n2.444\nProb(Omnibus):\n0.118\nJarque-Bera (JB):\n3.682\nSkew:\n0.556\nProb(JB):\n0.159\n33"
    },
    {
        "page": 326,
        "text": "Kurtosis:\n3.157\nCond. No.\n10.4\n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly\nspecified.\n\u2022 R-squared: 0.266 (26.6% of the variance in Purchase Frequency can be explained by the\npredictors).\n\u2022 Advertisements have a statistically significant negative impact on Purchase Frequency (coef\n= -0.5019, p < 0.01).\n\u2022 Peer Reviews show a positive but marginally significant impact (coef = 1.1754, p = 0.051).\n\u2022 Influencer Posts, Detailed Unsponsored Reviews, Brand Posts, and Discounts are\nnot statistically significant.\n[51]: # Make predictions on the test set\ny_pred = model.predict(X_test)\n# Perform cross-validation\ncv_scores = cross_val_score(LinearRegression(), X, y, cv=5, scoring='r2')\ncv_mean_score = cv_scores.mean()\nprint(f\"R\u00b2 using cross-validation: {cv_mean_score}\")\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Mean Squared Error on Test Set: {mse}\")\nprint(f\"R\u00b2 on Test Set: {r2}\")\n# Make predictions on the entire dataset\ny_pred_ols = model.predict(X)\n# Calculate R\u00b2 and MSE for the entire dataset\nr2_ols = r2_score(y, y_pred_ols)\nmse_ols = mean_squared_error(y, y_pred_ols)\nprint(f\"R\u00b2 without using cross-validation: {r2_ols}\")\nprint(f\"MSE without using cross-validation: {mse_ols}\")\n# Display regression coefficients\ncoefficients = pd.DataFrame(model.params, columns=['Coefficient'])\ncoefficients\nR\u00b2 using cross-validation: 0.15041129795755834\nMean Squared Error on Test Set: 0.39388051024246795\nR\u00b2 on Test Set: 0.2218458212282951\nR\u00b2 without using cross-validation: 0.26405024367687313\nMSE without using cross-validation: 0.32083760038053666\n34"
    },
    {
        "page": 327,
        "text": "[51]:\nCoefficient\nconst\n0.824561\nAdvertisements\n-0.501754\nPeer reviews\n1.175439\nInfluencer Posts\n0.140351\nDetailed unsponsored reviews\n-0.824561\nBrand posts\n-0.824561\nDiscount\n0.175439\n[52]: import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n# Assuming the previous steps to prepare data and mapping have been completed\n# df['Purchase Frequency'] is already mapped\n# Split the data into training and testing sets\nX = df[[\"Advertisements\", \"Peer reviews\", \"Influencer Posts\", \"Detailed\u2423\n\u21aaunsponsored reviews\", \"Brand posts\", \"Discount\"]]\ny = df[\"Purchase Frequency\"]\n# Initialize and train the decision tree model\ndt_model = DecisionTreeRegressor(random_state=42)\n# Perform cross-validation\ncv_scores = cross_val_score(dt_model, X, y, cv=5, scoring='r2')\ncv_mean_score = cv_scores.mean()\nprint(f\"R2 using cross validation: {cv_mean_score}\")\n# Train the model on the entire dataset\ndt_model.fit(X, y)\n# Make predictions on the test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\u2423\n\u21aarandom_state=42)\ny_pred = dt_model.predict(X_test)\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error on Test Set: {mse}\")\nr2=r2_score(y_test, y_pred)\nprint(\"R2 without using cross validation:\",r2)\n35"
    },
    {
        "page": 328,
        "text": "# Plot the decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(dt_model, feature_names=X.columns, filled=True, rounded=True,\u2423\n\u21aafontsize=10)\nplt.title(\"Decision Tree for Predicting Purchase Frequency\")\nplt.show()\n# Display feature importance\nfeature_importance = pd.Series(dt_model.feature_importances_, index=X.columns).\n\u21aasort_values(ascending=False)\nplt.figure(figsize=(10, 6))\nfeature_importance.plot(kind='bar', color='skyblue')\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title('Feature Importance from Decision Tree Model')\nplt.show()\n# Perform cross-validation\ncv_scores = cross_val_score(dt_model, X, y, cv=5, scoring='r2')\ncv_mean_score = cv_scores.mean()\nprint(f\"R\u00b2 using cross-validation: {cv_mean_score}\")\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Mean Squared Error on Test Set: {mse}\")\nprint(f\"R\u00b2 on Test Set: {r2}\")\nR2 using cross validation: 0.14369418162564318\nMean Squared Error on Test Set: 0.2606988662131519\nR2 without using cross validation: 0.48496077650572433\n36"
    },
    {
        "page": 329,
        "text": "37"
    },
    {
        "page": 330,
        "text": "R\u00b2 using cross-validation: 0.14369418162564318\nMean Squared Error on Test Set: 0.2606988662131519\nR\u00b2 on Test Set: 0.48496077650572433\n1.7.6\nDecision Tree Analysis\nThe decision tree analysis revealed that advertisements are the highest importance feature in pre-\ndicting purchase frequency, but they show a negative correlation in the OLS regression.\nPeer\nreviews and influencer posts positively influence purchase frequency, while detailed unsponsored\nreviews, brand posts, and discounts have moderate influence. In terms of feature importance from\nthe decision tree model, advertisements have the highest feature importance and play a significant\nrole in predicting purchase frequency, yet they negatively correlate in the OLS regression. Peer\nreviews significantly enhance trust and credibility, thus positively impacting purchase frequency.\nInfluencer posts are important, but they are less influential than advertisements and peer reviews.\n1.7.7\nKey Insights from the Analysis\nKey insights from the analysis indicate that advertisements have a negative impact on purchase fre-\nquency, as shown by both the OLS regression and decision tree models. Peer reviews are positively\ncorrelated with purchase frequency and significantly enhance trust and credibility. Influencer posts\nalso positively impact purchase frequency, especially when combined with peer reviews.\nWhile\ndetailed unsponsored reviews, brand posts, and discounts do not show statistically significant in-\ndividual effects, they may still play a role when combined with other factors.\n1.7.8\nPrediction Examples\n\u2022 Example 1:\n\u2013 Content Types: Advertisements: 1, Peer Reviews: 1, Detailed Unsponsored Reviews:\n1\n\u2013 OLS Regression: Moderate purchase frequency (negative impact of advertisements).\n\u2013 Decision Tree: Higher purchase frequency (influenced by advertisements, peer reviews,\nand unsponsored reviews).\n\u2022 Example 2:\n\u2013 Content Types: Peer Reviews: 1, Influencer Posts: 1, Brand Posts: 1, Discount: 1\n\u2013 OLS Regression: Higher purchase frequency (positive impact of peer reviews and\ninfluencer posts).\n\u2013 Decision Tree: Moderate purchase frequency (influenced by peer reviews, influencer\nposts, brand posts, and discount).\nIn Summary, the analysis suggests that advertisements tend to have a negative impact on purchase\nfrequency, while peer reviews and influencer posts positively influence consumer purchasing behav-\nior. The OLS regression model highlights the negative impact of advertisements and the positive\ninfluence of peer reviews. In contrast, the Decision Tree model underscores that advertisements,\npeer reviews, and influencer posts are the key factors affecting purchase frequency. Therefore, a\nbalanced and optimized content strategy that leverages various content types, particularly peer\nreviews and influencer posts, can enhance marketing effectiveness and drive consumer engagement.\n38"
    },
    {
        "page": 331,
        "text": "1.7.9\n5. Association Between Ad Frequency and Platform Usage Frequency\nThis analysis aims to explore the relationship between how frequently individuals are exposed to\nads on social media and how often they use specific platforms. Understanding this association can\nprovide insights into whether higher ad exposure correlates with more frequent platform usage,\nhelping to optimize advertising strategies and platform engagement.\nHypothesis\n\u2022 Null Hypothesis (H\uffff): There is no association between ad frequency and platform usage\nfrequency; they are independent of each other.\n\u2022 Alternative Hypothesis (H\uffff): Ad frequency and platform usage frequency are dependent\non each other; an association exists between them.\nTo\ntest\nthe\nhypothesis,\nwe\nconducted\nan\nindependent\nt-test\n(stats.ttest_ind)\nto\ncompare\nthe\nmeans\nof\nthe\nencoded\nvariables:\nad_frequency_on_social_media\nand\nplatform_usage_frequency. The t-test evaluates whether there is a statistically significant dif-\nference between the means of the two groups, helping to determine if ad frequency is associated\nwith platform usage frequency.\n[53]: df_copy_test=df.copy()\nlabel_encoder = LabelEncoder()\ndf_copy_test['ad_frequency_on_social_media'] = label_encoder.\n\u21aafit_transform(df['ad_frequency_on_social_media'])\ndf_copy_test['platform_usage_frequency'] = label_encoder.\n\u21aafit_transform(df['platform_usage_frequency'])\nt_stat, p_value = stats.ttest_ind(df_copy_test[\"ad_frequency_on_social_media\"],\u2423\n\u21aadf_copy_test[\"platform_usage_frequency\"])\nprint(f\"P-value: {p_value}\")\nP-value: 4.5129915523926704e-05\nConclusion:\nSince the p-value = 0.0002 is less than 0.05, we reject the null hypothesis, which\nindicates that there is a statistically significant relationship between ad frequency and\nplatform usage frequency. This suggests that the frequency with which individuals encounter\nads on social media is dependent on how often they use specific platforms. The results imply that\nmore frequent platform usage may lead to greater exposure to ads, or vice versa, pointing to a\npotential influence of ad exposure on user engagement with social media platforms.\n1.8\nConclusion and Future Scope\nConclusion\nSurveys and statistical analysis supported the hypothesis that social media influences purchasing\nbehavior. The t-test showed that advertisement frequency is dependent on platform usage frequency\n(p < 0.05). Linear Regression (R\u00b2: 0.4085) outperformed the Decision Tree Regressor (R\u00b2: 0.2881)\nin predicting consumer spending.\nBoth models indicated that influencer promotions are more\neffective than brand advertisements in driving purchases, highlighting the importance of targeted\nand personalized marketing strategies.\n39"
    },
    {
        "page": 332,
        "text": "Future Scope 1. Long-term Impact: Study the long-term effects of social media engagement\non brand loyalty and repeat purchases. 2. Cultural and Geographical Factors: Explore how\ndifferent cultural and geographical factors influence social media\u2019s impact on consumer behavior.\n3. Dynamic Content Optimization: Optimize influencer promotions and advertisements using\nreal-time data analysis. 4. Advanced Analytics: Use machine learning and AI for more accurate\nconsumer behavior predictions.\nLimitations 1. Sample Size and Demographics: Findings may not be representative of the\nbroader population; a larger, more diverse sample is needed. 2. Self-Reported Data: Potential\nbiases such as social desirability and recall bias may affect data accuracy. 3. Temporal Scope:\nImmediate impacts were studied; long-term effects need further exploration. 4. Platform-Specific\nAnalysis: Different social media platforms were not differentiated; future studies should explore\nplatform-specific impacts. 5. External Influences: External factors like economic conditions and\nseasonal trends were not accounted for; including these variables would provide a more comprehen-\nsive understanding.\n1.9\nTask Division and Group Collaboration\nTask Division for 606 Project\nRole Assignments\nRole\nAssigned Member\nResponsibilities\nProject Manager\nHritvik Gaind\nOversees project\nprogress, ensures\ndeadlines are met,\nand re-assigns\ntasks as needed.\nMeeting Scheduler\nAyush Senthil Nelli\nSchedules team\nmeetings using the\nTFDL website\nscheduler.\nMeeting Chair\nVenkateshwaran Balu Soundararajan\nSets agendas for\nmeetings,\naddresses\nexpectations, and\nfacilitates\ndecision-making.\nDocumentation\nRehan Chanegaon\nTakes meeting\nminutes,\nmaintains project\ndocumentation,\nand compiles the\nREADME file.\n40"
    },
    {
        "page": 333,
        "text": "Role\nAssigned Member\nResponsibilities\nQuality Assurance\nSatyam Kapoor\nEnsures the code\nquality meets\nproject\nexpectations and\nstreamlines it\nwhere needed.\nWorkload Distribution for Deliverables\nDeliverable\nAssigned Member(s)\nDescription\nGoogle Forms Survey Creation\nHritvik Gaind, Ayush Senthil Nelli,\nVenkateshwaran Balu\nSoundararajan\nDesigned the\nsurvey and\nensured\nappropriate\nquestion\nformatting.\nGuiding Questions (1 each)\nAll Members (1 each)\nEach member\ncontributed\none guiding\nquestion.\nExploratory Data Analysis\n(EDA)\nHritvik Gaind, Satyam Kapoor,\nVenkateshwaran Balu\nSoundararajan\nConducted\nEDA to derive\ninsights and\nprepare\nvisuals.\nPowerPoint Presentation\nAyush Senthil Nelli, Rehan\nChanegaon, Satyam Kapoor\nCompiled\nfindings into a\nstructured\npresentation.\nReport Writing\nAll Members\nCollaborative\neffort to draft\nand finalize the\nproject report.\n1.10\nReferences\n[1]\nGoogle,\n\u201cThe\nInfluence\nof\nSocial\nMedia\nUsage\non\nConsumer\nPurchasing\nDeci-\nsions,\u201dGoogleForms.[Online]. Available:https://docs.google.com/forms/d/e/1FAIpQLSfN6FVKhkTBqA_7D8FM\n[Accessed: 17-Feb-2025].\n[2] Nielsen, J. and Budiu, R., \u201cSurvey Best Practices,\u201d Nielsen Norman Group, 2012. [Online].\nAvailable: https://www.nngroup.com/articles/survey-best-practices/. [Accessed: 15-Jan-2025].\n[3]\nWikipedia\ncontributors,\n\u201cBinomial\ndistribution,\u201d\nWikipedia,\nThe\nFree\nEncyclope-\ndia.[Online].Available: https://en.wikipedia.org/wiki/Binomial_distribution.\n[Accessed: 15-Jan-\n2025]\n41"
    },
    {
        "page": 334,
        "text": "[4]\nWikipedia\ncontributors,\n\u201cPoisson\ndistribution,\u201d\nWikipedia,\nThe\nFree\nEncyclope-\ndia.[Online].Available:\nhttps://en.wikipedia.org/wiki/Poisson_distribution.\n[Accessed:\n15-\nJan-2025].\n[5] DASCA, \u201cWhat is statistical modelling in data science,\u201d DASCA - World of Data Sci-\nence. [Online]. Available: https://www.dasca.org/world-of-data-science/article/what-is-statistical-\nmodeling-in-data-science. [Accessed: 20-Jan-2025]\n42"
    },
    {
        "page": 335,
        "text": "Weather Prediction and Forecasting \nProject Report  \n \nSubmitted By: \n30262995 Ayush Senthil Nelli \n30239509 Venkatesh  \n30248059 Hritvik Gaind \n30255909 Satyam Kapoor \n \n \nDATA 608: Developing Big Data Applications"
    },
    {
        "page": 336,
        "text": "Table of Contents: \n \nTable of Contents: .................................................................................................. 2 \n1.0 Problem Statement ........................................................................................... 2 \n1.1 Summary of Results .......................................................................................... 3 \n2.0 Pipeline Diagram ............................................................................................... 3 \n2.1 Data Generation ................................................................................................ 4 \n2.2 Data Ingestion ................................................................................................... 5 \n2.3 Data Storage ..................................................................................................... 6 \n2.4 Data Transformation ......................................................................................... 8 \n2.5 Data Serving ................................................................................................... 10 \n2.6 Data Outputs .................................................................................................. 10 \n3.0 Limitations and Possible Next Steps ................................................................. 12 \n3.1 Project Limitations .......................................................................................... 12 \n3.1 Future Expansions ........................................................................................... 13 \n4.0 Conclusions ................................................................................................... 14 \n5. Code ................................................................................................................ 15 \nEvidence that the application takes data through all the lifecycle .......................... 15 \nEvidence of Application Optimization ................................................................. 16 \nStatic Link to the Project .................................................................................... 16 \n \n \n \n \n \n1.0 Problem Statement \nThis project aims to analyze short-term weather variations across ten major Canadian cities \nusing weather data from 2024 and 2025. Understanding these variations is critical for"
    },
    {
        "page": 337,
        "text": "industries such as agriculture and transportation, which depend on accurate and timely \nweather forecasts for decision-making and risk management. By identifying trends, \nanomalies, and regional differences, the project seeks to improve forecast accuracy and \nprovide insights into seasonal and regional weather patterns. These findings will assist \npolicymakers and businesses in planning for weather-related risks and optimizing resource \nmanagement. \n \n1.1 Summary of Results \n \nThe Weather Analytics Dashboard offers an in-depth analysis of the city\u2019s climate patterns \nthrough an interactive and visually rich interface powered by data from the Open-Meteo \nHistorical Weather API. The dashboard presents monthly and daily temperature trends \nusing line charts that effectively capture seasonal variations and hourly fluctuations. A pie \nchart outlines the distribution of weather conditions. Precipitation and snowfall patterns \nare depicted through grouped bar charts, while a dual-axis visualization integrates average \nhumidity with monthly precipitation, enabling comparative insights. Additionally, the \ndashboard includes statistical insights revealing a strong negative correlation between \ntemperature and humidity, as well as weaker relationships with snowfall, wind speed, and \nprecipitation. The temperature forecast module further enhances utility by providing short-\nterm temperature predictions with upper and lower confidence bounds. Altogether, the \ndashboard serves as a comprehensive tool for understanding 10 canadian cities weather \ndynamics throughout the year. \n2.0 Pipeline Diagram \n \n \nThe above pipeline represents a comprehensive weather analytics system designed to \ncollect, process, analyze, and visualize meteorological data. The system begins with data \ningestion from the Open Meteo data source through a RESTful API, which is handled using"
    },
    {
        "page": 338,
        "text": "AWS Lambda functions to enable efficient and serverless data processing. The ingested \ndata is then stored in Amazon DynamoDB, a highly scalable NoSQL database, ensuring fast \nand reliable data access. This stored data supports two primary components: machine \nlearning models for temperature forecasting and model comparison, and analytics \nfunctions for identifying weather patterns and generating visualizations. Both the machine \nlearning outputs and analytical insights are served through an interactive Streamlit \napplication hosted on Amazon EC2, providing users with real-time, accessible, and \ninterpretable weather insights. \n2.1 Data Generation \nThe data for this project is sourced from the Open-Meteo Historical Weather API, which \naggregates weather information from multiple authoritative sources, including reanalysis \ndatasets and national meteorological services. For this analysis, we utilize time series \nweather data from January 1st, 2024 to the current date for various cities across Canada, \nretrieved using their respective longitude and latitude coordinates. \nA key source of this data is the ECMWF IFS (Integrated Forecasting System), a global \nreanalysis dataset available from 2017 to present and updated daily with a 2-day delay. \nThe dataset integrates observations from weather stations, satellites, aircraft, and radar, \nusing advanced numerical models to reconstruct historical weather conditions, even in \nremote or sparsely monitored areas. \nIn addition, Open-Meteo incorporates weather data from the GEM (Global Environmental \nMultiscale) model by the Canadian Weather Service, which is updated every 6 hours, \nenhancing the temporal resolution and accuracy of the data retrieved. \nThe historical weather data includes a wide array of hourly variables such as temperature \nat 2 meters, apparent temperature, relative humidity, dewpoint, precipitation \nprobability, total precipitation (including rain and snow), snowfall, snow depth, \nsunshine duration, UV index, and daylight duration. Other details, such as sunrise and \nsunset times and WMO (World Meteorological Organization) weather codes, are also \nincluded to support more meaningful interpretation of weather conditions. For this project, \nonly the most relevant parameters were selected to ensure the analysis remains focused \nand efficient."
    },
    {
        "page": 339,
        "text": "2.2 Data Ingestion \n \nThe data ingestion process was carefully designed to establish a reliable foundation for our \nweather prediction system. After evaluating several weather data sources, we selected the \nOpen-Meteo API for its comprehensive coverage of essential meteorological variables \nincluding temperature, precipitation, wind speed, and weather codes. The API's consistent \ndata formatting and reliable uptime made it particularly suitable for our needs. \n \n \n \nTo implement an efficient ingestion pipeline, we adopted a serverless architecture using \nAWS Lambda. This approach allowed us to focus on data processing rather than \ninfrastructure management, while automatically scaling to handle varying workloads. The \nLambda functions were configured to run on a scheduled basis, ensuring fresh data is \nalways available for analysis. \n \nFor daily weather updates, we developed an automated process that fetches the previous \nday's data, accounting for the API's natural latency in data availability. This decision came"
    },
    {
        "page": 340,
        "text": "after observing that real-time data sometimes contained incomplete measurements. The \nsystem now consistently delivers verified, complete weather records ready for processing. \n \nHistorical data collection presented its own challenges. We implemented a batch \nprocessing solution that retrieves comprehensive weather records spanning from January \n2024 to March 2025. This historical dataset proved invaluable for identifying long-term \nweather patterns and training our predictive models. \n \n \n \nKey features of our ingestion system include: \n\u2022 Lambda-based data fetching that provides serverless, scalable API integration \n\u2022 Automated validation checks implemented within Lambda functions to verify data \ncompleteness \n\u2022 Comprehensive error handling in Lambda to manage API rate limits and temporary \noutages \n\u2022 EventBridge-triggered Lambda executions that ensure timely data updates \n\u2022 Cost-effective processing through Lambda's pay-per-use pricing model \nThe implementation process taught us valuable lessons about working with cloud services \nand handling real-world data inconsistencies. These insights directly informed our system \ndesign, resulting in a robust ingestion framework that forms the cornerstone of our weather \nprediction capabilities. \n \n2.3 Data Storage \n \nOur storage architecture implements a carefully designed two-tier system to balance \nperformance, cost, and reliability. The primary data store utilizes AWS DynamoDB, chosen"
    },
    {
        "page": 341,
        "text": "specifically for its high velocity read/write capabilities, flexible schema architecture, and \nautomatic scaling properties. As a purpose-built NoSQL database, DynamoDB delivers the \nsingle-digit millisecond latency we require for real-time weather data operations and \ndashboard visualizations. \n \nTo ensure comprehensive data protection and enable different access patterns, we \nimplemented Amazon S3 as our secondary storage layer with three key functions: \n1. Primary API Data Backup: All ingested weather data from Open-Meteo is \nautomatically written to S3 as a protective measure. \n2. Forecast Output Repository: Model predictions are systematically serialized to \nCSV format and archived in S3 buckets \n3. Analysis Ready Storage: S3 houses processed datasets optimized for batch \nanalytics jobs. \nThe strategic advantages of this architecture include: \n\u2022 Real-Time Performance: DynamoDB serves operational queries with consistent \n<10ms response times. \n\u2022 Data Safeguarding: S3's extreme durability protects against any potential data loss \nscenarios. \n\u2022 Cost Optimization: High-performance DynamoDB handles frequent access \npatterns while S3 economically stores less-active data. \n\u2022 Model Versioning: Forecast outputs in S3 create timestamped records of all \nprediction runs. \n \nImplementation highlights: \n\u2022 DynamoDB tables configured with optimized partition keys matching our primary \naccess patterns"
    },
    {
        "page": 342,
        "text": "\u2022 S3 bucket organization using clear naming conventions. \n \n \nThis storage framework has demonstrated excellent performance during load testing, \ncapably handling both the velocity of real-time data ingestion and the volume of historical \nweather records. The combination of DynamoDB's low-latency performance with S3's \nmassive storage capacity creates a future-proof foundation that can scale with our growing \ndata needs. \n \n2.4 Data Transformation \n \nThe transformation process converts raw API data into a clean, analysis-ready format \nthrough a series of carefully designed operations executed within AWS Lambda functions. \nOur goal was to enhance data quality while preserving the original information's integrity \nand meaning, ensuring all transformations occur before storage in DynamoDB. \nUpon fetching data from the Open-Meteo API, our Lambda functions immediately process \nit through these transformation stages: \n \nData Quality Foundations: \n\u2022 Standardizing timestamp formats into consistent datetime objects for reliable time-\nseries analysis. \n\u2022 Intelligently handling missing values (e.g., substituting zeros for absent snowfall \nmeasurements). \n\u2022 Validating all measurement ranges to identify and flag potential anomalies."
    },
    {
        "page": 343,
        "text": "Value-Added Feature Engineering: \n\u2022 Extracting temporal features (month, year) from timestamps. \n\u2022 Applying accurate seasonal classifications (winter, spring, summer, fall) based on \nmeteorological standards. \n\u2022 Translating numeric weather codes into human-readable descriptions (e.g., \n\"sunny\", \"rainy\"). \nThese transformations are efficiently implemented using pandas operations within \nLambda, achieving an optimal balance between processing thoroughness and \ncomputational efficiency. The EventBridge scheduler ensures new daily data undergoes \nthe same transformation pipeline before being appended to DynamoDB. \n \n \n \nKey advantages of this pre-storage transformation approach: \n\u2022 Dashboard Efficiency: Streamlit visualizations work with ready-to-use data, \neliminating transformation overhead during rendering \n\u2022 Data Consistency: All records in DynamoDB maintain uniform formatting and \nenrichment \n\u2022 Processing Isolation: Transformation logic is encapsulated in Lambda functions, \nseparate from storage and visualization layers \n\u2022 Temporal Integrity: Historical and new data receive identical processing, ensuring \nanalytical continuity \nPerformance metrics confirm the transformations add minimal latency while significantly \nenhancing data usability. The system's modular design allows for future optimization \nshould processing requirements evolve, though current benchmarks show the Lambda \nfunctions comfortably handle our daily data volumes within allocated resource limits."
    },
    {
        "page": 344,
        "text": "By completing all transformations before storage, we've created a streamlined pipeline \nwhere DynamoDB contains analysis-ready data, and our Streamlit dashboard can focus \nsolely on visualization without additional processing burdens. \n \n2.5 Data Serving \n \nTransformed data is presented via a Streamlit dashboard, which provides an interactive \nand user-friendly interface for displaying and interacting with data insights. The dashboard \nserves multiple roles, including visualizing trends, providing forecasts, and displaying \nessential weather data to the user. \nThe data is served and processed on an EC2 instance, which hosts the Streamlit app. The \nEC2 instance retrieves weather data from DynamoDB, and for temperature forecasts, it \npulls relevant data from S3 storage. The transformed data and forecasts are then \ndisplayed in the dashboard, which includes various visualizations and provides a \nprediction of the temperature for the next 24 hours. \n\u2022 Streamlit: This technology is used for creating an interactive and user-friendly \ndashboard. It allows the user to view visualizations and interact with data in real-\ntime. Streamlit makes it easy to generate plots, charts, and other elements that are \ncritical for displaying weather-related information. \n\u2022 EC2 Instance: The EC2 instance serves as the cloud-based server that hosts the \nStreamlit app. It also handles the retrieval of data from DynamoDB for historical \nweather information and from S3 for the temperature forecast data. The EC2 \ninstance ensures that the app remains accessible, scalable, and efficient for \nprocessing large datasets and generating predictions. \n \n \n2.6 Data Outputs \n \nVisualizations in the Dashboard \nThe dashboard presents multiple visualizations to help users understand the weather \ntrends, variations, and predictions. Below are the key visualizations provided in the \nStreamlit app: \n1. Temperature Trends:"
    },
    {
        "page": 345,
        "text": "\u2022 Graphical Representation: Displays how the temperature varies across \ndifferent months and times of the day. \n\u2022 Purpose: Helps users understand the seasonal and daily temperature \nfluctuations, enabling them to identify patterns and trends in temperature \nover time. This insight can assist in making decisions related to outdoor \nactivities, clothing choices, or energy consumption. \n2. Weather Distribution: \n\u2022 Pie Chart: Visualizes the distribution of different weather conditions (e.g., \nsunny, cloudy, snowy, rainy). \n\u2022 Purpose: Provides a quick overview of the overall weather trends over a \nspecified period, helping users assess the frequency of various weather \nconditions. \n3. Rain & Snow Levels: \n\u2022 Chart: Illustrates the monthly distribution of rain and snow levels. \n\u2022 Purpose: This helps users understand seasonal variations in precipitation \nand snow accumulation, providing insights into overall weather patterns. \n4. Precipitation & Humidity Trends: \n\u2022 Chart: Displays trends of precipitation and humidity over the months. \n\u2022 Purpose: Offers insights into seasonal changes in humidity and \nprecipitation, allowing users to correlate weather changes with specific \nmonths. \n5. Temperature Correlations in Calgary: \n\u2022 Graphical Representation: Displays the relationships between temperature \nand various weather features, including humidity, snowfall, precipitation, \nand wind speed in Calgary. \n\u2022 Purpose: This visualization helps users understand how different weather \nfactors correlate with temperature variations in Calgary. By exploring these \ncorrelations, users can gain insights into how conditions like humidity or \nwind speed may influence temperature patterns, assisting with more \ninformed weather-related decisions. \n \n \nMachine Learning Model \u2013 Temperature Prediction \nA temperature prediction model, utilizing historical weather data, contributes to the \nforecast section of the dashboard. The model used for this prediction is the ARIMA model \nfrom the statsmodels library. \n1. ARIMA Model:"
    },
    {
        "page": 346,
        "text": "\u2022 Purpose: The ARIMA (AutoRegressive Integrated Moving Average) model is \nemployed to forecast the temperature for the next 24 hours based on \nhistorical time series data. \n\u2022 Implementation: The model takes into account past temperature values \nand uses statistical methods to predict future temperatures. \n2. Temperature Plot: \n\u2022 Output: A visual representation of the predicted temperature for the next \nday. \n\u2022 Purpose: Users can easily visualize how the temperature is expected to \nchange throughout the day, helping with planning and decision-making. \n \n3.0 Limitations and Possible Next Steps \n \n3.1 Project Limitations \n \nDuring the course of this project, we encountered several limitations that shaped our \nimplementation decisions and influenced the outcome. One of the key constraints was the \nusage restrictions imposed by the free weather API we selected. The API allowed for a \nmaximum of 600 calls per minute, 5,000 per hour, and 10,000 per day. To stay within these \nlimits, we restricted the data collection to just one year of historical weather data. This \nlimited volume of data, while sufficient for a basic proof of concept, may reduce the \ngeneralizability and accuracy of our model under more complex or long-term forecasting \nscenarios. \n \nAnother significant limitation was the delayed availability of data. The API did not provide \naccess to real-time or same-day weather data, only allowing data retrieval up to two days \nbefore the current date. This delay restricted our ability to validate forecasts promptly and \nlimited the dashboard\u2019s ability to display the most current weather information. Using a \nmore real-time API in the future could address this shortcoming and improve system \nresponsiveness. \n \nTo further manage API usage and complexity, we limited our geographic scope to 10 major \ncities in Canada. While this approach kept the data manageable, it also reduced the scale \nand potential applications of the tool, especially for users interested in broader or more \nlocalized coverage. Additionally, we limited our forecasting horizon to a single day (24-hour"
    },
    {
        "page": 347,
        "text": "prediction), which, although useful for immediate insights, does not support medium- or \nlong-term planning. \n \nInitially, the system was designed to utilize Amazon RDS along with AWS Lambda and \nEventBridge Scheduler to automate data ingestion and structured storage. However, due \nto restricted IAM roles and limited access within the AWS educational lab environment, we \nhad to pivot and use AWS DynamoDB. While DynamoDB offered simplicity and easy \nintegration, it came with trade-offs in terms of querying capabilities and relational data \nsupport. \n3.1 Future Expansions \nLooking ahead, we see multiple opportunities to expand and enhance our project. A key \npriority is to experiment with more advanced time-series models beyond ARIMA. We plan \nto evaluate SARIMA for handling seasonal trends, Prophet for its flexible forecasting \ncapabilities, and LSTM networks for their ability to model complex temporal relationships \nin the data. These models could allow us to generate forecasts for longer periods, such as \n7 to 15 days, and offer more accurate predictions. \n \nWe also plan to broaden the geographic scope of the project by including more cities \nacross Canada, and potentially other countries, to improve the system\u2019s reach and \nusefulness. To further support scalability and timeliness, integrating real-time or near-real-\ntime weather data sources will be critical. This would enable real-time forecasting and \nallow us to validate predictions as new data comes in. \n \nAnother major area of enhancement is the dashboard. At present, the Streamlit dashboard \nprovides a 1-day temperature forecast for a selected city using basic visualizations. In the \nfuture, we aim to incorporate geospatial visualizations and live heatmaps using tools like \nPlotly, Folium, or Deck.gl. These enhancements will offer a more interactive and visually \ninformative user experience, allowing users to explore temperature trends across regions \nin an intuitive way. \n \nAdditionally, we hope to revisit our original architecture if we get an account with AWS \nadditional featured roles. This would involve transitioning back to Amazon RDS for \nstructured data storage and using Lambda functions and Event Bridge for orchestration. \nFinally, implementing CI/CD pipelines using GitHub Actions or AWS Code Pipeline, as well \nas infrastructure automation tools like Terraform, would further improve the project\u2019s \nmaintainability, scalability, and deployment process."
    },
    {
        "page": 348,
        "text": "4.0 Conclusions \n \nThis project demonstrates the successful development of a weather data analytics and \nforecasting pipeline using AWS cloud services and ARIMA modeling. By integrating \nLambda, S3, and DynamoDB, we automated the retrieval and storage of weather data and \nvisualized 24-hour temperature forecasts for ten Canadian cities through a Streamlit \ndashboard. \nWhile we met our initial goals, we faced limitations such as API call restrictions, delayed \ndata access, and limited AWS lab permissions. These influenced our design choices and \nrevealed opportunities for future improvements, including real-time data integration, \nlonger-term forecasting, expanded geographic coverage, and enhanced dashboard \nfeatures. \nOverall, this project serves as a strong foundation for scalable weather analytics using \ncloud infrastructure and machine learning. \nKey Learnings \nThrough this project, we gained practical experience in: \n\u2022 \nBuilding a full data pipeline with AWS services like Lambda, S3, and DynamoDB \n\u2022 \nManaging event-driven workflows and overcoming infrastructure constraints \n\u2022 \nApplying ARIMA for time-series forecasting and tuning models with real-world \nweather data \n\u2022 \nDeveloping an interactive dashboard in Streamlit to present insights clearly \n\u2022 \nAdapting to API limitations and planning for scalability and automation \n \nThis hands-on experience deepened our understanding of data engineering and taught us \nhow to deliver insights by combining data science with thoughtful system design."
    },
    {
        "page": 349,
        "text": "5. Code \nhttps://github.com/ayushnelli03/Data608 \nEvidence that the application takes data through all the lifecycle \nThe application demonstrates the complete data lifecycle\u2014from data collection to \ntransformation, storage, and serving\u2014through a series of interconnected scripts and \nprocesses. Below are the key components that show how the data flows through the \nlifecycle: \n\u2022 db_lambda_fetchHisData.py: \no Purpose: This Lambda function is responsible for fetching weather data from \nan external API and storing it in DynamoDB. \no Data Lifecycle: It initiates the lifecycle by retrieving raw weather data, \npreprocessing it (such as cleaning and transformation), and then storing it in \nDynamoDB for future use. This is the first step in the process of transforming \nraw data into usable insights. \n\u2022 db_lambda_automate.py: \no Purpose: This function automates the process of fetching updated weather \ndata from the API via an EventBridge scheduler. \no Data Lifecycle: It runs on a scheduled basis to fetch fresh data, transforming \nand cleaning it before appending the new records to the existing data in \nDynamoDB. This ensures the data stays current, and the application \nmaintains an up-to-date repository for future analysis and forecasting. \n\u2022 ec2_dbConnect.py: \no Purpose: This script tests the connectivity between the EC2 instance and \nDynamoDB, fetching paginated data for validation. \no Data Lifecycle: It ensures that data stored in DynamoDB is accessible from \nthe EC2 instance, which is an essential part of serving data to the Streamlit \ndashboard for visualization. This step validates that the data can be fetched \nreliably from the database for further processing and serving. \n\u2022 streamlitdb.py: \no Purpose: This Python script serves as the core of the Streamlit dashboard. It \nfetches data from DynamoDB, processes it, and visualizes the insights \nthrough interactive components in the Streamlit application. \no Data Lifecycle: It retrieves processed and cleaned data from DynamoDB and \npresents it to the user in an intuitive, interactive format. This is the final step"
    },
    {
        "page": 350,
        "text": "in the lifecycle where users can explore the data through various \nvisualizations (e.g., temperature trends, weather distribution, and forecasts). \nTogether, these scripts ensure that data is effectively ingested, processed, stored, and \nmade available for visualization, completing the data engineering lifecycle. \n \nEvidence of Application Optimization \nThe application has been optimized to ensure efficient data processing and serving, with \narchitectural choices and parallel processing techniques aimed at improving performance. \n\u2022 Lambda Functions: \nBoth db_lambda_fetchHisData.py and db_lambda_automate.py utilize AWS \nLambda functions, which allow for serverless execution of code. This ensures that \nthe data-fetching and transformation tasks are executed efficiently without the \nneed for dedicated servers. Lambda also supports parallel processing\u2014if \nnecessary, multiple invocations can be handled simultaneously, making the \nprocess more scalable and reducing the time required to process and store data. \n\u2022 EC2 Instance: \nThe EC2 instance is utilized for running the Streamlit application and for performing \nthe heavy lifting of connecting to the DynamoDB database and fetching data. The \nEC2 instance is optimized for higher performance, meaning it is likely provisioned \nwith more computational power to handle larger volumes of data and perform data \nprocessing tasks more efficiently. In this case, higher-powered EC2 instances can \nensure that visualizations load quickly and that forecasts are generated with \nminimal latency. \n \n \nStatic Link to the Project \nYou \ncan \naccess \nthe \nStreamlit \ndashboard \nat \nthe \nfollowing \nstatic \nURL: \nhttp://streamlit-dashboard-launcher.s3-website-us-east-1.amazonaws.com"
    }
]